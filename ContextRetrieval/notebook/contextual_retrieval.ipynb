{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H46QOuoHEg4x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "386jgs8vEg4y"
      },
      "source": [
        "# Description\n",
        "\n",
        "## This aim of this notbook to show and compare Contextual Retrieval implementation of RAG vs. simple/traditional implemintation\n",
        "### Steps:\n",
        "- Chucking\n",
        "- Summarization\n",
        "- BM25 embedding\n",
        "- BM25 model saving to file\n",
        "- Model embedding\n",
        "- Storage of dense and sparse vectors\n",
        "- Retrieval of sparse and dense vectors\n",
        "- Fusion of Ranking\n",
        "- Simple Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i0rQp-2yE9tj"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers -qU\n",
        "!pip install rank_bm25 -qU\n",
        "!pip install datasets -qU\n",
        "!pip install pinecone -qU\n",
        "!pip install langchain -qU\n",
        "!pip install langchain_core -qU\n",
        "!pip install langchain_groq -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLvPphUjI1yb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7pHvMtoEg41"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oTin1IoFEg42"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "# from rouge import Rouge\n",
        "from datasets import load_dataset\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pinecone\n",
        "import pandas as pd # for dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bv35qGcEg43"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw1FTzkTEg44"
      },
      "source": [
        "# Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "8056bab4785e425f8556147a8f90d7ba",
            "5f45a15713e54dbda25dd0231581a58f",
            "0e581db8782b4322909118cd7934a853",
            "731076d6ad234d988b97536832899098",
            "827176e3e8c540bc9158b4b24de100b1",
            "6e229a6919e44fa7b093759655102985",
            "a2a41b32407f464aa506f7eea25ddf96",
            "f045da443a3540d78753cc0813384e19",
            "da47b2c8d6f3456ca8a10fc88f95a419",
            "1dff4bd0b03a4855b8142a192f2f8caa",
            "60d4f4b6769c40189d072a05c4a1ee94",
            "87a31898206e40188e569879d70289ec",
            "d4e976ed60cb4046b1266bb3b5cb2ea8",
            "1cb677f600d5417fa673aaf8cb2cbaca",
            "77e6947746384fd7bbb13eb8f7874bd3",
            "536f16f75e904f8b88cb1da3563da37c",
            "d29952ed432541fbb755a2a67dc6c4fb",
            "5842b04e603047c1a0f1e3aa8927915a",
            "1005f949f24a4dd398646fc88fdef0f3",
            "a573f6b3367f4dc280b91752f3b0c5e7",
            "a84419ba5d1e4379b91de8c9527f32bc",
            "1a2337aa8aac44a2b1fece87289d9784",
            "79a35a3156534e7b9e155b96ca0313d2",
            "fb8babd540c64fb9b4638ff5c7ffa35b",
            "2074667885f3477aa58c562bb9f30ebb",
            "6d2012f40551430b993899e4836f3307",
            "010bdb492c7845c2948baa14080a19c3",
            "1fa8e93c28d24a4397fbc832422b5bf5",
            "0f5a529098b14d6eab10823e0685b19a",
            "76583f5ae0b0413eb701f6f416ff72de",
            "32669220fd824dbeb68fd6228a5d1438",
            "37ee0dece8c14c379ed2b5aa499daa47",
            "dd0e104a17c1443387f5b059096a0592"
          ]
        },
        "id": "plaURsn2eFlM",
        "outputId": "72c0d2b2-8031-4d4b-8038-cd91c38bef60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/893 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8056bab4785e425f8556147a8f90d7ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87a31898206e40188e569879d70289ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79a35a3156534e7b9e155b96ca0313d2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 1: Load and Chunk the Knowledge Base\n",
        "# Load dataset from Hugging Face\n",
        "nltk.download('punkt')\n",
        "dataset = load_dataset(\"m-ric/huggingface_doc_qa_eval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbXt0DzrEg44",
        "outputId": "57516e58-47a1-4f36-b1f9-c8218daea77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             context  \\\n",
            "0   `tokenizers-linux-x64-musl`\\n\\nThis is the **...   \n",
            "1  !--Copyright 2023 The HuggingFace Team. All ri...   \n",
            "2   Paper Pages\\n\\nPaper pages allow people to fi...   \n",
            "3   Datasets server API\\n\\n> API on 🤗 datasets\\n\\...   \n",
            "4  !--Copyright 2022 The HuggingFace Team. All ri...   \n",
            "\n",
            "                                            question  \\\n",
            "0  What architecture is the `tokenizers-linux-x64...   \n",
            "1  What is the purpose of the BLIP-Diffusion mode...   \n",
            "2  How can a user claim authorship of a paper on ...   \n",
            "3  What is the purpose of the /healthcheck endpoi...   \n",
            "4  What is the default context window size for Lo...   \n",
            "\n",
            "                                              answer  \\\n",
            "0                          x86_64-unknown-linux-musl   \n",
            "1  The BLIP-Diffusion model is designed for contr...   \n",
            "2  By clicking their name on the corresponding Pa...   \n",
            "3                          Ensure the app is running   \n",
            "4                                         127 tokens   \n",
            "\n",
            "                                          source_doc  standalone_score  \\\n",
            "0  huggingface/tokenizers/blob/main/bindings/node...                 5   \n",
            "1  huggingface/diffusers/blob/main/docs/source/en...                 5   \n",
            "2  huggingface/hub-docs/blob/main/docs/hub/paper-...                 5   \n",
            "3  huggingface/datasets-server/blob/main/services...                 5   \n",
            "4  huggingface/transformers/blob/main/docs/source...                 5   \n",
            "\n",
            "                                     standalone_eval  relatedness_score  \\\n",
            "0  The question is asking about the specific arch...                  5   \n",
            "1  The question is asking for the purpose of a sp...                  5   \n",
            "2  The question is clear and does not depend on a...                  5   \n",
            "3  The question is asking for the purpose of a sp...                  5   \n",
            "4  The question is asking for a specific paramete...                  5   \n",
            "\n",
            "                                    relatedness_eval  relevance_score  \\\n",
            "0  The context directly specifies the architectur...                3   \n",
            "1  The context provides a detailed description of...                3   \n",
            "2  The context provides a clear explanation of ho...                3   \n",
            "3  The context directly states the purpose of the...                4   \n",
            "4  The context provides a specific detail about t...                3   \n",
            "\n",
            "                                      relevance_eval  \n",
            "0  The question is asking for specific technical ...  \n",
            "1  The question asks about the purpose of the BLI...  \n",
            "2  The question is specific to the Hugging Face H...  \n",
            "3  The question is specific and technical, asking...  \n",
            "4  This question is specific and technical, askin...  \n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(dataset['train'])\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjRiBI5tEg45",
        "outputId": "4bd9a10d-56b1-4daa-df31-6f1c041ee35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             context  \\\n",
            "0   `tokenizers-linux-x64-musl`\\n\\nThis is the **...   \n",
            "1  !--Copyright 2023 The HuggingFace Team. All ri...   \n",
            "2   Paper Pages\\n\\nPaper pages allow people to fi...   \n",
            "3   Datasets server API\\n\\n> API on 🤗 datasets\\n\\...   \n",
            "4  !--Copyright 2022 The HuggingFace Team. All ri...   \n",
            "\n",
            "                                            question  \\\n",
            "0  What architecture is the `tokenizers-linux-x64...   \n",
            "1  What is the purpose of the BLIP-Diffusion mode...   \n",
            "2  How can a user claim authorship of a paper on ...   \n",
            "3  What is the purpose of the /healthcheck endpoi...   \n",
            "4  What is the default context window size for Lo...   \n",
            "\n",
            "                                              answer  \\\n",
            "0                          x86_64-unknown-linux-musl   \n",
            "1  The BLIP-Diffusion model is designed for contr...   \n",
            "2  By clicking their name on the corresponding Pa...   \n",
            "3                          Ensure the app is running   \n",
            "4                                         127 tokens   \n",
            "\n",
            "                                          source_doc  standalone_score  \\\n",
            "0  huggingface/tokenizers/blob/main/bindings/node...                 5   \n",
            "1  huggingface/diffusers/blob/main/docs/source/en...                 5   \n",
            "2  huggingface/hub-docs/blob/main/docs/hub/paper-...                 5   \n",
            "3  huggingface/datasets-server/blob/main/services...                 5   \n",
            "4  huggingface/transformers/blob/main/docs/source...                 5   \n",
            "\n",
            "                                     standalone_eval  relatedness_score  \\\n",
            "0  The question is asking about the specific arch...                  5   \n",
            "1  The question is asking for the purpose of a sp...                  5   \n",
            "2  The question is clear and does not depend on a...                  5   \n",
            "3  The question is asking for the purpose of a sp...                  5   \n",
            "4  The question is asking for a specific paramete...                  5   \n",
            "\n",
            "                                    relatedness_eval  relevance_score  \\\n",
            "0  The context directly specifies the architectur...                3   \n",
            "1  The context provides a detailed description of...                3   \n",
            "2  The context provides a clear explanation of ho...                3   \n",
            "3  The context directly states the purpose of the...                4   \n",
            "4  The context provides a specific detail about t...                3   \n",
            "\n",
            "                                      relevance_eval  \n",
            "0  The question is asking for specific technical ...  \n",
            "1  The question asks about the purpose of the BLI...  \n",
            "2  The question is specific to the Hugging Face H...  \n",
            "3  The question is specific and technical, asking...  \n",
            "4  This question is specific and technical, askin...  \n"
          ]
        }
      ],
      "source": [
        "best_answers_df = df[df['standalone_score'] >= 4]\n",
        "print(best_answers_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsJFVK-WEg45",
        "outputId": "2cfe27a0-a57e-4ab4-ff9d-9e7628d47395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 65 entries, 0 to 64\n",
            "Data columns (total 10 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   context            65 non-null     object\n",
            " 1   question           65 non-null     object\n",
            " 2   answer             65 non-null     object\n",
            " 3   source_doc         65 non-null     object\n",
            " 4   standalone_score   65 non-null     int64 \n",
            " 5   standalone_eval    65 non-null     object\n",
            " 6   relatedness_score  65 non-null     int64 \n",
            " 7   relatedness_eval   65 non-null     object\n",
            " 8   relevance_score    65 non-null     int64 \n",
            " 9   relevance_eval     65 non-null     object\n",
            "dtypes: int64(3), object(7)\n",
            "memory usage: 5.2+ KB\n"
          ]
        }
      ],
      "source": [
        "best_answers_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AU0Loz2Eg45"
      },
      "source": [
        "# Extract contexts from the dataset and create Langchain documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83Szf9dyEg46",
        "outputId": "75f68965-a89b-4888-9753-4981b57dfd92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={}, page_content=' `tokenizers-linux-x64-musl`\\n\\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`\\n'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# BLIP-Diffusion\\n\\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \\n\\n\\nThe abstract from the paper is:\\n\\n*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*\\n\\nThe original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\\n\\n`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\\n\\n<Tip>\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n\\n## BlipDiffusionPipeline\\n[[autodoc]] BlipDiffusionPipeline\\n    - all\\n    - __call__\\n\\n## BlipDiffusionControlNetPipeline\\n[[autodoc]] BlipDiffusionControlNetPipeline\\n    - all\\n    - __call__\\n'), Document(metadata={}, page_content=' Paper Pages\\n\\nPaper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-discussions-dark.png\"/>\\n</div>\\n\\n## Linking a Paper to a model, dataset or Space\\n\\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository\\'s tags. Clicking on the arxiv tag will let you:\\n\\n* Visit the Paper page.\\n* Filter for other models or datasets on the Hub that cite the same paper.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\\n</div>\\n\\n## Claiming authorship to a Paper\\n\\nThe Hub will attempt to automatically match paper to users based on their email. \\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\\n</div>\\n\\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\\n</div>\\n\\n\\n## Frequently Asked Questions\\n\\n### Can I control which Paper pages show in my profile?\\n\\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile. \\n\\n### Do you support ACL anthology?\\n\\nWe\\'re starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We\\'ll check how this evolve and potentially extend to other paper hosts in the future.\\n\\n### Can I have a Paper page even if I have no model/dataset/Space?\\n\\nYes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index.\\n'), Document(metadata={}, page_content=' Datasets server API\\n\\n> API on 🤗 datasets\\n\\n## Configuration\\n\\nThe service can be configured using environment variables. They are grouped by scope.\\n\\n### API service\\n\\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\\n\\n## Endpoints\\n\\nSee https://huggingface.co/docs/datasets-server\\n\\n- /healthcheck: Ensure the app is running\\n- /metrics: Return a list of metrics in the Prometheus format\\n- /webhook: Add, update or remove a dataset\\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset\\n'), Document(metadata={}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# LongT5\\n\\n## Overview\\n\\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It\\'s an\\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\\nTransient-Global attention.\\n\\n\\nThe abstract from the paper is the following:\\n\\n*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\\\em Transient Global}\\n(TGlobal), which mimics ETC\\'s local/global attention mechanism, but without requiring additional side-inputs. We are\\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\\nquestion answering tasks.*\\n\\nThis model was contributed by [stancld](https://huggingface.co/stancld).\\nThe original code can be found [here](https://github.com/google-research/longt5).\\n\\n## Usage tips\\n\\n- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token\\nin the block. Thanks to this, the attention allows each token to attend to both nearby tokens like in Local attention, and\\nalso every global token like in the case of standard global attention (*transient* represents the fact the global tokens\\nare constructed dynamically within each attention operation).  As a consequence, *TGlobal* attention introduces\\na few new parameters -- global relative position biases and a layer normalization for global token\\'s embedding.\\nThe complexity of this mechanism is `O(l(r + l/k))`.\\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below.\\n\\n```python\\n>>> import evaluate\\n>>> from datasets import load_dataset\\n>>> from transformers import AutoTokenizer, LongT5ForConditionalGeneration\\n\\n>>> dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\\n>>> model = (\\n...     LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\\n...     .to(\"cuda\")\\n...     .half()\\n... )\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\\n\\n\\n>>> def generate_answers(batch):\\n...     inputs_dict = tokenizer(\\n...         batch[\"article\"], max_length=16384, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\\n...     )\\n...     input_ids = inputs_dict.input_ids.to(\"cuda\")\\n...     attention_mask = inputs_dict.attention_mask.to(\"cuda\")\\n...     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)\\n...     batch[\"predicted_abstract\"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\\n...     return batch\\n\\n\\n>>> result = dataset.map(generate_answer, batched=True, batch_size=2)\\n>>> rouge = evaluate.load(\"rouge\")\\n>>> rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"])\\n```\\n\\n\\n## Resources\\n\\n- [Translation task guide](../tasks/translation)\\n- [Summarization task guide](../tasks/summarization)\\n\\n## LongT5Config\\n\\n[[autodoc]] LongT5Config\\n\\n<frameworkcontent>\\n<pt>\\n\\n## LongT5Model\\n\\n[[autodoc]] LongT5Model\\n    - forward\\n\\n## LongT5ForConditionalGeneration\\n\\n[[autodoc]] LongT5ForConditionalGeneration\\n    - forward\\n\\n## LongT5EncoderModel\\n\\n[[autodoc]] LongT5EncoderModel\\n    - forward\\n\\n</pt>\\n<jax>\\n\\n## FlaxLongT5Model\\n\\n[[autodoc]] FlaxLongT5Model\\n    - __call__\\n    - encode\\n    - decode\\n\\n## FlaxLongT5ForConditionalGeneration\\n\\n[[autodoc]] FlaxLongT5ForConditionalGeneration\\n    - __call__\\n    - encode\\n    - decode\\n\\n</jax>\\n</frameworkcontent>\\n'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# AutoPipeline\\n\\n`AutoPipeline` is designed to:\\n\\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\\n2. use multiple pipelines in your workflow\\n\\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\\n\\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\\n\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2Image.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\\n).to(\"cuda\")\\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\\n\\nimage = pipeline(prompt, num_inference_steps=25).images[0]\\n```\\n\\n<Tip>\\n\\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\\n\\n</Tip>\\n\\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\\n\\n- [Stable Diffusion](./stable_diffusion/overview)\\n- [ControlNet](./controlnet)\\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\\n- [DeepFloyd IF](./deepfloyd_if)\\n- [Kandinsky 2.1](./kandinsky)\\n- [Kandinsky 2.2](./kandinsky_v22)\\n\\n\\n## AutoPipelineForText2Image\\n\\n[[autodoc]] AutoPipelineForText2Image\\n\\t- all\\n\\t- from_pretrained\\n\\t- from_pipe\\n\\n## AutoPipelineForImage2Image\\n\\n[[autodoc]] AutoPipelineForImage2Image\\n\\t- all\\n\\t- from_pretrained\\n\\t- from_pipe\\n\\n## AutoPipelineForInpainting\\n\\n[[autodoc]] AutoPipelineForInpainting\\n\\t- all\\n\\t- from_pretrained\\n\\t- from_pipe\\n'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Philosophy\\n\\n🧨 Diffusers provides **state-of-the-art** pretrained diffusion models across multiple modalities.\\nIts purpose is to serve as a **modular toolbox** for both inference and training.\\n\\nWe aim at building a library that stands the test of time and therefore take API design very seriously.\\n\\nIn a nutshell, Diffusers is built to be a natural extension of PyTorch. Therefore, most of our design choices are based on [PyTorch\\'s Design Principles](https://pytorch.org/docs/stable/community/design.html#pytorch-design-philosophy). Let\\'s go over the most important ones:\\n\\n## Usability over Performance\\n\\n- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.\\n- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.\\n\\n## Simple over easy\\n\\nAs PyTorch states, **explicit is better than implicit** and **simple is better than complex**. This design philosophy is reflected in multiple parts of the library:\\n- We follow PyTorch\\'s API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.to) to let the user handle device management.\\n- Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible.\\n- Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers.\\n- Separately trained components of the diffusion pipeline, *e.g.* the text encoder, the unet, and the variational autoencoder, each have their own model class. This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\\nis very simple thanks to Diffusers\\' ability to separate single components of the diffusion pipeline.\\n\\n## Tweakable, contributor-friendly over abstraction\\n\\nFor large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don\\'t repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.\\n**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:\\n- Machine Learning is an extremely fast-moving field in which paradigms, model architectures, and algorithms are changing rapidly, which therefore makes it very difficult to define long-lasting code abstractions.\\n- Machine Learning practitioners like to be able to quickly tweak existing code for ideation and research and therefore prefer self-contained code over one that contains many abstractions.\\n- Open-source libraries rely on community contributions and therefore must build a library that is easy to contribute to. The more abstract the code, the more dependencies, the harder to read, and the harder to contribute to. Contributors simply stop contributing to very abstract libraries out of fear of breaking vital functionality. If contributing to a library cannot break other fundamental code, not only is it more inviting for potential new contributors, but it is also easier to review and contribute to multiple parts in parallel.\\n\\nAt Hugging Face, we call this design the **single-file policy** which means that almost all of the code of a certain class should be written in a single, self-contained file. To read more about the philosophy, you can have a look\\nat [this blog post](https://huggingface.co/blog/transformers-design-philosophy).\\n\\nIn Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffusion models. The reason we don\\'t follow this design fully for diffusion models is because almost all diffusion pipelines, such\\nas [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm), [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview#stable-diffusion-pipelines), [unCLIP (DALL·E 2)](https://huggingface.co/docs/diffusers/api/pipelines/unclip) and [Imagen](https://imagen.research.google/) all rely on the same diffusion model, the [UNet](https://huggingface.co/docs/diffusers/api/models/unet2d-cond).\\n\\nGreat, now you should have generally understood why 🧨 Diffusers is designed the way it is 🤗.\\nWe try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would ❤️  to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).\\n\\n## Design Philosophy in Details\\n\\nNow, let\\'s look a bit into the nitty-gritty details of the design philosophy. Diffusers essentially consists of three major classes: [pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines), [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models), and [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\\nLet\\'s walk through more in-detail design decisions for each class.\\n\\n### Pipelines\\n\\nPipelines are designed to be easy to use (therefore do not follow [*Simple over easy*](#simple-over-easy) 100%), are not feature complete, and should loosely be seen as examples of how to use [models](#models) and [schedulers](#schedulers) for inference.\\n\\nThe following design principles are followed:\\n- Pipelines follow the single-file policy. All pipelines can be found in individual directories under src/diffusers/pipelines. One pipeline folder corresponds to one diffusion paper/project/release. Multiple pipeline files can be gathered in one pipeline folder, as it’s done for [`src/diffusers/pipelines/stable-diffusion`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion). If pipelines share similar functionality, one can make use of the [#Copied from mechanism](https://github.com/huggingface/diffusers/blob/125d783076e5bd9785beb05367a2d2566843a271/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L251).\\n- Pipelines all inherit from [`DiffusionPipeline`].\\n- Every pipeline consists of different model and scheduler components, that are documented in the [`model_index.json` file](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json), are accessible under the same name as attributes of the pipeline and can be shared between pipelines with [`DiffusionPipeline.components`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.components) function.\\n- Every pipeline should be loadable via the [`DiffusionPipeline.from_pretrained`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained) function.\\n- Pipelines should be used **only** for inference.\\n- Pipelines should be very readable, self-explanatory, and easy to tweak.\\n- Pipelines should be designed to build on top of each other and be easy to integrate into higher-level APIs.\\n- Pipelines are **not** intended to be feature-complete user interfaces. For future complete user interfaces one should rather have a look at [InvokeAI](https://github.com/invoke-ai/InvokeAI), [Diffuzers](https://github.com/abhishekkrthakur/diffuzers), and [lama-cleaner](https://github.com/Sanster/lama-cleaner).\\n- Every pipeline should have one and only one way to run it via a `__call__` method. The naming of the `__call__` arguments should be shared across all pipelines.\\n- Pipelines should be named after the task they are intended to solve.\\n- In almost all cases, novel diffusion pipelines shall be implemented in a new pipeline folder/file.\\n\\n### Models\\n\\nModels are designed as configurable toolboxes that are natural extensions of [PyTorch\\'s Module class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). They only partly follow the **single-file policy**.\\n\\nThe following design principles are followed:\\n- Models correspond to **a type of model architecture**. *E.g.* the [`UNet2DConditionModel`] class is used for all UNet variations that expect 2D image inputs and are conditioned on some context.\\n- All models can be found in [`src/diffusers/models`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models) and every model architecture shall be defined in its file, e.g. [`unet_2d_condition.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py), [`transformer_2d.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformer_2d.py), etc...\\n- Models **do not** follow the single-file policy and should make use of smaller model building blocks, such as [`attention.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py), [`resnet.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/resnet.py), [`embeddings.py`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/embeddings.py), etc... **Note**: This is in stark contrast to Transformers\\' modeling files and shows that models do not really follow the single-file policy.\\n- Models intend to expose complexity, just like PyTorch\\'s `Module` class, and give clear error messages.\\n- Models all inherit from `ModelMixin` and `ConfigMixin`.\\n- Models can be optimized for performance when it doesn’t demand major code changes, keeps backward compatibility, and gives significant memory or compute gain.\\n- Models should by default have the highest precision and lowest performance setting.\\n- To integrate new model checkpoints whose general architecture can be classified as an architecture that already exists in Diffusers, the existing model architecture shall be adapted to make it work with the new checkpoint. One should only create a new file if the model architecture is fundamentally different.\\n- Models should be designed to be easily extendable to future changes. This can be achieved by limiting public function arguments, configuration arguments, and \"foreseeing\" future changes, *e.g.* it is usually better to add `string` \"...type\" arguments that can easily be extended to new future types instead of boolean `is_..._type` arguments. Only the minimum amount of changes shall be made to existing architectures to make a new model checkpoint work.\\n- The model design is a difficult trade-off between keeping code readable and concise and supporting many model checkpoints. For most parts of the modeling code, classes shall be adapted for new model checkpoints, while there are some exceptions where it is preferred to add new classes to make sure the code is kept concise and\\nreadable long-term, such as [UNet blocks](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_blocks.py) and [Attention processors](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\\n\\n### Schedulers\\n\\nSchedulers are responsible to guide the denoising process for inference as well as to define a noise schedule for training. They are designed as individual classes with loadable configuration files and strongly follow the **single-file policy**.\\n\\nThe following design principles are followed:\\n- All schedulers are found in [`src/diffusers/schedulers`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\\n- Schedulers are **not** allowed to import from large utils files and shall be kept very self-contained.\\n- One scheduler Python file corresponds to one scheduler algorithm (as might be defined in a paper).\\n- If schedulers share similar functionalities, we can make use of the `#Copied from` mechanism.\\n- Schedulers all inherit from `SchedulerMixin` and `ConfigMixin`.\\n- Schedulers can be easily swapped out with the [`ConfigMixin.from_config`](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config) method as explained in detail [here](../using-diffusers/schedulers.md).\\n- Every scheduler has to have a `set_num_inference_steps`, and a `step` function. `set_num_inference_steps(...)` has to be called before every denoising process, *i.e.* before `step(...)` is called.\\n- Every scheduler exposes the timesteps to be \"looped over\" via a `timesteps` attribute, which is an array of timesteps the model will be called upon.\\n- The `step(...)` function takes a predicted model output and the \"current\" sample (x_t) and returns the \"previous\", slightly more denoised sample (x_t-1).\\n- Given the complexity of diffusion schedulers, the `step` function does not expose all the complexity and can be a bit of a \"black box\".\\n- In almost all cases, novel schedulers shall be implemented in a new scheduling file.\\n'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# EulerAncestralDiscreteScheduler\\n\\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\\n\\n## EulerAncestralDiscreteScheduler\\n[[autodoc]] EulerAncestralDiscreteScheduler\\n\\n## EulerAncestralDiscreteSchedulerOutput\\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput\\n'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Image tasks with IDEFICS\\n\\n[[open-in-colab]]\\n\\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach \\nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. \\nFor instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. \\nThis approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can \\nsolve image-text tasks with a large multimodal model called IDEFICS. \\n\\n[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198), \\na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image \\nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content, \\ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b) \\nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the 🤗 Hub. For each variant, you can also find fine-tuned instructed \\nversions of the model adapted for conversational use cases.\\n\\nThis model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However, \\nbeing a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether \\nthis approach suits your use case better than fine-tuning specialized models for each individual task. \\n\\nIn this guide, you\\'ll learn how to: \\n- [Load IDEFICS](#loading-the-model) and [load the quantized version of the model](#loading-the-quantized-version-of-the-model)\\n- Use IDEFICS for: \\n  - [Image captioning](#image-captioning)\\n  - [Prompted image captioning](#prompted-image-captioning)\\n  - [Few-shot prompting](#few-shot-prompting)\\n  - [Visual question answering](#visual-question-answering)\\n  - [Image classificaiton](#image-classification)\\n  - [Image-guided text generation](#image-guided-text-generation)\\n- [Run inference in batch mode](#running-inference-in-batch-mode)\\n- [Run IDEFICS instruct for conversational use](#idefics-instruct-for-conversational-use)\\n\\nBefore you begin, make sure you have all the necessary libraries installed. \\n\\n```bash\\npip install -q bitsandbytes sentencepiece accelerate transformers\\n```\\n\\n<Tip>\\nTo run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of GPU memory.\\n</Tip>\\n\\n## Loading the model\\n\\nLet\\'s start by loading the model\\'s 9 billion parameters checkpoint: \\n\\n```py\\n>>> checkpoint = \"HuggingFaceM4/idefics-9b\"\\n```\\n\\nJust like for other Transformers models, you need to load a processor and the model itself from the checkpoint. \\nThe IDEFICS processor wraps a [`LlamaTokenizer`] and IDEFICS image processor into a single processor to take care of \\npreparing text and image inputs for the model.\\n\\n```py\\n>>> import torch\\n\\n>>> from transformers import IdeficsForVisionText2Text, AutoProcessor\\n\\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\\n\\n>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\\n```\\n\\nSetting `device_map` to `\"auto\"` will automatically determine how to load and store the model weights in the most optimized \\nmanner given existing devices.\\n\\n### Quantized model\\n\\nIf high-memory GPU availability is an issue, you can load the quantized version of the model. To load the model and the \\nprocessor in 4bit precision, pass a `BitsAndBytesConfig` to the `from_pretrained` method and the model will be compressed \\non the fly while loading.\\n\\n```py\\n>>> import torch\\n>>> from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig\\n\\n>>> quantization_config = BitsAndBytesConfig(\\n...     load_in_4bit=True,\\n...     bnb_4bit_compute_dtype=torch.float16,\\n... )\\n\\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\\n\\n>>> model = IdeficsForVisionText2Text.from_pretrained(\\n...     checkpoint,\\n...     quantization_config=quantization_config,\\n...     device_map=\"auto\"\\n... )\\n```\\n\\nNow that you have the model loaded in one of the suggested ways, let\\'s move on to exploring tasks that you can use IDEFICS for.\\n\\n## Image captioning\\nImage captioning is the task of predicting a caption for a given image. A common application is to aid visually impaired \\npeople navigate through different situations, for instance, explore image content online. \\n\\nTo illustrate the task, get an image to be captioned, e.g.:\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-im-captioning.jpg\" alt=\"Image of a puppy in a flower bed\"/>\\n</div>\\n\\nPhoto by [Hendo Wang](https://unsplash.com/@hendoo). \\n\\nIDEFICS accepts text and image prompts. However, to caption an image, you do not have to provide a text prompt to the \\nmodel, only the preprocessed input image. Without a text prompt, the model will start generating text from the \\nBOS (beginning-of-sequence) token thus creating a caption.\\n\\nAs image input to the model, you can use either an image object (`PIL.Image`) or a url from which the image can be retrieved.\\n\\n```py\\n>>> prompt = [\\n...     \"https://images.unsplash.com/photo-1583160247711-2191776b4b91?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3542&q=80\",\\n... ]\\n\\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\\n\\n>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\\n>>> print(generated_text[0])\\nA puppy in a flower bed\\n```\\n\\n<Tip>\\n\\nIt is a good idea to include the `bad_words_ids` in the call to `generate` to avoid errors arising when increasing \\nthe `max_new_tokens`: the model will want to generate a new `<image>` or `<fake_token_around_image>` token when there \\nis no image being generated by the model.\\nYou can set it on-the-fly as in this guide, or store in the `GenerationConfig` as described in the [Text generation strategies](../generation_strategies) guide.\\n</Tip>\\n\\n## Prompted image captioning\\n\\nYou can extend image captioning by providing a text prompt, which the model will continue given the image. Let\\'s take \\nanother image to illustrate:\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-prompted-im-captioning.jpg\" alt=\"Image of the Eiffel Tower at night\"/>\\n</div>\\n\\nPhoto by [Denys Nevozhai](https://unsplash.com/@dnevozhai).\\n   \\nTextual and image prompts can be passed to the model\\'s processor as a single list to create appropriate inputs.\\n\\n```py\\n>>> prompt = [\\n...     \"https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80\",\\n...     \"This is an image of \",\\n... ]\\n\\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\\n\\n>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\\n>>> print(generated_text[0])\\nThis is an image of the Eiffel Tower in Paris, France.\\n```\\n\\n## Few-shot prompting\\n\\nWhile IDEFICS demonstrates great zero-shot results, your task may require a certain format of the caption, or come with \\nother restrictions or requirements that increase task\\'s complexity. Few-shot prompting can be used to enable in-context learning.\\nBy providing examples in the prompt, you can steer the model to generate results that mimic the format of given examples. \\n\\nLet\\'s use the previous image of the Eiffel Tower as an example for the model and build a prompt that demonstrates to the model \\nthat in addition to learning what the object in an image is, we would also like to get some interesting information about it. \\nThen, let\\'s see, if we can get the same response format for an image of the Statue of Liberty:\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\" alt=\"Image of the Statue of Liberty\"/>\\n</div>\\n\\nPhoto by [Juan Mayobre](https://unsplash.com/@jmayobres).\\n  \\n```py\\n>>> prompt = [\"User:\",\\n...            \"https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80\",\\n...            \"Describe this image.\\\\nAssistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building.\\\\n\",\\n...            \"User:\",\\n...            \"https://images.unsplash.com/photo-1524099163253-32b7f0256868?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3387&q=80\",\\n...            \"Describe this image.\\\\nAssistant:\"\\n...            ]\\n\\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\\n\\n>>> generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)\\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\\n>>> print(generated_text[0])\\nUser: Describe this image.\\nAssistant: An image of the Eiffel Tower at night. Fun fact: the Eiffel Tower is the same height as an 81-storey building. \\nUser: Describe this image.\\nAssistant: An image of the Statue of Liberty. Fun fact: the Statue of Liberty is 151 feet tall.\\n```\\n\\nNotice that just from a single example (i.e., 1-shot) the model has learned how to perform the task. For more complex tasks, \\nfeel free to experiment with a larger number of examples (e.g., 3-shot, 5-shot, etc.).\\n\\n## Visual question answering\\n\\nVisual Question Answering (VQA) is the task of answering open-ended questions based on an image. Similar to image \\ncaptioning it can be used in accessibility applications, but also in education (reasoning about visual materials), customer \\nservice (questions about products based on images), and image retrieval.\\n\\nLet\\'s get a new image for this task: \\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-vqa.jpg\" alt=\"Image of a couple having a picnic\"/>\\n</div>\\n\\nPhoto by [Jarritos Mexican Soda](https://unsplash.com/@jarritos). \\n\\nYou can steer the model from image captioning to visual question answering by prompting it with appropriate instructions: \\n\\n```py\\n>>> prompt = [\\n...     \"Instruction: Provide an answer to the question. Use the image to answer.\\\\n\",\\n...     \"https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\\n...     \"Question: Where are these people and what\\'s the weather like? Answer:\"\\n... ]\\n\\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\\n\\n>>> generated_ids = model.generate(**inputs, max_new_tokens=20, bad_words_ids=bad_words_ids)\\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\\n>>> print(generated_text[0])\\nInstruction: Provide an answer to the question. Use the image to answer.\\n Question: Where are these people and what\\'s the weather like? Answer: They\\'re in a park in New York City, and it\\'s a beautiful day.\\n```\\n\\n## Image classification\\n\\nIDEFICS is capable of classifying images into different categories without being explicitly trained on data containing \\nlabeled examples from those specific categories. Given a list of categories and using its image and text understanding \\ncapabilities, the model can infer which category the image likely belongs to. \\n\\nSay, we have this image of a vegetable stand: \\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-classification.jpg\" alt=\"Image of a vegetable stand\"/>\\n</div>\\n\\nPhoto by [Peter Wendt](https://unsplash.com/@peterwendt).\\n\\nWe can instruct the model to classify the image into one of the categories that we have:\\n\\n```py\\n>>> categories = [\\'animals\\',\\'vegetables\\', \\'city landscape\\', \\'cars\\', \\'office\\']\\n>>> prompt = [f\"Instruction: Classify the following image into a single category from the following list: {categories}.\\\\n\",\\n...     \"https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",    \\n...     \"Category: \"\\n... ]\\n\\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\\n\\n>>> generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)\\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\\n>>> print(generated_text[0])\\nInstruction: Classify the following image into a single category from the following list: [\\'animals\\', \\'vegetables\\', \\'city landscape\\', \\'cars\\', \\'office\\'].\\nCategory: Vegetables\\n```  \\n\\nIn the example above we instruct the model to classify the image into a single category, however, you can also prompt the model to do rank classification.\\n\\n## Image-guided text generation\\n\\nFor more creative applications, you can use image-guided text generation to generate text based on an image. This can be \\nuseful to create descriptions of products, ads, descriptions of a scene, etc. \\n\\nLet\\'s prompt IDEFICS to write a story based on a simple image of a red door: \\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-story-generation.jpg\" alt=\"Image of a red door with a pumpkin on the steps\"/>\\n</div>\\n\\nPhoto by [Craig Tidball](https://unsplash.com/@devonshiremedia).\\n  \\n```py\\n>>> prompt = [\"Instruction: Use the image to write a story. \\\\n\",\\n...     \"https://images.unsplash.com/photo-1517086822157-2b0358e7684a?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2203&q=80\",\\n...     \"Story: \\\\n\"]\\n\\n>>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\\n\\n>>> generated_ids = model.generate(**inputs, num_beams=2, max_new_tokens=200, bad_words_ids=bad_words_ids)\\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\\n>>> print(generated_text[0]) \\nInstruction: Use the image to write a story. \\n Story: \\nOnce upon a time, there was a little girl who lived in a house with a red door.  She loved her red door.  It was the prettiest door in the whole world.\\n\\nOne day, the little girl was playing in her yard when she noticed a man standing on her doorstep.  He was wearing a long black coat and a top hat.\\n\\nThe little girl ran inside and told her mother about the man.\\n\\nHer mother said, “Don’t worry, honey.  He’s just a friendly ghost.”\\n\\nThe little girl wasn’t sure if she believed her mother, but she went outside anyway.\\n\\nWhen she got to the door, the man was gone.\\n\\nThe next day, the little girl was playing in her yard again when she noticed the man standing on her doorstep.\\n\\nHe was wearing a long black coat and a top hat.\\n\\nThe little girl ran\\n```\\n\\nLooks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky Halloween story about a ghost.\\n\\n<Tip>\\n\\nFor longer outputs like this, you will greatly benefit from tweaking the text generation strategy. This can help \\nyou significantly improve the quality of the generated output. Check out [Text generation strategies](../generation_strategies) \\nto learn more. \\n</Tip>\\n\\n## Running inference in batch mode\\n\\nAll of the earlier sections illustrated IDEFICS for a single example. In a very similar fashion, you can run inference \\nfor a batch of examples by passing a list of prompts:\\n\\n```py\\n>>> prompts = [\\n...     [   \"https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80\",\\n...         \"This is an image of \",\\n...     ],\\n...     [   \"https://images.unsplash.com/photo-1623944889288-cd147dbb517c?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\\n...         \"This is an image of \",\\n...     ],\\n...     [   \"https://images.unsplash.com/photo-1471193945509-9ad0617afabf?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3540&q=80\",\\n...         \"This is an image of \",\\n...     ],\\n... ]\\n\\n>>> inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")\\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\\n\\n>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\\n>>> for i,t in enumerate(generated_text):\\n...     print(f\"{i}:\\\\n{t}\\\\n\") \\n0:\\nThis is an image of the Eiffel Tower in Paris, France.\\n\\n1:\\nThis is an image of a couple on a picnic blanket.\\n\\n2:\\nThis is an image of a vegetable stand.\\n```\\n\\n## IDEFICS instruct for conversational use\\n\\nFor conversational use cases, you can find fine-tuned instructed versions of the model on the 🤗 Hub: \\n`HuggingFaceM4/idefics-80b-instruct` and `HuggingFaceM4/idefics-9b-instruct`.\\n\\nThese checkpoints are the result of fine-tuning the respective base models on a mixture of supervised and instruction \\nfine-tuning datasets, which boosts the downstream performance while making the models more usable in conversational settings.\\n\\nThe use and prompting for the conversational use is very similar to using the base models: \\n\\n```py\\n>>> import torch\\n>>> from transformers import IdeficsForVisionText2Text, AutoProcessor\\n\\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n>>> checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\\n>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\\n\\n>>> prompts = [\\n...     [\\n...         \"User: What is in this image?\",\\n...         \"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\\n...         \"<end_of_utterance>\",\\n\\n...         \"\\\\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>\",\\n\\n...         \"\\\\nUser:\",\\n...         \"https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052\",\\n...         \"And who is that?<end_of_utterance>\",\\n\\n...         \"\\\\nAssistant:\",\\n...     ],\\n... ]\\n\\n>>> # --batched mode\\n>>> inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=\"pt\").to(device)\\n>>> # --single sample mode\\n>>> # inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\\n\\n>>> # Generation args\\n>>> exit_condition = processor.tokenizer(\"<end_of_utterance>\", add_special_tokens=False).input_ids\\n>>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\\n\\n>>> generated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=100)\\n>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\\n>>> for i, t in enumerate(generated_text):\\n...     print(f\"{i}:\\\\n{t}\\\\n\")\\n```\\n'), Document(metadata={}, page_content=' Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\\n\\n<CourseFloatingBanner chapter={9}\\n  classNames=\"absolute z-10 right-0 top-0\"\\n  notebooks={[\\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb\"},\\n]} />\\n\\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`.\\n\\nNow, what\\'s the difference between `Interface` and `Blocks`?\\n\\n- ⚡ `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.\\n\\n- 🧱 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\").\\n\\n\\n### Why Blocks 🧱?[[why-blocks-]]\\n\\nAs we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to:\\n\\n- Group together related demos as multiple tabs in one web application\\n- Change the layout of your demo, e.g. to specify where the inputs and outputs are located\\n- Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\\n- Change a component\\'s properties (for example, the choices in a dropdown) or its visibility based on user input\\n\\nWe will explore all of these concepts below.\\n\\n### Creating a simple demo using Blocks[[creating-a-simple-demo-using-blocks]]\\n\\nAfter you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook.\\n\\n```py\\nimport gradio as gr\\n\\n\\ndef flip_text(x):\\n    return x[::-1]\\n\\n\\ndemo = gr.Blocks()\\n\\nwith demo:\\n    gr.Markdown(\\n        \"\"\"\\n    # Flip Text!\\n    Start typing below to see the output.\\n    \"\"\"\\n    )\\n    input = gr.Textbox(placeholder=\"Flip this text\")\\n    output = gr.Textbox()\\n\\n    input.change(fn=flip_text, inputs=input, outputs=output)\\n\\ndemo.launch()\\n```\\n\\n<iframe src=\"https://course-demos-flip-text.hf.space\" frameBorder=\"0\" height=\"400\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\\n\\nThis simple example above introduces 4 concepts that underlie Blocks:\\n\\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\\n<Tip>\\n🙋If you\\'re not familiar with the `with` statement in Python, we recommend checking out the excellent [tutorial](https://realpython.com/python-with-statement/) from Real Python. Come back here after reading that 🤗\\n</Tip>\\nThe order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below)\\n\\n2. You can define regular Python functions anywhere in your code and run them with user input using `Blocks`. In our example, we have a simple function that \"flips\" the input text, but you can write any Python function, from a simple calculation to processing the predictions from a machine learning model.\\n\\n3. You can assign events to any `Blocks` component. This will run your function when the component is clicked, changed, etc. When you assign an event, you pass in three parameters: `fn`: the function that should be called, `inputs`: the (list) of input component(s), and `outputs`: the (list) of output components that should be called.\\n\\n   In the example above, we run the `flip_text()` function when the value in the `Textbox` named input `input` changes. The event reads the value in `input`, passes it as the name parameter to `flip_text()`, which then returns a value that gets assigned to our second `Textbox` named `output`.\\n\\n   To see a list of events that each component supports, see the Gradio [documentation](https://www.gradio.app/docs/).\\n\\n4. Blocks automatically figures out whether a component should be interactive (accept user input) or not, based on the event triggers you define. In our example, the first textbox is interactive, since its value is used by the `flip_text()` function. The second textbox is not interactive, since its value is never used as an input. In some cases, you might want to override this, which you can do by passing a boolean to the `interactive` parameter of the component (e.g. `gr.Textbox(placeholder=\"Flip this text\", interactive=True)`).\\n\\n### Customizing the layout of your demo[[customizing-the-layout-of-your-demo]]\\n\\nHow can we use `Blocks` to customize the layout of our demo? By default, `Blocks` renders the components that you create vertically in one column. You can change that by creating additional columns `with gradio.Column():` or rows `with gradio.Row():` and creating components within those contexts.\\n\\nHere\\'s what you should keep in mind: any components created under a `Column` (this is also the default) will be laid out vertically. Any component created under a `Row` will be laid out horizontally, similar to the [flexbox model in web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).\\n\\nFinally, you can also create tabs for your demo by using the `with gradio.Tabs()` context manager. Within this context, you can create multiple tabs by specifying `with gradio.TabItem(name_of_tab):` children. Any component created inside of a `with gradio.TabItem(name_of_tab):` context appears in that tab.\\n\\nNow let\\'s add a `flip_image()` function to our demo and add a new tab that flips images. Below is an example with 2 tabs and also uses a Row:\\n\\n```py\\nimport numpy as np\\nimport gradio as gr\\n\\ndemo = gr.Blocks()\\n\\n\\ndef flip_text(x):\\n    return x[::-1]\\n\\n\\ndef flip_image(x):\\n    return np.fliplr(x)\\n\\n\\nwith demo:\\n    gr.Markdown(\"Flip text or image files using this demo.\")\\n    with gr.Tabs():\\n        with gr.TabItem(\"Flip Text\"):\\n            with gr.Row():\\n                text_input = gr.Textbox()\\n                text_output = gr.Textbox()\\n            text_button = gr.Button(\"Flip\")\\n        with gr.TabItem(\"Flip Image\"):\\n            with gr.Row():\\n                image_input = gr.Image()\\n                image_output = gr.Image()\\n            image_button = gr.Button(\"Flip\")\\n\\n    text_button.click(flip_text, inputs=text_input, outputs=text_output)\\n    image_button.click(flip_image, inputs=image_input, outputs=image_output)\\n\\ndemo.launch()\\n```\\n\\n<iframe src=\"https://course-demos-flip-text-image.hf.space\" frameBorder=\"0\" height=\"450\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\\n\\n\\nYou\\'ll notice that in this example, we\\'ve also created a `Button` component in each tab, and we\\'ve assigned a click event to each button, which is what actually runs the function.\\n\\n### Exploring events and state[[exploring-events-and-state]]\\n\\nJust as you can control the layout, `Blocks` gives you fine-grained control over what events trigger function calls. Each component and many layouts have specific events that they support.\\n\\nFor example, the `Textbox` component has 2 events: `change()` (when the value inside of the textbox changes), and `submit()` (when a user presses the enter key while focused on the textbox). More complex components can have even more events: for example, the `Audio` component also has separate events for when the audio file is played, cleared, paused, etc. See the documentation for the events each component supports.\\n\\nYou can attach event trigger to none, one, or more of these events. You create an event trigger by calling the name of the event on the component instance as a function -- e.g. `textbox.change(...)` or `btn.click(...)`. The function takes in three parameters, as discussed above:\\n\\n- `fn`: the function to run\\n- `inputs`: a (list of) component(s) whose values should supplied as the input parameters to the function. Each component\\'s value gets mapped to the corresponding function parameter, in order. This parameter can be None if the function does not take any parameters.\\n- `outputs`: a (list of) component(s) whose values should be updated based on the values returned by the function. Each return value sets the corresponding component\\'s value, in order. This parameter can be None if the function does not return anything.\\n\\nYou can even make the input and output component be the same component, as we do in this example that uses a GPT model to do text completion:\\n\\n```py\\nimport gradio as gr\\n\\napi = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\")\\n\\n\\ndef complete_with_gpt(text):\\n    # Use the last 50 characters of the text as context\\n    return text[:-50] + api(text[-50:])\\n\\n\\nwith gr.Blocks() as demo:\\n    textbox = gr.Textbox(placeholder=\"Type here and press enter...\", lines=4)\\n    btn = gr.Button(\"Generate\")\\n\\n    btn.click(complete_with_gpt, textbox, textbox)\\n\\ndemo.launch()\\n```\\n\\n<iframe src=\"https://course-demos-blocks-gpt.hf.space\" frameBorder=\"0\" height=\"300\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\\n\\n### Creating multi-step demos[[creating-multi-step-demos]]\\n\\nIn some cases, you might want a _multi-step demo_, in which you reuse the output of one function as the input to the next. This is really easy to do with `Blocks`, as you can use a component for the input of one event trigger but the output of another. Take a look at the text component in the example below, its value is the result of a speech-to-text model, but also gets passed into a sentiment analysis model:\\n\\n```py\\nfrom transformers import pipeline\\n\\nimport gradio as gr\\n\\nasr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\\nclassifier = pipeline(\"text-classification\")\\n\\n\\ndef speech_to_text(speech):\\n    text = asr(speech)[\"text\"]\\n    return text\\n\\n\\ndef text_to_sentiment(text):\\n    return classifier(text)[0][\"label\"]\\n\\n\\ndemo = gr.Blocks()\\n\\nwith demo:\\n    audio_file = gr.Audio(type=\"filepath\")\\n    text = gr.Textbox()\\n    label = gr.Label()\\n\\n    b1 = gr.Button(\"Recognize Speech\")\\n    b2 = gr.Button(\"Classify Sentiment\")\\n\\n    b1.click(speech_to_text, inputs=audio_file, outputs=text)\\n    b2.click(text_to_sentiment, inputs=text, outputs=label)\\n\\ndemo.launch()\\n```\\n\\n<iframe src=\"https://course-demos-blocks-multi-step.hf.space\" frameBorder=\"0\" height=\"600\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\\n\\n### Updating Component Properties[[updating-component-properties]]\\n\\nSo far, we have seen how to create events to update the value of another component. But what happens if you want to change other properties of a component, like the visibility of a textbox or the choices in a radio button group? You can do this by returning a component class\\'s `update()` method instead of a regular return value from your function.\\n\\nThis is most easily illustrated with an example:\\n\\n```py\\nimport gradio as gr\\n\\n\\ndef change_textbox(choice):\\n    if choice == \"short\":\\n        return gr.Textbox.update(lines=2, visible=True)\\n    elif choice == \"long\":\\n        return gr.Textbox.update(lines=8, visible=True)\\n    else:\\n        return gr.Textbox.update(visible=False)\\n\\n\\nwith gr.Blocks() as block:\\n    radio = gr.Radio(\\n        [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\"\\n    )\\n    text = gr.Textbox(lines=2, interactive=True)\\n\\n    radio.change(fn=change_textbox, inputs=radio, outputs=text)\\n    block.launch()\\n```\\n\\n<iframe src=\"https://course-demos-blocks-update-component-properties.hf.space\" frameBorder=\"0\" height=\"300\" title=\"Gradio app\" class=\"container p-0 flex-grow space-iframe\" allow=\"accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking\" sandbox=\"allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads\"></iframe>\\n\\nWe just explored all the core concepts of `Blocks`! Just like with `Interfaces`, you can create cool demos that can be shared by using `share=True` in the `launch()` method or deployed on [Hugging Face Spaces](https://huggingface.co/spaces).'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# unCLIP\\n\\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in 🤗 Diffusers comes from kakaobrain\\'s [karlo](https://github.com/kakaobrain/karlo).\\n\\nThe abstract from the paper is following:\\n\\n*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*\\n\\nYou can find lucidrains\\' DALL-E 2 recreation at [lucidrains/DALLE2-pytorch](https://github.com/lucidrains/DALLE2-pytorch).\\n\\n<Tip>\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## UnCLIPPipeline\\n[[autodoc]] UnCLIPPipeline\\n\\t- all\\n\\t- __call__\\n\\n## UnCLIPImageVariationPipeline\\n[[autodoc]] UnCLIPImageVariationPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ImagePipelineOutput\\n[[autodoc]] pipelines.ImagePipelineOutput\\n'), Document(metadata={}, page_content='!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# Research projects\\n\\nThis folder contains various research projects using 🤗 Transformers. They are not maintained and require a specific\\nversion of 🤗 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.\\n\\nTo use any of them, just run the command\\n```\\npip install -r requirements.txt\\n```\\ninside the folder of your choice.\\n\\nIf you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.\\n'), Document(metadata={}, page_content='!-- DISABLE-FRONTMATTER-SECTIONS -->\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n<CourseFloatingBanner\\n    chapter={1}\\n    classNames=\"absolute z-10 right-0 top-0\"\\n/>\\n\\nThis chapter covered a lot of ground! Don\\'t worry if you didn\\'t grasp all the details; the next chapters will help you understand how things work under the hood.\\n\\nFirst, though, let\\'s test what you learned in this chapter!\\n\\n\\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?\\n\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"Summarization\",\\n\\t\\t\\texplain: \"Look again on the <a href=\\\\\"https://huggingface.co/roberta-large-mnli\\\\\">roberta-large-mnli page</a>.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"Text classification\",\\n\\t\\t\\texplain: \"More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) — a task also called <em>natural language inference</em>.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"Text generation\",\\n\\t\\t\\texplain: \"Look again on the <a href=\\\\\"https://huggingface.co/roberta-large-mnli\\\\\">roberta-large-mnli page</a>.\"\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 2. What will the following code return?\\n\\n```py\\nfrom transformers import pipeline\\n\\nner = pipeline(\"ner\", grouped_entities=True)\\nner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\\n```\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"It will return classification scores for this sentence, with labels \\\\\"positive\\\\\" or \\\\\"negative\\\\\".\",\\n\\t\\t\\texplain: \"This is incorrect — this would be a <code>sentiment-analysis</code> pipeline.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"It will return a generated text completing this sentence.\",\\n\\t\\t\\texplain: \"This is incorrect — it would be a <code>text-generation</code> pipeline.\",\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"It will return the words representing persons, organizations or locations.\",\\n\\t\\t\\texplain: \"Furthermore, with <code>grouped_entities=True</code>, it will group together the words belonging to the same entity, like \\\\\"Hugging Face\\\\\".\",\\n\\t\\t\\tcorrect: true\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 3. What should replace ... in this code sample?\\n\\n```py\\nfrom transformers import pipeline\\n\\nfiller = pipeline(\"fill-mask\", model=\"bert-base-cased\")\\nresult = filler(\"...\")\\n```\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"This &#60;mask> has been waiting for you.\",\\n\\t\\t\\texplain: \"This is incorrect. Check out the <code>bert-base-cased</code> model card and try to spot your mistake.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"This [MASK] has been waiting for you.\",\\n\\t\\t\\texplain: \"Correct! This model\\'s mask token is [MASK].\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"This man has been waiting for you.\",\\n\\t\\t\\texplain: \"This is incorrect. This pipeline fills in masked words, so it needs a mask token somewhere.\"\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 4. Why will this code fail?\\n\\n```py\\nfrom transformers import pipeline\\n\\nclassifier = pipeline(\"zero-shot-classification\")\\nresult = classifier(\"This is a course about the Transformers library\")\\n```\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"This pipeline requires that labels be given to classify this text.\",\\n\\t\\t\\texplain: \"Right — the correct code needs to include <code>candidate_labels=[...]</code>.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"This pipeline requires several sentences, not just one.\",\\n\\t\\t\\texplain: \"This is incorrect, though when properly used, this pipeline can take a list of sentences to process (like all other pipelines).\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"The 🤗 Transformers library is broken, as usual.\",\\n\\t\\t\\texplain: \"We won\\'t dignify this answer with a comment!\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"This pipeline requires longer inputs; this one is too short.\",\\n\\t\\t\\texplain: \"This is incorrect. Note that a very long text will be truncated when processed by this pipeline.\"\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 5. What does \"transfer learning\" mean?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"Transferring the knowledge of a pretrained model to a new model by training it on the same dataset.\",\\n\\t\\t\\texplain: \"No, that would be two versions of the same model.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model\\'s weights.\",\\n\\t\\t\\texplain: \"Correct: when the second model is trained on a new task, it *transfers* the knowledge of the first model.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"Transferring the knowledge of a pretrained model to a new model by building the second model with the same architecture as the first model.\",\\n\\t\\t\\texplain: \"The architecture is just the way the model is built; there is no knowledge shared or transferred in this case.\"\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 6. True or false? A language model usually does not need labels for its pretraining.\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"True\",\\n\\t\\t\\texplain: \"The pretraining is usually <em>self-supervised</em>, which means the labels are created automatically from the inputs (like predicting the next word or filling in some masked words).\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"False\",\\n\\t\\t\\texplain: \"This is not the correct answer.\"\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 7. Select the sentence that best describes the terms \"model\", \"architecture\", and \"weights\".\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"If a model is a building, its architecture is the blueprint and the weights are the people living inside.\",\\n\\t\\t\\texplain: \"Following this metaphor, the weights would be the bricks and other materials used to construct the building.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"An architecture is a map to build a model and its weights are the cities represented on the map.\",\\n\\t\\t\\texplain: \"The problem with this metaphor is that a map usually represents one existing reality (there is only one city in France named Paris). For a given architecture, multiple weights are possible.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters.\",\\n\\t\\t\\texplain: \"The same set of mathematical functions (architecture) can be used to build different models by using different parameters (weights).\",\\n\\t\\t\\tcorrect: true\\n\\t\\t}\\n\\t]}\\n/>\\n\\n\\n### 8. Which of these types of models would you use for completing prompts with generated text?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"An encoder model\",\\n\\t\\t\\texplain: \"An encoder model generates a representation of the whole sentence that is better suited for tasks like classification.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"A decoder model\",\\n\\t\\t\\texplain: \"Decoder models are perfectly suited for text generation from a prompt.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"A sequence-to-sequence model\",\\n\\t\\t\\texplain: \"Sequence-to-sequence models are better suited for tasks where you want to generate sentences in relation to the input sentences, not a given prompt.\"\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 9. Which of those types of models would you use for summarizing texts?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"An encoder model\",\\n\\t\\t\\texplain: \"An encoder model generates a representation of the whole sentence that is better suited for tasks like classification.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"A decoder model\",\\n\\t\\t\\texplain: \"Decoder models are good for generating output text (like summaries), but they don\\'t have the ability to exploit a context like the whole text to summarize.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"A sequence-to-sequence model\",\\n\\t\\t\\texplain: \"Sequence-to-sequence models are perfectly suited for a summarization task.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 10. Which of these types of models would you use for classifying text inputs according to certain labels?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"An encoder model\",\\n\\t\\t\\texplain: \"An encoder model generates a representation of the whole sentence which is perfectly suited for a task like classification.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"A decoder model\",\\n\\t\\t\\texplain: \"Decoder models are good for generating output texts, not extracting a label out of a sentence.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"A sequence-to-sequence model\",\\n\\t\\t\\texplain: \"Sequence-to-sequence models are better suited for tasks where you want to generate text based on an input sentence, not a label.\",\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 11. What possible source can the bias observed in a model have?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"The model is a fine-tuned version of a pretrained model and it picked up its bias from it.\",\\n\\t\\t\\texplain: \"When applying Transfer Learning, the bias in the pretrained model used persists in the fine-tuned model.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"The data the model was trained on is biased.\",\\n\\t\\t\\texplain: \"This is the most obvious source of bias, but not the only one.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"The metric the model was optimizing for is biased.\",\\n\\t\\t\\texplain: \"A less obvious source of bias is the way the model is trained. Your model will blindly optimize for whatever metric you chose, without any second thoughts.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t}\\n\\t]}\\n/>\\n'), Document(metadata={}, page_content=\"--\\ntitle: Introducing our new pricing\\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\\nauthors:\\n- user: sbrandeis\\n- user: pierric\\n---\\n\\n# Introducing our new pricing\\n\\n\\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\\n\\nFirst of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).\\n\\nAlong with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations).\\n\\nYour billing settings centralize everything about our paid services. From there, you can manage your personal PRO subscription, update your payment method, and visualize your usage for the past three months. Usage for all our paid services and subscriptions will be charged at the start of each month, and a consolidated invoice will be available for your records.\\n\\n**TL;DR**: **At HF we monetize by providing simple access to compute for AI**, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub. [Read more](https://huggingface.co/docs/hub/billing) about our pricing and billing system.\\n\\nIf you have any questions, feel free to reach out. We welcome your feedback 🔥\\n\\n\"), Document(metadata={}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# SqueezeBERT\\n\\n## Overview\\n\\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It\\'s a\\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\\ninstead of fully-connected layers for the Q, K, V and FFN layers.\\n\\nThe abstract from the paper is the following:\\n\\n*Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets,\\nlarge computing systems, and better neural network models, natural language processing (NLP) technology has made\\nsignificant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant\\nopportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. In particular, we\\nconsider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today\\'s\\nhighly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods\\nsuch as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\\nset. The SqueezeBERT code will be released.*\\n\\nThis model was contributed by [forresti](https://huggingface.co/forresti).\\n\\n## Usage tips\\n\\n- SqueezeBERT is a model with absolute position embeddings so it\\'s usually advised to pad the inputs on the right\\n  rather than the left.\\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\\n  with a causal language modeling (CLM) objective are better in that regard.\\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\\n\\n## Resources\\n\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\\n- [Multiple choice task guide](../tasks/multiple_choice)\\n\\n## SqueezeBertConfig\\n\\n[[autodoc]] SqueezeBertConfig\\n\\n## SqueezeBertTokenizer\\n\\n[[autodoc]] SqueezeBertTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## SqueezeBertTokenizerFast\\n\\n[[autodoc]] SqueezeBertTokenizerFast\\n\\n## SqueezeBertModel\\n\\n[[autodoc]] SqueezeBertModel\\n\\n## SqueezeBertForMaskedLM\\n\\n[[autodoc]] SqueezeBertForMaskedLM\\n\\n## SqueezeBertForSequenceClassification\\n\\n[[autodoc]] SqueezeBertForSequenceClassification\\n\\n## SqueezeBertForMultipleChoice\\n\\n[[autodoc]] SqueezeBertForMultipleChoice\\n\\n## SqueezeBertForTokenClassification\\n\\n[[autodoc]] SqueezeBertForTokenClassification\\n\\n## SqueezeBertForQuestionAnswering\\n\\n[[autodoc]] SqueezeBertForQuestionAnswering\\n'), Document(metadata={}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Under construction'), Document(metadata={}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# ALBERT\\n\\n<div class=\"flex flex-wrap space-x-1\">\\n<a href=\"https://huggingface.co/models?filter=albert\">\\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-albert-blueviolet\">\\n</a>\\n<a href=\"https://huggingface.co/spaces/docs-demos/albert-base-v2\">\\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\\n</a>\\n</div>\\n\\n## Overview\\n\\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\\nspeed of BERT:\\n\\n- Splitting the embedding matrix into two smaller matrices.\\n- Using repeating layers split among groups.\\n\\nThe abstract from the paper is the following:\\n\\n*Increasing model size when pretraining natural language representations often results in improved performance on\\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks\\nwith multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and\\nSQuAD benchmarks while having fewer parameters compared to BERT-large.*\\n\\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\\n\\n## Usage tips\\n\\n- ALBERT is a model with absolute position embeddings so it\\'s usually advised to pad the inputs on the right rather\\n  than the left.\\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\\n  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\\n  number of (repeating) layers.\\n- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it\\'s more logical to have H >> E. Also, the embedding matrix is large since it\\'s V x E (V being the vocab size). If E < H, it has less parameters.\\n- Layers are split in groups that share parameters (to save memory).\\nNext sentence prediction is replaced by a sentence ordering prediction: in the inputs, we have two sentences A and B (that are consecutive) and we either feed A followed by B or B followed by A. The model must predict if they have been swapped or not.\\n\\n\\n\\nThis model was contributed by [lysandre](https://huggingface.co/lysandre). This model jax version was contributed by\\n[kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/google-research/ALBERT).\\n\\n\\n## Resources\\n\\n\\nThe resources provided in the following sections consist of a list of official Hugging Face and community (indicated by 🌎) resources to help you get started with AlBERT. If you\\'re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\\'ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n\\n<PipelineTag pipeline=\"text-classification\"/>\\n\\n\\n- [`AlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification).\\n\\n\\n- [`TFAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification).\\n\\n- [`FlaxAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb).\\n- Check the [Text classification task guide](../tasks/sequence_classification) on how to use the model.\\n\\n\\n<PipelineTag pipeline=\"token-classification\"/>\\n\\n\\n- [`AlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification).\\n\\n\\n- [`TFAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\\n\\n\\n\\n- [`FlaxAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification).\\n- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the 🤗 Hugging Face Course.\\n- Check the [Token classification task guide](../tasks/token_classification) on how to use the model.\\n\\n<PipelineTag pipeline=\"fill-mask\"/>\\n\\n- [`AlbertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\\n- [`TFAlbertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\\n- [`FlaxAlbertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).\\n- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the 🤗 Hugging Face Course.\\n- Check the [Masked language modeling task guide](../tasks/masked_language_modeling) on how to use the model.\\n\\n<PipelineTag pipeline=\"question-answering\"/>\\n\\n- [`AlbertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb).\\n- [`TFAlbertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb).\\n- [`FlaxAlbertForQuestionAnswering`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering).\\n- [Question answering](https://huggingface.co/course/chapter7/7?fw=pt) chapter of the 🤗 Hugging Face Course.\\n- Check the [Question answering task guide](../tasks/question_answering) on how to use the model.\\n\\n**Multiple choice**\\n\\n- [`AlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).\\n- [`TFAlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\\n\\n- Check the  [Multiple choice task guide](../tasks/multiple_choice) on how to use the model.\\n\\n\\n## AlbertConfig\\n\\n[[autodoc]] AlbertConfig\\n\\n## AlbertTokenizer\\n\\n[[autodoc]] AlbertTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## AlbertTokenizerFast\\n\\n[[autodoc]] AlbertTokenizerFast\\n\\n## Albert specific outputs\\n\\n[[autodoc]] models.albert.modeling_albert.AlbertForPreTrainingOutput\\n\\n[[autodoc]] models.albert.modeling_tf_albert.TFAlbertForPreTrainingOutput\\n\\n<frameworkcontent>\\n<pt>\\n\\n## AlbertModel\\n\\n[[autodoc]] AlbertModel\\n    - forward\\n\\n## AlbertForPreTraining\\n\\n[[autodoc]] AlbertForPreTraining\\n    - forward\\n\\n## AlbertForMaskedLM\\n\\n[[autodoc]] AlbertForMaskedLM\\n    - forward\\n\\n## AlbertForSequenceClassification\\n\\n[[autodoc]] AlbertForSequenceClassification\\n    - forward\\n\\n## AlbertForMultipleChoice\\n\\n[[autodoc]] AlbertForMultipleChoice\\n\\n## AlbertForTokenClassification\\n\\n[[autodoc]] AlbertForTokenClassification\\n    - forward\\n\\n## AlbertForQuestionAnswering\\n\\n[[autodoc]] AlbertForQuestionAnswering\\n    - forward\\n\\n</pt>\\n\\n<tf>\\n\\n## TFAlbertModel\\n\\n[[autodoc]] TFAlbertModel\\n    - call\\n\\n## TFAlbertForPreTraining\\n\\n[[autodoc]] TFAlbertForPreTraining\\n    - call\\n\\n## TFAlbertForMaskedLM\\n\\n[[autodoc]] TFAlbertForMaskedLM\\n    - call\\n\\n## TFAlbertForSequenceClassification\\n\\n[[autodoc]] TFAlbertForSequenceClassification\\n    - call\\n\\n## TFAlbertForMultipleChoice\\n\\n[[autodoc]] TFAlbertForMultipleChoice\\n    - call\\n\\n## TFAlbertForTokenClassification\\n\\n[[autodoc]] TFAlbertForTokenClassification\\n    - call\\n\\n## TFAlbertForQuestionAnswering\\n\\n[[autodoc]] TFAlbertForQuestionAnswering\\n    - call\\n\\n</tf>\\n<jax>\\n\\n## FlaxAlbertModel\\n\\n[[autodoc]] FlaxAlbertModel\\n    - __call__\\n\\n## FlaxAlbertForPreTraining\\n\\n[[autodoc]] FlaxAlbertForPreTraining\\n    - __call__\\n\\n## FlaxAlbertForMaskedLM\\n\\n[[autodoc]] FlaxAlbertForMaskedLM\\n    - __call__\\n\\n## FlaxAlbertForSequenceClassification\\n\\n[[autodoc]] FlaxAlbertForSequenceClassification\\n    - __call__\\n\\n## FlaxAlbertForMultipleChoice\\n\\n[[autodoc]] FlaxAlbertForMultipleChoice\\n    - __call__\\n\\n## FlaxAlbertForTokenClassification\\n\\n[[autodoc]] FlaxAlbertForTokenClassification\\n    - __call__\\n\\n## FlaxAlbertForQuestionAnswering\\n\\n[[autodoc]] FlaxAlbertForQuestionAnswering\\n    - __call__\\n\\n</jax>\\n</frameworkcontent>\\n\\n\\n'), Document(metadata={}, page_content=' Introduction[[introduction]]\\n\\n<CourseFloatingBanner\\n    chapter={5}\\n    classNames=\"absolute z-10 right-0 top-0\"\\n/>\\n\\nIn [Chapter 3](/course/chapter3) you got your first taste of the 🤗 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\\n\\n1. Load a dataset from the Hugging Face Hub.\\n2. Preprocess the data with `Dataset.map()`.\\n3. Load and compute metrics.\\n\\nBut this is just scratching the surface of what 🤗 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we\\'ll find answers to the following questions:\\n\\n* What do you do when your dataset is not on the Hub?\\n* How can you slice and dice a dataset? (And what if you _really_ need to use Pandas?)\\n* What do you do when your dataset is huge and will melt your laptop\\'s RAM?\\n* What the heck are \"memory mapping\" and Apache Arrow?\\n* How can you create your own dataset and push it to the Hub?\\n\\nThe techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) -- so grab a coffee and let\\'s get started!'), Document(metadata={}, page_content='--\\ntitle: \"Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\"\\nthumbnail: /blog/assets/46_infinity_cpu_performance/thumbnail.png\\nauthors:\\n- user: philschmid\\n- user: jeffboudier\\n- user: mfuntowicz\\n---\\n# Case Study: Millisecond Latency using Hugging Face Infinity and modern CPUs\\n\\n\\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\\n\\n<br>\\n<div style=\"background-color: #e6f9e6; padding: 16px 32px; outline: 2px solid; border-radius: 10px;\">\\n  December 2022 Update: Infinity is no longer offered by Hugging Face as a commercial inference solution. To deploy and accelerate your models, we recommend the following new solutions:\\n\\n  * [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) to easily deploy models on dedicated infrastructure managed by Hugging Face.\\n\\n  * Our open-source optimization libraries, [🤗 Optimum Intel](https://huggingface.co/blog/openvino) and [🤗 Optimum ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview), to get the highest efficiency out of training and running models for inference.\\n\\n  * Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models.\\n</div>\\n\\n\\n## Introduction \\n\\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale.\\n\\nThe main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.\\n\\nWith [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\\n\\n\\n## What is Hugging Face Infinity\\n\\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.\\n\\nHugging Face Infinity consists of 2 main services:\\n* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.\\n* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.\\n\\nThe Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.\\n\\n<br>\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Product overview\" src=\"assets/46_infinity_cpu_performance/overview.png\"></medium-zoom>\\n  <figcaption>Figure 1. Infinity Overview</figcaption>\\n</figure>\\n<br>\\n\\nAn Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines). As of the writing of this blog post, supported tasks include feature extraction/document embedding, ranking, sequence classification, and token classification.\\n\\nYou can find more information about Hugging Face Infinity at [hf.co/infinity](https://huggingface.co/infinity), and if you are interested in testing it for yourself, you can sign up for a free trial at [hf.co/infinity-trial](https://huggingface.co/infinity-trial).\\n\\n---\\n\\n## Benchmark \\n\\nInference performance benchmarks often only measure the execution of the model. In this blog post, and when discussing the performance of Infinity, we always measure the end-to-end pipeline including pre-processing, prediction, post-processing. Please keep this in mind when comparing these results with other latency measurements. \\n\\n<br>\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Pipeline\" src=\"assets/46_infinity_cpu_performance/pipeline.png\"></medium-zoom>\\n  <figcaption>Figure 2. Infinity End-to-End Pipeline</figcaption>\\n</figure>\\n<br>\\n\\n### Environment\\n\\nAs a benchmark environment, we are going to use the [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i), which are compute-optimized instances powered by the 3rd generation of Intel Xeon Scalable processors. These new Intel-based instances are using the ice-lake Process Technology and support Intel AVX-512, Intel Turbo Boost, and Intel Deep Learning Boost.\\n\\nIn addition to superior performance for machine learning workloads, the Intel Ice Lake C6i instances offer great cost-performance and are our recommendation to deploy Infinity on Amazon Web Services.  To learn more, visit the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) page. \\n\\n\\n### Methodologies\\n\\nWhen it comes to benchmarking BERT-like models, two metrics are most adopted:\\n* **Latency**: Time it takes for a single prediction of the model (pre-process, prediction, post-process)\\n* **Throughput**: Number of executions performed in a fixed amount of time for one benchmark configuration, respecting Physical CPU cores, Sequence Length, and Batch Size\\n\\nThese two metrics will be used to benchmark Hugging Face Infinity across different setups to understand the benefits and tradeoffs in this blog post.\\n\\n---\\n\\n## Results\\n\\nTo run the benchmark, we created an infinity container for the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) (Ice-lake) and optimized a [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) model for sequence classification using Infinity Multiverse. \\n\\nThis ice-lake optimized Infinity Container can achieve up to 34% better latency & throughput compared to existing cascade-lake-based instances, and up to 800% better latency & throughput compared to vanilla transformers running on ice-lake.\\n\\nThe Benchmark we created consists of 192 different experiments and configurations. We ran experiments for: \\n* Physical CPU cores: 1, 2, 4, 8\\n* Sequence length: 8, 16, 32, 64, 128, 256, 384, 512\\n* Batch_size: 1, 2, 4, 8, 16, 32\\n\\nIn each experiment, we collect numbers for:\\n* Throughput (requests per second)\\n* Latency (min, max, avg, p90, p95, p99)\\n\\nYou can find the full data of the benchmark in this google spreadsheet: [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing).\\n\\nIn this blog post, we will highlight a few results of the benchmark including the best latency and throughput configurations.\\n\\nIn addition to this, we deployed the [DistilBERT](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) model we used for the benchmark as an API endpoint on 2 physical cores. You can test it and get a feeling for the performance of Infinity. Below you will find a `curl` command on how to send a request to the hosted endpoint. The API returns a `x-compute-time` HTTP Header, which contains the duration of the end-to-end pipeline.\\n\\n```bash\\ncurl --request POST `-i` \\\\\\n  --url https://infinity.huggingface.co/cpu/distilbert-base-uncased-emotion \\\\\\n  --header \\'Content-Type: application/json\\' \\\\\\n  --data \\'{\"inputs\":\"I like you. I love you\"}\\'\\n```\\n\\n### Throughput\\n\\nBelow you can find the throughput comparison for running infinity on 2 physical cores with batch size 1, compared with vanilla transformers.\\n\\n<br>\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Throughput\" src=\"assets/46_infinity_cpu_performance/throughput.png\"></medium-zoom>\\n  <figcaption>Figure 3. Throughput: Infinity vs Transformers</figcaption>\\n</figure>\\n<br>\\n\\n\\n| Sequence Length | Infinity    | Transformers | improvement |\\n|-----------------|-------------|--------------|-------------|\\n| 8               | 248 req/sec | 49 req/sec   | +506%       |\\n| 16              | 212 req/sec | 50 req/sec   | +424%       |\\n| 32              | 150 req/sec | 40 req/sec   | +375%       |\\n| 64              | 97 req/sec  | 28 req/sec   | +346%       |\\n| 128             | 55 req/sec  | 18 req/sec   | +305%       |\\n| 256             | 27 req/sec  | 9 req/sec    | +300%       |\\n| 384             | 17 req/sec  | 5 req/sec    | +340%       |\\n| 512             | 12 req/sec  | 4 req/sec    | +300%       |\\n\\n\\n### Latency \\n\\nBelow, you can find the latency results for an experiment running Hugging Face Infinity on 2 Physical Cores with Batch Size 1. It is remarkable to see how robust and constant Infinity is, with minimal deviation for p95, p99, or p100 (max latency). This result is confirmed for other experiments as well in the benchmark.  \\n\\n<br>\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Latency\" src=\"assets/46_infinity_cpu_performance/latency.png\"></medium-zoom>\\n  <figcaption>Figure 4. Latency (Batch=1, Physical Cores=2)</figcaption>\\n</figure>\\n<br>\\n\\n---\\n\\n## Conclusion\\n\\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\\n\\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\\n\\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \\n\\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \\n\\n\\n## Resources\\n\\n* [Hugging Face Infinity](https://huggingface.co/infinity)\\n* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\\n* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i) \\n* [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\\n* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\\n* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\\n* [🤗 Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)\\n'), Document(metadata={}, page_content='--\\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\\nthumbnail: /blog/assets/23_spacy/thumbnail.png\\n\\nauthors:\\n- user: osanseviero\\n- user: ines\\n---\\n\\n# Welcome spaCy to the Hugging Face Hub\\n\\n\\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.\\n\\nHugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You\\'ll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\\n\\n### Finding models\\n\\nOver 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.\\n\\n\\n### Widgets\\n\\nThis integration includes support for NER widgets, so all models with a NER component will have this out of the box! Coming soon there will be support for text classification and POS.\\n\\n<div><a class=\"text-xs block mb-3 text-gray-300\" href=\"/spacy/en_core_web_sm\"><code>spacy/en_core_web_sm</code></a>\\n<div class=\"SVELTE_HYDRATER \" data-props=\"{&quot;apiUrl&quot;:&quot;https://api-inference.huggingface.co&quot;,&quot;model&quot;:{&quot;author&quot;:&quot;spacy&quot;,&quot;autoArchitecture&quot;:&quot;AutoModel&quot;,&quot;branch&quot;:&quot;main&quot;,&quot;cardData&quot;:{&quot;tags&quot;:[&quot;spacy&quot;,&quot;token-classification&quot;],&quot;language&quot;:[&quot;en&quot;],&quot;license&quot;:&quot;MIT&quot;,&quot;model-index&quot;:[{&quot;name&quot;:&quot;en_core_web_sm&quot;,&quot;results&quot;:[{&quot;tasks&quot;:{&quot;name&quot;:&quot;NER&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Precision&quot;,&quot;type&quot;:&quot;precision&quot;,&quot;value&quot;:0.8424355924},{&quot;name&quot;:&quot;Recall&quot;,&quot;type&quot;:&quot;recall&quot;,&quot;value&quot;:0.8335336538},{&quot;name&quot;:&quot;F Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.8379609817}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;POS&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9720712187}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;SENTER&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Precision&quot;,&quot;type&quot;:&quot;precision&quot;,&quot;value&quot;:0.9074955788},{&quot;name&quot;:&quot;Recall&quot;,&quot;type&quot;:&quot;recall&quot;,&quot;value&quot;:0.8801372122},{&quot;name&quot;:&quot;F Score&quot;,&quot;type&quot;:&quot;f_score&quot;,&quot;value&quot;:0.893607046}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;UNLABELED_DEPENDENCIES&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9185392711}]}},{&quot;tasks&quot;:{&quot;name&quot;:&quot;LABELED_DEPENDENCIES&quot;,&quot;type&quot;:&quot;token-classification&quot;,&quot;metrics&quot;:[{&quot;name&quot;:&quot;Accuracy&quot;,&quot;type&quot;:&quot;accuracy&quot;,&quot;value&quot;:0.9185392711}]}}]}]},&quot;cardSource&quot;:true,&quot;id&quot;:&quot;spacy/en_core_web_sm&quot;,&quot;pipeline_tag&quot;:&quot;token-classification&quot;,&quot;library_name&quot;:&quot;spacy&quot;,&quot;modelId&quot;:&quot;spacy/en_core_web_sm&quot;,&quot;private&quot;:false,&quot;siblings&quot;:[{&quot;rfilename&quot;:&quot;.gitattributes&quot;},{&quot;rfilename&quot;:&quot;LICENSE&quot;},{&quot;rfilename&quot;:&quot;LICENSES_SOURCES&quot;},{&quot;rfilename&quot;:&quot;README.md&quot;},{&quot;rfilename&quot;:&quot;accuracy.json&quot;},{&quot;rfilename&quot;:&quot;config.cfg&quot;},{&quot;rfilename&quot;:&quot;en_core_web_sm-any-py3-none-any.whl&quot;},{&quot;rfilename&quot;:&quot;meta.json&quot;},{&quot;rfilename&quot;:&quot;tokenizer&quot;},{&quot;rfilename&quot;:&quot;attribute_ruler/patterns&quot;},{&quot;rfilename&quot;:&quot;lemmatizer/lookups/lookups.bin&quot;},{&quot;rfilename&quot;:&quot;ner/cfg&quot;},{&quot;rfilename&quot;:&quot;ner/model&quot;},{&quot;rfilename&quot;:&quot;ner/moves&quot;},{&quot;rfilename&quot;:&quot;vocab/lookups.bin&quot;},{&quot;rfilename&quot;:&quot;vocab/strings.json&quot;},{&quot;rfilename&quot;:&quot;vocab/vectors&quot;}],&quot;tags&quot;:[&quot;en&quot;,&quot;spacy&quot;,&quot;token-classification&quot;,&quot;license:mit&quot;,&quot;model-index&quot;],&quot;tag_objs&quot;:[{&quot;id&quot;:&quot;token-classification&quot;,&quot;label&quot;:&quot;Token Classification&quot;,&quot;type&quot;:&quot;pipeline_tag&quot;},{&quot;id&quot;:&quot;spacy&quot;,&quot;label&quot;:&quot;spaCy&quot;,&quot;type&quot;:&quot;library&quot;},{&quot;id&quot;:&quot;en&quot;,&quot;label&quot;:&quot;en&quot;,&quot;type&quot;:&quot;language&quot;},{&quot;id&quot;:&quot;license:mit&quot;,&quot;label&quot;:&quot;mit&quot;,&quot;type&quot;:&quot;license&quot;},{&quot;id&quot;:&quot;model-index&quot;,&quot;label&quot;:&quot;model-index&quot;,&quot;type&quot;:&quot;other&quot;}],&quot;widgetData&quot;:[{&quot;text&quot;:&quot;My name is Wolfgang and I live in Berlin&quot;},{&quot;text&quot;:&quot;My name is Sarah and I live in London&quot;},{&quot;text&quot;:&quot;My name is Clara and I live in Berkeley, California.&quot;}]},&quot;shouldUpdateUrl&quot;:true}\" data-target=\"InferenceWidget\"><div class=\"flex flex-col w-full max-w-full\\n\\t\"> <div class=\"font-semibold flex items-center mb-2\"><div class=\"text-lg flex items-center\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" class=\"-ml-1 mr-1 text-yellow-500\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path d=\"M11 15H6l7-14v8h5l-7 14v-8z\" fill=\"currentColor\"></path></svg>\\n\\t\\t\\tHosted inference API</div> <a target=\"_blank\" href=\"/docs\"><svg class=\"ml-1.5 text-sm text-gray-400 hover:text-black\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M17 22v-8h-4v2h2v6h-3v2h8v-2h-3z\" fill=\"currentColor\"></path><path d=\"M16 8a1.5 1.5 0 1 0 1.5 1.5A1.5 1.5 0 0 0 16 8z\" fill=\"currentColor\"></path><path d=\"M16 30a14 14 0 1 1 14-14a14 14 0 0 1-14 14zm0-26a12 12 0 1 0 12 12A12 12 0 0 0 16 4z\" fill=\"currentColor\"></path></svg></a></div> <div class=\"flex items-center text-sm text-gray-500 mb-1.5\"><div class=\"inline-flex items-center\"><svg class=\"mr-1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" fill=\"currentColor\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 18 18\"><path d=\"M11.075 10.1875H12.1625V11.275H11.075V10.1875Z\"></path><path d=\"M15.425 9.10004H16.5125V10.1875H15.425V9.10004Z\"></path><path d=\"M7.8125 3.66254H8.9V4.75004H7.8125V3.66254Z\"></path><path d=\"M8.90001 12.3625H6.72501V9.09998C6.72472 8.81165 6.61005 8.5352 6.40617 8.33132C6.20228 8.12744 5.92584 8.01277 5.63751 8.01248H2.37501C2.08667 8.01277 1.81023 8.12744 1.60635 8.33132C1.40246 8.5352 1.28779 8.81165 1.28751 9.09998V12.3625C1.28779 12.6508 1.40246 12.9273 1.60635 13.1311C1.81023 13.335 2.08667 13.4497 2.37501 13.45H5.63751V15.625C5.63779 15.9133 5.75246 16.1898 5.95635 16.3936C6.16023 16.5975 6.43667 16.7122 6.72501 16.7125H8.90001C9.18834 16.7122 9.46478 16.5975 9.66867 16.3936C9.87255 16.1898 9.98722 15.9133 9.98751 15.625V13.45C9.98722 13.1616 9.87255 12.8852 9.66867 12.6813C9.46478 12.4774 9.18834 12.3628 8.90001 12.3625V12.3625ZM2.37501 12.3625V9.09998H5.63751V12.3625H2.37501ZM6.72501 15.625V13.45H8.90001V15.625H6.72501Z\"></path><path d=\"M15.425 16.7125H13.25C12.9617 16.7122 12.6852 16.5976 12.4813 16.3937C12.2775 16.1898 12.1628 15.9134 12.1625 15.625V13.45C12.1628 13.1617 12.2775 12.8852 12.4813 12.6814C12.6852 12.4775 12.9617 12.3628 13.25 12.3625H15.425C15.7133 12.3628 15.9898 12.4775 16.1937 12.6814C16.3976 12.8852 16.5122 13.1617 16.5125 13.45V15.625C16.5122 15.9134 16.3976 16.1898 16.1937 16.3937C15.9898 16.5976 15.7133 16.7122 15.425 16.7125ZM13.25 13.45V15.625H15.425V13.45H13.25Z\"></path><path d=\"M15.425 1.48752H12.1625C11.8742 1.48781 11.5977 1.60247 11.3938 1.80636C11.19 2.01024 11.0753 2.28668 11.075 2.57502V5.83752H9.98751C9.69917 5.83781 9.42273 5.95247 9.21885 6.15636C9.01496 6.36024 8.9003 6.63668 8.90001 6.92502V8.01252C8.9003 8.30085 9.01496 8.5773 9.21885 8.78118C9.42273 8.98506 9.69917 9.09973 9.98751 9.10002H11.075C11.3633 9.09973 11.6398 8.98506 11.8437 8.78118C12.0476 8.5773 12.1622 8.30085 12.1625 8.01252V6.92502H15.425C15.7133 6.92473 15.9898 6.81006 16.1937 6.60618C16.3976 6.4023 16.5122 6.12585 16.5125 5.83752V2.57502C16.5122 2.28668 16.3976 2.01024 16.1937 1.80636C15.9898 1.60247 15.7133 1.48781 15.425 1.48752ZM9.98751 8.01252V6.92502H11.075V8.01252H9.98751ZM12.1625 5.83752V2.57502H15.425V5.83752H12.1625Z\"></path><path d=\"M4.55001 5.83752H2.37501C2.08667 5.83723 1.81023 5.72256 1.60635 5.51868C1.40246 5.3148 1.28779 5.03835 1.28751 4.75002V2.57502C1.28779 2.28668 1.40246 2.01024 1.60635 1.80636C1.81023 1.60247 2.08667 1.48781 2.37501 1.48752H4.55001C4.83834 1.48781 5.11478 1.60247 5.31867 1.80636C5.52255 2.01024 5.63722 2.28668 5.63751 2.57502V4.75002C5.63722 5.03835 5.52255 5.3148 5.31867 5.51868C5.11478 5.72256 4.83834 5.83723 4.55001 5.83752V5.83752ZM2.37501 2.57502V4.75002H4.55001V2.57502H2.37501Z\"></path></svg> <span>Token Classification</span></div> <div class=\"ml-auto\"></div></div> <form><div class=\"flex h-10\"><input class=\"form-input-alt flex-1 rounded-r-none \" placeholder=\"Your sentence here...\" required=\"\" type=\"text\"> <button class=\"btn-widget w-24 h-10 px-5 rounded-l-none border-l-0 \" type=\"submit\">Compute</button></div></form> <div class=\"mt-1.5\"><div class=\"text-gray-400 text-xs\">This model is currently loaded and running on the Inference API.</div> </div>   <div class=\"mt-auto pt-4 flex items-center text-xs text-gray-500\"><button class=\"flex items-center cursor-not-allowed text-gray-300\" disabled=\"\"><svg class=\"mr-1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\" style=\"transform: rotate(360deg);\"><path d=\"M31 16l-7 7l-1.41-1.41L28.17 16l-5.58-5.59L24 9l7 7z\" fill=\"currentColor\"></path><path d=\"M1 16l7-7l1.41 1.41L3.83 16l5.58 5.59L8 23l-7-7z\" fill=\"currentColor\"></path><path d=\"M12.419 25.484L17.639 6l1.932.518L14.35 26z\" fill=\"currentColor\"></path></svg>\\n\\t\\tJSON Output</button> <button class=\"flex items-center ml-auto\"><svg class=\"mr-1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M22 16h2V8h-8v2h6v6z\" fill=\"currentColor\"></path><path d=\"M8 24h8v-2h-6v-6H8v8z\" fill=\"currentColor\"></path><path d=\"M26 28H6a2.002 2.002 0 0 1-2-2V6a2.002 2.002 0 0 1 2-2h20a2.002 2.002 0 0 1 2 2v20a2.002 2.002 0 0 1-2 2zM6 6v20h20.001L26 6z\" fill=\"currentColor\"></path></svg>\\n\\t\\tMaximize</button></div> </div></div></div>\\n                \\n\\n### Using existing models\\n\\nAll models from the Hub can be directly installed using `pip install`. \\n\\n\\n```bash\\npip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl\\n```\\n\\n```python\\n# Using spacy.load().\\nimport spacy\\nnlp = spacy.load(\"en_core_web_sm\")\\n\\n# Importing as module.\\nimport en_core_web_sm\\nnlp = en_core_web_sm.load()\\n```\\n\\nWhen you open a repository, you can click `Use in spaCy` and you will be given a working snippet that you can use to install and load the model!\\n\\n![snippet](assets/23_spacy/snippet.png)\\n![snippet](assets/23_spacy/snippet2.png)\\n\\nYou can even make HTTP requests to call the models from the Inference API, which is useful in production settings. Here is an example of a simple request:\\n\\n```bash\\ncurl -X POST  --data \\'{\"inputs\": \"Hello, this is Omar\"}\\' https://api-inference.huggingface.co/models/spacy/en_core_web_sm\\n>>> [{\"entity_group\":\"PERSON\",\"word\":\"Omar\",\"start\":15,\"end\":19,\"score\":1.0}]\\n```\\n\\nAnd for larger-scale use cases, you can click \"Deploy > Accelerated Inference\" and see how to do this with Python.\\n\\n\\n### Sharing your models\\n\\nBut probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. \\n\\n```bash\\nhuggingface-cli login\\npython -m spacy package ./en_ner_fashion ./output --build wheel\\ncd ./output/en_ner_fashion-0.0.0/dist\\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\\n```\\n\\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\\n\\nTry it out and share your models with the community!\\n\\n## Would you like to integrate your library to the Hub?\\n\\nThis integration is possible thanks to the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library which has all our widgets and the API for all our supported libraries. If you would like to integrate your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you!\\n'), Document(metadata={}, page_content='--\\ntitle: \"Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\"\\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\\nauthors:\\n- user: asi\\n  guest: true\\n---\\n\\n# Nyströmformer: Approximating self-attention in linear time and memory via the Nyström method\\n\\n\\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\\n\\n## Introduction\\n\\nTransformers have exhibited remarkable performance on various Natural Language Processing and Computer Vision tasks. Their success can be attributed to the self-attention mechanism, which captures the pairwise interactions between all the tokens in an input. However, the standard self-attention mechanism has a time and memory complexity of \\\\\\\\(O(n^2)\\\\\\\\) (where \\\\\\\\(n\\\\\\\\) is the length of the input sequence), making it expensive to train on long input sequences. \\n\\nThe [Nyströmformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\\\\\(O(n)\\\\\\\\) complexity. Nyströmformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nyström method and how it can be adapted to approximate self-attention.\\n\\n\\n## Nyström method for matrix approximation\\n\\nAt the heart of Nyströmformer is the Nyström method for matrix approximation. It allows us to approximate a matrix by sampling some of its rows and columns. Let\\'s consider a matrix \\\\\\\\(P^{n \\\\times n}\\\\\\\\), which is expensive to compute in its entirety. So, instead, we approximate it using the Nyström method. We start by sampling \\\\\\\\(m\\\\\\\\) rows and columns from \\\\\\\\(P\\\\\\\\). We can then arrange the sampled rows and columns as follows:\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Representing P as a block matrix\" src=\"assets/86_nystromformer/p_block.png\"></medium-zoom>\\n  <figcaption>Representing P as a block matrix</figcaption>\\n</figure>\\n\\nWe now have four submatrices: \\\\\\\\(A_P, B_P, F_P,\\\\\\\\) and \\\\\\\\(C_P\\\\\\\\), with sizes \\\\\\\\(m \\\\times m, m \\\\times (n - m), (n - m) \\\\times m\\\\\\\\) and \\n\\\\\\\\((n - m) \\\\times (n - m)\\\\\\\\) respectively. The \\\\\\\\(m\\\\\\\\) sampled columns are contained in \\\\\\\\(A_P\\\\\\\\) and \\\\\\\\(F_P\\\\\\\\), whereas the \\\\\\\\(m\\\\\\\\) sampled rows are contained in \\\\\\\\(A_P\\\\\\\\) and \\\\\\\\(B_P\\\\\\\\). So, the entries of \\\\\\\\(A_P, B_P,\\\\\\\\) and \\\\\\\\(F_P\\\\\\\\) are known to us, and we will estimate \\\\\\\\(C_P\\\\\\\\). According to the Nyström method, \\\\\\\\(C_P\\\\\\\\) is given by:\\n\\n$$C_P = F_P A_P^+ B_P$$\\n\\nHere, \\\\\\\\(+\\\\\\\\) denotes the Moore-Penrose inverse (or pseudoinverse). \\nThus, the Nyström approximation of \\\\\\\\(P, \\\\hat{P}\\\\\\\\) can be written as:\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nyström approximation of P\" src=\"assets/86_nystromformer/p_hat.png\"></medium-zoom>\\n  <figcaption>Nyström approximation of P</figcaption>\\n</figure>\\n\\nAs shown in the second line, \\\\\\\\(\\\\hat{P}\\\\\\\\) can be expressed as a product of three matrices. The reason for doing so will become clear later.\\n\\n\\n## Can we approximate self-attention with the Nyström method?\\n\\nOur goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\\\\\( \\\\frac{QK^T}{\\\\sqrt{d}} \\\\\\\\)\\n\\nHere, \\\\\\\\(Q\\\\\\\\) and \\\\\\\\(K\\\\\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\\\\\(m\\\\\\\\) rows and columns from \\\\\\\\(S\\\\\\\\), form four submatrices, and obtain \\\\\\\\(\\\\hat{S}\\\\\\\\):\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Nyström approximation of S\" src=\"assets/86_nystromformer/s_hat.png\"></medium-zoom>\\n  <figcaption>Nyström approximation of S</figcaption>\\n</figure>\\n\\nBut, what does it mean to sample a column from \\\\\\\\(S\\\\\\\\)? It means we select one element from each row. Recall how S is calculated: the final operation is a row-wise softmax. To find a single entry in a row, we must access all other entries (for the denominator in softmax). So, sampling one column requires us to know all other columns in the matrix. Therefore, we cannot directly apply the Nyström method to approximate the softmax matrix.\\n\\n\\n## How can we adapt the Nyström method to approximate self-attention?\\n\\nInstead of sampling from \\\\\\\\(S\\\\\\\\), the authors propose to sample landmarks (or Nyström points) from queries and keys. We denote the query landmarks and key landmarks as \\\\\\\\(\\\\tilde{Q}\\\\\\\\) and \\\\\\\\(\\\\tilde{K}\\\\\\\\) respectively. \\\\\\\\(\\\\tilde{Q}\\\\\\\\) and \\\\\\\\(\\\\tilde{K}\\\\\\\\) can be used to construct three matrices corresponding to those in the Nyström approximation of \\\\\\\\(S\\\\\\\\). We define the following matrices:\\n\\n$$\\\\tilde{F} = softmax(\\\\frac{Q\\\\tilde{K}^T}{\\\\sqrt{d}}) \\\\hspace{40pt} \\\\tilde{A} = softmax(\\\\frac{\\\\tilde{Q}\\\\tilde{K}^T}{\\\\sqrt{d}})^+ \\\\hspace{40pt} \\\\tilde{B} = softmax(\\\\frac{\\\\tilde{Q}K^T}{\\\\sqrt{d}})$$\\n\\nThe sizes of \\\\\\\\(\\\\tilde{F}\\\\\\\\), \\\\\\\\(\\\\tilde{A}\\\\\\\\), and \\\\\\\\(\\\\tilde{B}) are \\\\\\\\(n \\\\times m, m \\\\times m,\\\\\\\\) and \\\\\\\\(m \\\\times n\\\\\\\\) respectively. \\nWe replace the three matrices in the Nyström approximation of \\\\\\\\(S\\\\\\\\) with the new matrices we have defined to obtain an alternative Nyström approximation:\\n\\n$$\\\\begin{aligned}\\\\hat{S} &= \\\\tilde{F} \\\\tilde{A} \\\\tilde{B} \\\\\\\\ &= softmax(\\\\frac{Q\\\\tilde{K}^T}{\\\\sqrt{d}}) softmax(\\\\frac{\\\\tilde{Q}\\\\tilde{K}^T}{\\\\sqrt{d}})^+  softmax(\\\\frac{\\\\tilde{Q}K^T}{\\\\sqrt{d}}) \\\\end{aligned}$$\\n\\nThis is the Nyström approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix with the values ( \\\\\\\\(V\\\\\\\\)) to obtain a linear approximation of self-attention. Note that we never calculated the product \\\\\\\\(QK^T\\\\\\\\), avoiding the \\\\\\\\(O(n^2)\\\\\\\\) complexity. \\n\\n\\n## How do we select landmarks?\\n\\nInstead of sampling \\\\\\\\(m\\\\\\\\) rows from \\\\\\\\(Q\\\\\\\\) and \\\\\\\\(K\\\\\\\\), the authors propose to construct \\\\\\\\(\\\\tilde{Q}\\\\\\\\) and \\\\\\\\(\\\\tilde{K}\\\\\\\\)\\nusing segment means. In this procedure, \\\\\\\\(n\\\\\\\\) tokens are grouped into \\\\\\\\(m\\\\\\\\) segments, and the mean of each segment is computed. Ideally, \\\\\\\\(m\\\\\\\\) is much smaller than \\\\\\\\(n\\\\\\\\). According to experiments from the paper, selecting just \\\\\\\\(32\\\\\\\\) or \\\\\\\\(64\\\\\\\\) landmarks produces competetive performance compared to standard self-attention and other efficient attention mechanisms, even for long sequences lengths ( \\\\\\\\(n=4096\\\\\\\\) or \\\\\\\\(8192\\\\\\\\)). \\n\\nThe overall algorithm is summarised by the following figure from the paper:\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Efficient self-attention with the Nyström method\" src=\"assets/86_nystromformer/paper_figure.png\"></medium-zoom>\\n  <figcaption>Efficient self-attention with the Nyström method</figcaption>\\n</figure>\\n\\nThe three orange matrices above correspond to the three matrices we constructed using the key and query landmarks. Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution.\\n\\n\\n## How is Nyströmformer implemented?\\n\\nThe original implementation of Nyströmformer can be found [here](https://github.com/mlpen/Nystromformer) and the HuggingFace implementation can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/nystromformer/modeling_nystromformer.py). Let\\'s take a look at a few lines of code (with some comments added) from the HuggingFace implementation. Note that some details such as normalization, attention masking, and depthwise convolution are avoided for simplicity.\\n\\n```python\\n\\nkey_layer = self.transpose_for_scores(self.key(hidden_states)) # K\\nvalue_layer = self.transpose_for_scores(self.value(hidden_states)) # V\\nquery_layer = self.transpose_for_scores(mixed_query_layer) # Q\\n\\nq_landmarks = query_layer.reshape(\\n    -1,\\n    self.num_attention_heads,\\n    self.num_landmarks,\\n    self.seq_len // self.num_landmarks,\\n    self.attention_head_size,\\n).mean(dim=-2) # \\\\tilde{Q}\\n\\nk_landmarks = key_layer.reshape(\\n    -1,\\n    self.num_attention_heads,\\n    self.num_landmarks,\\n    self.seq_len // self.num_landmarks,\\n    self.attention_head_size,\\n).mean(dim=-2) # \\\\tilde{K}\\n\\nkernel_1 = torch.nn.functional.softmax(torch.matmul(query_layer, k_landmarks.transpose(-1, -2)), dim=-1) # \\\\tilde{F}\\nkernel_2 = torch.nn.functional.softmax(torch.matmul(q_landmarks, k_landmarks.transpose(-1, -2)), dim=-1) # \\\\tilde{A} before pseudo-inverse\\n\\nattention_scores = torch.matmul(q_landmarks, key_layer.transpose(-1, -2)) # \\\\tilde{B} before softmax\\n\\nkernel_3 = nn.functional.softmax(attention_scores, dim=-1) # \\\\tilde{B}\\nattention_probs = torch.matmul(kernel_1, self.iterative_inv(kernel_2)) # \\\\tilde{F} * \\\\tilde{A}\\nnew_value_layer = torch.matmul(kernel_3, value_layer) # \\\\tilde{B} * V\\ncontext_layer = torch.matmul(attention_probs, new_value_layer) # \\\\tilde{F} * \\\\tilde{A} * \\\\tilde{B} * V\\n```\\n\\n\\n## Using Nyströmformer with HuggingFace\\n\\nNyströmformer for Masked Language Modeling (MLM) is available on HuggingFace. Currently, there are 4 checkpoints, corresponding to various sequence lengths: [`nystromformer-512`](https://huggingface.co/uw-madison/nystromformer-512), [`nystromformer-1024`](https://huggingface.co/uw-madison/nystromformer-1024), [`nystromformer-2048`](https://huggingface.co/uw-madison/nystromformer-2048), and [`nystromformer-4096`](https://huggingface.co/uw-madison/nystromformer-4096). The number of landmarks, \\\\\\\\(m\\\\\\\\), can be controlled using the `num_landmarks` parameter in the [`NystromformerConfig`](https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/nystromformer#transformers.NystromformerConfig). Let\\'s take a look at a minimal example of Nyströmformer for MLM:\\n\\n```python\\nfrom transformers import AutoTokenizer, NystromformerForMaskedLM\\nimport torch\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"uw-madison/nystromformer-512\")\\nmodel = NystromformerForMaskedLM.from_pretrained(\"uw-madison/nystromformer-512\")\\n\\ninputs = tokenizer(\"Paris is the [MASK] of France.\", return_tensors=\"pt\")\\n\\nwith torch.no_grad():\\n    logits = model(**inputs).logits\\n\\n# retrieve index of [MASK]\\nmask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\\n\\npredicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\\ntokenizer.decode(predicted_token_id)\\n```\\n\\n<div class=\"output stream stdout\">\\n\\n    Output:\\n    ----------------------------------------------------------------------------------------------------\\n    capital\\n\\n</div>\\n\\nAlternatively, we can use the [pipeline API](https://huggingface.co/docs/transformers/main_classes/pipelines) (which handles all the complexity for us):\\n```python\\nfrom transformers import pipeline\\nunmasker = pipeline(\\'fill-mask\\', model=\\'uw-madison/nystromformer-512\\')\\nunmasker(\"Paris is the [MASK] of France.\")\\n```\\n\\n<div class=\"output stream stdout\">\\n\\n    Output:\\n    ----------------------------------------------------------------------------------------------------\\n    [{\\'score\\': 0.829957902431488,\\n      \\'token\\': 1030,\\n      \\'token_str\\': \\'capital\\',\\n      \\'sequence\\': \\'paris is the capital of france.\\'},\\n    {\\'score\\': 0.022157637402415276,\\n      \\'token\\': 16081,\\n      \\'token_str\\': \\'birthplace\\',\\n      \\'sequence\\': \\'paris is the birthplace of france.\\'},\\n    {\\'score\\': 0.01904447190463543,\\n      \\'token\\': 197,\\n      \\'token_str\\': \\'name\\',\\n      \\'sequence\\': \\'paris is the name of france.\\'},\\n    {\\'score\\': 0.017583081498742104,\\n      \\'token\\': 1107,\\n      \\'token_str\\': \\'kingdom\\',\\n      \\'sequence\\': \\'paris is the kingdom of france.\\'},\\n    {\\'score\\': 0.005948934704065323,\\n      \\'token\\': 148,\\n      \\'token_str\\': \\'city\\',\\n      \\'sequence\\': \\'paris is the city of france.\\'}]\\n\\n</div>\\n\\n## Conclusion\\n\\nNyströmformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the Nyström method and how it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nyströmformer for downstream tasks can find the HuggingFace documentation [here](https://huggingface.co/docs/transformers/model_doc/nystromformer). \\n'), Document(metadata={}, page_content=\"ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI_JMQtyEr-sLJSy5_SnLCb4vtQf\\n\\nToken Classification video\\n\\nWelcome to the Hugging Face tasks series! In this video we’ll take a look at the token classification task.\\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\\nLet’s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\\nAnother token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.\\nToken classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\\nAn example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\\nYou can extract important information from invoices using named entity recognition models, such as date, organization name or address.\\nFor more information about the Token classification task, check out the Hugging Face course.\\n\\n\\nQuestion Answering video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a look at the Question Answering task.\\nQuestion answering is the task of extracting an answer in a given document.\\nQuestion answering models take a context, which is the document you want to search in, and a question and return an answer. Note that the answer is not generated, but extracted from the context. This type of question answering is called extractive.\\nThe task is evaluated on two metrics, exact match and F1-Score.\\nAs the name implies, exact match looks for an exact match between the predicted answer and the correct answer.\\nA common metric used is the F1-Score, which is calculated over tokens that are predicted correctly and incorrectly. It is calculated over the average of two metrics called precision and recall which are metrics that are used widely in classification problems.\\nAn example dataset used for this task is called SQuAD. This dataset contains contexts, questions and the answers that are obtained from English Wikipedia articles.\\nYou can use question answering models to automatically answer the questions asked by your customers. You simply need a document containing information about your business and query through that document with the questions asked by your customers.\\nFor more information about the Question Answering task, check out the Hugging Face course.\\n\\n\\nCausal Language Modeling video\\n\\nWelcome to the Hugging Face tasks series! In this video we’ll take a look at Causal Language Modeling.\\nCausal language modeling is the task of predicting the next \\nword in a sentence, given all the previous words. This task is very similar to the autocorrect function that you might have on your phone. \\nThese models take a sequence to be completed and outputs the complete sequence.\\nClassification metrics can’t be used as there’s no single correct answer for completion. Instead, we evaluate the distribution of the text completed by the model.\\nA common metric to do so is the cross-entropy loss. Perplexity is also a widely used metric and it is calculated as the exponential of the cross-entropy loss.\\nYou can use any dataset with plain text and tokenize the text to prepare the data. \\nCausal language models can be used to generate code.\\nFor more information about the Causal Language Modeling task, check out the Hugging Face course.\\n\\n\\nMasked Language Modeling video\\n\\nWelcome to the Hugging Face tasks series! In this video we’ll take a look at Masked Language Modeling.\\nMasked language modeling is the task of predicting which words should fill in the blanks of a sentence.\\nThese models take a masked text as the input and output the possible values for that mask.\\nMasked language modeling is handy before fine-tuning your model for your task. For example, if you need to use a model in a specific domain, say, biomedical documents, models like BERT will treat your domain-specific words as rare tokens. If you train a masked language model using your biomedical corpus and then fine tune your model on a downstream task, you will have a better performance.\\nClassification metrics can’t be used as there’s no single correct answer to mask values. Instead, we evaluate the distribution of the mask values.\\nA common metric to do so is the cross-entropy loss. Perplexity is also a widely used metric and it is calculated as the exponential of the cross-entropy loss.\\nYou can use any dataset with plain text and tokenize the text to mask the data.\\nFor more information about the Masked Language Modeling, check out the Hugging Face course.\\n\\n\\nSummarization video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a look at the Text Summarization task.\\nSummarization is a task of producing a shorter version of a document while preserving the relevant and important information in the document.\\nSummarization models take a document to be summarized and output the summarized text.\\nThis task is evaluated on the ROUGE score. It’s based on the overlap between the produced sequence and the correct sequence.\\nYou might see this as ROUGE-1, which is the overlap of single tokens and ROUGE-2, the overlap of subsequent token pairs. ROUGE-N refers to the overlap of n subsequent tokens. Here we see an example of how overlaps take place.\\nAn example dataset used for this task is called Extreme Summarization, XSUM. This dataset contains texts and their summarized versions.\\nYou can use summarization models to summarize research papers which would enable researchers to easily pick papers for their reading list.\\nFor more information about the Summarization task, check out the Hugging Face course.\\n\\n\\nTranslation video\\n\\nWelcome to the Hugging Face tasks series. In this video, we will take a look at the Translation task.\\nTranslation is the task of translating text from one language to another.\\nThese models take a text in the source language and output the translation of that text in the target language.\\nThe task is evaluated on the BLEU score.\\nThe score ranges from 0 to 1, in which 1 means the translation perfectly matched and 0 did not match at all.\\nBLEU is calculated over subsequent tokens called n-grams. Unigram refers to a single token while bi-gram refers to token pairs and n-grams refer to n subsequent tokens. \\nMachine translation datasets contain pairs of text in a language and translation of the text in another language.\\nThese models can help you build conversational agents across different languages.\\nOne option is to translate the training data used for the chatbot and train a separate chatbot.\\nYou can put one translation model from your user’s language to the language your chatbot is trained on, translate the user inputs and do intent classification, take the output of the chatbot and translate it from the language your chatbot was trained on to the user’s language.\\nFor more information about the Translation task, check out the Hugging Face course.\\n\"), Document(metadata={}, page_content='--\\ntitle: Zero-shot image segmentation with CLIPSeg\\nthumbnail: /blog/assets/123_clipseg-zero-shot/thumb.png\\nauthors:\\n- user: segments-tobias\\n  guest: true\\n- user: nielsr\\n---\\n\\n# Zero-shot image segmentation with CLIPSeg\\n\\n\\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/123_clipseg-zero-shot.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n**This guide shows how you can use [CLIPSeg](https://huggingface.co/docs/transformers/main/en/model_doc/clipseg), a zero-shot image segmentation model, using [`🤗 transformers`](https://huggingface.co/transformers). CLIPSeg creates rough segmentation masks that can be used for robot perception, image inpainting, and many other tasks. If you need more precise segmentation masks, we’ll show how you can refine the results of CLIPSeg on [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).**\\n\\nImage segmentation is a well-known task within the field of computer vision. It allows a computer to not only know what is in an image (classification), where objects are in the image (detection), but also what the outlines of those objects are. Knowing the outlines of objects is essential in fields such as robotics and autonomous driving. For example, a robot has to know the shape of an object to grab it correctly. Segmentation can also be combined with [image inpainting](https://t.co/5q8YHSOfx7) to allow users to describe which part of the image they want to replace.\\n\\nOne limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? That’s exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves.\\n\\nCurrently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let’s first take a look at how CLIPSeg works.\\n\\n## CLIP: the magic model behind CLIPSeg\\n\\n[CLIP](https://huggingface.co/docs/transformers/main/en/model_doc/clip), which stands for **C**ontrastive **L**anguage–**I**mage **P**re-training, is a model developed by OpenAI in 2021. You can give CLIP an image or a piece of text, and CLIP will output an abstract *representation* of your input. This abstract representation, also called an *embedding*, is really just a vector (a list of numbers). You can think of this vector as a point in high-dimensional space. CLIP is trained so that the representations of similar pictures and texts are similar as well. This means that if we input an image and a text description that fits that image, the representations of the image and the text will be similar (i.e., the high-dimensional points will be close together).\\n\\nAt first, this might not seem very useful, but it is actually very powerful. As an example, let’s take a quick look at how CLIP can be used to classify images without ever having been trained on that task. To classify an image, we input the image and the different categories we want to choose from to CLIP (e.g. we input an image and the words “apple”, “orange”, …).  CLIP then gives us back an embedding of the image and of each category. Now, we simply have to check which category embedding is closest to the embedding of the image, et voilà! Feels like magic, doesn’t it? \\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png\"></medium-zoom>\\n  <figcaption>Example of image classification using CLIP (<a href=\"https://openai.com/blog/clip/\">source</a>).</figcaption>\\n</figure>\\n\\nWhat’s more, CLIP is not only useful for classification, but it can also be used for [image search](https://huggingface.co/spaces/DrishtiSharma/Text-to-Image-search-using-CLIP) (can you see how this is similar to classification?), [text-to-image models](https://huggingface.co/spaces/kamiyamai/stable-diffusion-webui) ([DALL-E 2](https://openai.com/dall-e-2/) is powered by CLIP), [object detection](https://segments.ai/zeroshot?utm_source=hf&utm_medium=blog&utm_campaign=clipseg) ([OWL-ViT](https://arxiv.org/abs/2205.06230)), and most importantly for us: image segmentation. Now you see why CLIP was truly a breakthrough in machine learning.\\n\\nThe reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them.\\n\\n## CLIPSeg: image segmentation with CLIP\\n\\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo Lüddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn’t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP. \\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clipseg-overview.png\"></medium-zoom>\\n  <figcaption><a href=\"https://arxiv.org/abs/2112.10003\">Source</a></figcaption>\\n</figure>\\n\\nThe decoder is trained on the [PhraseCut dataset](https://arxiv.org/abs/2008.01187), which contains over 340,000 phrases with corresponding image segmentation masks. The authors also experimented with various augmentations to expand the size of the dataset. The goal here is not only to be able to segment the categories that are present in the dataset, but also to segment unseen categories. Experiments indeed show that the decoder can generalize to unseen categories. \\n\\nOne interesting feature of CLIPSeg is that both the query (the image we want to segment) and the prompt (the thing we want to segment in the image) are input as CLIP embeddings. The CLIP embedding for the prompt can either come from a piece of text (the category name), **or from another image**. This means you can segment oranges in a photo by giving CLIPSeg an example image of an orange.\\n\\nThis technique, which is called \"visual prompting\", is really helpful when the thing you want to segment is hard to describe. For example, if you want to segment a logo in a picture of a t-shirt, it\\'s not easy to describe the shape of the logo, but CLIPSeg allows you to simply use the image of the logo as the prompt.\\n\\nThe CLIPSeg paper contains some tips on improving the effectiveness of visual prompting. They find that cropping the query image (so that it only contains the object you want to segment) helps a lot. Blurring and darkening the background of the query image also helps a little bit. In the next section, we\\'ll show how you can try out visual prompting yourself using [`🤗 transformers`](https://huggingface.co/transformers).\\n\\n## Using CLIPSeg with Hugging Face Transformers\\n\\nUsing Hugging Face Transformers, you can easily download and run a\\npre-trained CLIPSeg model on your images. Let\\'s start by installing\\ntransformers.\\n\\n```python\\n!pip install -q transformers\\n```\\n\\nTo download the model, simply instantiate it.\\n\\n```python\\nfrom transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\\n\\nprocessor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\\nmodel = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\\n```\\n\\nNow we can load an image to try out the segmentation. We\\\\\\'ll choose a\\npicture of a delicious breakfast taken by [Calum\\nLewis](https://unsplash.com/@calumlewis).\\n\\n```python\\nfrom PIL import Image\\nimport requests\\n\\nurl = \"https://unsplash.com/photos/8Nc_oQsc2qQ/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjcxMjAwNzI0&force=true&w=640\"\\nimage = Image.open(requests.get(url, stream=True).raw)\\nimage\\n```\\n\\n<figure class=\"image table text-center m-0 w-6/12\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A picture of a pancake breakfast.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/73d97c93dc0f5545378e433e956509b8acafb8d9.png\"></medium-zoom>\\n</figure>\\n\\n### Text prompting\\n\\nLet\\'s start by defining some text categories we want to segment.\\n\\n```python\\nprompts = [\"cutlery\", \"pancakes\", \"blueberries\", \"orange juice\"]\\n```\\n\\nNow that we have our inputs, we can process them and input them to the\\nmodel.\\n\\n```python\\nimport torch\\n\\ninputs = processor(text=prompts, images=[image] * len(prompts), padding=\"max_length\", return_tensors=\"pt\")\\n# predict\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\npreds = outputs.logits.unsqueeze(1)\\n```\\n\\nFinally, let\\'s visualize the output.\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\n_, ax = plt.subplots(1, len(prompts) + 1, figsize=(3*(len(prompts) + 1), 4))\\n[a.axis(\\'off\\') for a in ax.flatten()]\\nax[0].imshow(image)\\n[ax[i+1].imshow(torch.sigmoid(preds[i][0])) for i in range(len(prompts))];\\n[ax[i+1].text(0, -15, prompt) for i, prompt in enumerate(prompts)];\\n```\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"The masks of the different categories in the breakfast image.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/14c048ea92645544c1bbbc9e55f3c620eaab8886.png\"></medium-zoom>\\n</figure>\\n\\n### Visual prompting\\n\\nAs mentioned before, we can also use images as the input prompts (i.e.\\nin place of the category names). This can be especially useful if it\\\\\\'s\\nnot easy to describe the thing you want to segment. For this example,\\nwe\\\\\\'ll use a picture of a coffee cup taken by [Daniel\\nHooper](https://unsplash.com/@dan_fromyesmorecontent).\\n\\n```python\\nurl = \"https://unsplash.com/photos/Ki7sAc8gOGE/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTJ8fGNvZmZlJTIwdG8lMjBnb3xlbnwwfHx8fDE2NzExOTgzNDQ&force=true&w=640\"\\nprompt = Image.open(requests.get(url, stream=True).raw)\\nprompt\\n```\\n\\n<figure class=\"image table text-center m-0 w-6/12\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A picture of a paper coffee cup.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/7931f9db82ab07af7d161f0cfbfc347645da6646.png\"></medium-zoom>\\n</figure>\\n\\nWe can now process the input image and prompt image and input them to\\nthe model.\\n\\n```python\\nencoded_image = processor(images=[image], return_tensors=\"pt\")\\nencoded_prompt = processor(images=[prompt], return_tensors=\"pt\")\\n# predict\\nwith torch.no_grad():\\n  outputs = model(**encoded_image, conditional_pixel_values=encoded_prompt.pixel_values)\\npreds = outputs.logits.unsqueeze(1)\\npreds = torch.transpose(preds, 0, 1)\\n```\\n\\nThen, we can visualize the results as before.\\n\\n```python\\n_, ax = plt.subplots(1, 2, figsize=(6, 4))\\n[a.axis(\\'off\\') for a in ax.flatten()]\\nax[0].imshow(image)\\nax[1].imshow(torch.sigmoid(preds[0]))\\n```\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"The mask of the coffee cup in the breakfast image.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/fbde45fc65907d17de38b0db3eb262bdec1f1784.png\"></medium-zoom>\\n</figure>\\n\\nLet\\'s try one last time by using the visual prompting tips described in\\nthe paper, i.e. cropping the image and darkening the background.\\n\\n```python\\nurl = \"https://i.imgur.com/mRSORqz.jpg\"\\nalternative_prompt = Image.open(requests.get(url, stream=True).raw)\\nalternative_prompt\\n```\\n\\n<figure class=\"image table text-center m-0 w-6/12\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A cropped version of the image of the coffee cup with a darker background.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/915a97da22131e0ab6ff4daa78ffe3f1889e3386.png\"></medium-zoom>\\n</figure>\\n\\n```python\\nencoded_alternative_prompt = processor(images=[alternative_prompt], return_tensors=\"pt\")\\n# predict\\nwith torch.no_grad():\\n  outputs = model(**encoded_image, conditional_pixel_values=encoded_alternative_prompt.pixel_values)\\npreds = outputs.logits.unsqueeze(1)\\npreds = torch.transpose(preds, 0, 1)\\n```\\n\\n```python\\n_, ax = plt.subplots(1, 2, figsize=(6, 4))\\n[a.axis(\\'off\\') for a in ax.flatten()]\\nax[0].imshow(image)\\nax[1].imshow(torch.sigmoid(preds[0]))\\n```\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"The mask of the coffee cup in the breakfast image.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/7f75badfc245fc3a75e0e05058b8c4b6a3a991fa.png\"></medium-zoom>\\n</figure>\\n\\nIn this case, the result is pretty much the same. This is probably\\nbecause the coffee cup was already separated well from the background in\\nthe original image.\\n\\n## Using CLIPSeg to pre-label images on Segments.ai\\n\\nAs you can see, the results from CLIPSeg are a little fuzzy and very\\nlow-res. If we want to obtain better results, you can fine-tune a\\nstate-of-the-art segmentation model, as explained in [our previous\\nblogpost](https://huggingface.co/blog/fine-tune-segformer). To finetune\\nthe model, we\\\\\\'ll need labeled data. In this section, we\\\\\\'ll show you\\nhow you can use CLIPSeg to create some rough segmentation masks and then\\nrefine them on\\n[Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg),\\na labeling platform with smart labeling tools for image segmentation.\\n\\nFirst, create an account at\\n[https://segments.ai/join](https://segments.ai/join?utm_source=hf&utm_medium=blog&utm_campaign=clipseg)\\nand install the Segments Python SDK. Then you can initialize the\\nSegments.ai Python client using an API key. This key can be found on\\n[the account page](https://segments.ai/account?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).\\n\\n```python\\n!pip install -q segments-ai\\n```\\n\\n```python\\nfrom segments import SegmentsClient\\nfrom getpass import getpass\\n\\napi_key = getpass(\\'Enter your API key: \\')\\nsegments_client = SegmentsClient(api_key)\\n```\\n\\nNext, let\\\\\\'s load an image from a dataset using the Segments client.\\nWe\\\\\\'ll use the [a2d2 self-driving\\ndataset](https://www.a2d2.audi/a2d2/en.html). You can also create your\\nown dataset by following [these\\ninstructions](https://docs.segments.ai/tutorials/getting-started?utm_source=hf&utm_medium=blog&utm_campaign=clipseg).\\n\\n```python\\nsamples = segments_client.get_samples(\"admin-tobias/clipseg\")\\n\\n# Use the last image as an example\\nsample = samples[1]\\nimage = Image.open(requests.get(sample.attributes.image.url, stream=True).raw)\\nimage\\n```\\n\\n<figure class=\"image table text-center m-0 w-9/12\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A picture of a street with cars from the a2d2 dataset.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/a0ca3accab5a40547f16b2abc05edd4558818bdf.png\"></medium-zoom>\\n</figure>\\n\\nWe also need to get the category names from the dataset attributes.\\n\\n```python\\ndataset = segments_client.get_dataset(\"admin-tobias/clipseg\")\\ncategory_names = [category.name for category in dataset.task_attributes.categories]\\n```\\n\\nNow we can use CLIPSeg on the image as before. This time, we\\\\\\'ll also\\nscale up the outputs so that they match the input image\\\\\\'s size.\\n\\n```python\\nfrom torch import nn\\n\\ninputs = processor(text=category_names, images=[image] * len(category_names), padding=\"max_length\", return_tensors=\"pt\")\\n\\n# predict\\nwith torch.no_grad():\\n  outputs = model(**inputs)\\n\\n# resize the outputs\\npreds = nn.functional.interpolate(\\n    outputs.logits.unsqueeze(1),\\n    size=(image.size[1], image.size[0]),\\n    mode=\"bilinear\"\\n)\\n```\\n\\nAnd we can visualize the results again.\\n\\n```python\\nlen_cats = len(category_names)\\n_, ax = plt.subplots(1, len_cats + 1, figsize=(3*(len_cats + 1), 4))\\n[a.axis(\\'off\\') for a in ax.flatten()]\\nax[0].imshow(image)\\n[ax[i+1].imshow(torch.sigmoid(preds[i][0])) for i in range(len_cats)];\\n[ax[i+1].text(0, -15, category_name) for i, category_name in enumerate(category_names)];\\n```\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"The masks of the different categories in the street image.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/7782da300097ce4dcb3891257db7cc97ccf1deb3.png\"></medium-zoom>\\n</figure>\\n\\nNow we have to combine the predictions to a single segmented image.\\nWe\\\\\\'ll simply do this by taking the category with the greatest sigmoid\\nvalue for each patch. We\\\\\\'ll also make sure that all the values under a\\ncertain threshold do not count.\\n\\n```python\\nthreshold = 0.1\\n\\nflat_preds = torch.sigmoid(preds.squeeze()).reshape((preds.shape[0], -1))\\n\\n# Initialize a dummy \"unlabeled\" mask with the threshold\\nflat_preds_with_treshold = torch.full((preds.shape[0] + 1, flat_preds.shape[-1]), threshold)\\nflat_preds_with_treshold[1:preds.shape[0]+1,:] = flat_preds\\n\\n# Get the top mask index for each pixel\\ninds = torch.topk(flat_preds_with_treshold, 1, dim=0).indices.reshape((preds.shape[-2], preds.shape[-1]))\\n```\\n\\nLet\\\\\\'s quickly visualize the result.\\n\\n```python\\nplt.imshow(inds)\\n```\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A combined segmentation label of the street image.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/b92dc12452108a0b2769ddfc1d7f79909e65144b.png\"></medium-zoom>\\n</figure>\\n\\nLastly, we can upload the prediction to Segments.ai. To do that, we\\\\\\'ll\\nfirst convert the bitmap to a png file, then we\\\\\\'ll upload this file to\\nthe Segments, and finally we\\\\\\'ll add the label to the sample.\\n\\n```python\\nfrom segments.utils import bitmap2file\\nimport numpy as np\\n\\ninds_np = inds.numpy().astype(np.uint32)\\nunique_inds = np.unique(inds_np).tolist()\\nf = bitmap2file(inds_np, is_segmentation_bitmap=True)\\n\\nasset = segments_client.upload_asset(f, \"clipseg_prediction.png\")\\n\\nattributes = {\\n      \\'format_version\\': \\'0.1\\',\\n      \\'annotations\\': [{\"id\": i, \"category_id\": i} for i in unique_inds if i != 0],\\n      \\'segmentation_bitmap\\': { \\'url\\': asset.url },\\n  }\\n\\nsegments_client.add_label(sample.uuid, \\'ground-truth\\', attributes)\\n```\\n\\nIf you take a look at the [uploaded prediction on\\nSegments.ai](https://segments.ai/admin-tobias/clipseg/samples/71a80d39-8cf3-4768-a097-e81e0b677517/ground-truth),\\nyou can see that it\\\\\\'s not perfect. However, you can manually correct\\nthe biggest mistakes, and then you can use the corrected dataset to\\ntrain a better model than CLIPSeg.\\n\\n<figure class=\"image table text-center m-0 w-9/12\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Thumbnails of the final segmentation labels on Segments.ai.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/segments-thumbs.png\"></medium-zoom>\\n</figure>\\n\\n## Conclusion\\n\\nCLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you’ll probably still want to fine-tune a different segmentation model if accuracy is important. \\n\\nNote that there\\'s more research on zero-shot segmentation currently being conducted, so you can expect more models to be added in the near future. One example is [GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit), which is already available in 🤗 Transformers. To stay up to date with the latest news in segmentation research, you can follow us on Twitter: [@TobiasCornille](https://twitter.com/tobiascornille), [@NielsRogge](https://twitter.com/nielsrogge), and [@huggingface](https://twitter.com/huggingface).\\n\\n\\nIf you’re interested in learning how to fine-tune a state-of-the-art segmentation model, check out our previous blog post: [https://huggingface.co/blog/fine-tune-segformer](https://huggingface.co/blog/fine-tune-segformer).\\n'), Document(metadata={}, page_content='!-- DISABLE-FRONTMATTER-SECTIONS -->\\n\\n# End-of-chapter quiz[[end-of-chapter-quiz]]\\n\\n<CourseFloatingBanner\\n    chapter={9}\\n    classNames=\"absolute z-10 right-0 top-0\"\\n/>\\n\\nLet\\'s test what you learned in this chapter!\\n\\n### 1. What can you use Gradio to do?\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"Create a demo for your machine learning model\",\\n\\t\\t\\texplain: \"With a few lines of python code you can generate a demo for your ML model using our library of pre-built components.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"Share your machine learning model with others\",\\n\\t\\t\\texplain: \"Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone.\",\\n            correct: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"Debug your model\",\\n\\t\\t\\texplain: \"One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model\\'s predictions change in real time, helping you debug your model.\",\\n\\t\\t\\tcorrect: true\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"Train your model\",\\n\\t\\t\\texplain: \"Gradio is designed to be used for model inference, AFTER your model is trained.\",\\n\\t\\t}\\n\\t]}\\n/>\\n\\n### 2. Gradio ONLY works with PyTorch models\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"True\",\\n\\t\\t\\texplain: \"Gradio works with PyTorch models, but also works for any type of machine learning model!\"\\n        },\\n        {\\n\\t\\t\\ttext: \"False\",\\n\\t\\t\\texplain: \"Gradio is model agnostic, meaning you can create a demo for any type of machine learning model.\",\\n\\t\\t\\tcorrect: true\\n        }\\n\\t]}\\n/>\\n\\n### 3. Where can you launch a Gradio demo from?\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"Standard python IDEs\",\\n\\t\\t\\texplain: \"Gradio works great with your favorite IDE.\",\\n            correct: true\\n        },\\n        {\\n\\t\\t\\ttext: \"Google Colab notebooks\",\\n\\t\\t\\texplain: \"You can create and launch a demo within your Google colab notebook.\",\\n\\t\\t\\tcorrect: true\\n        },\\n        {\\n\\t\\t\\ttext: \"Jupyter notebooks\",\\n\\t\\t\\texplain: \"Good choice - You can create and launch a demo within your Jupyter notebook.\",\\n\\t\\t\\tcorrect: true\\n        }\\n\\t]}\\n/>\\n\\n### 4. Gradio is designed primarily for NLP models\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"True\",\\n\\t\\t\\texplain: \"Gradio works with pretty much any data type, not just NLP.\"\\n        },\\n        {\\n\\t\\t\\ttext: \"False\",\\n\\t\\t\\texplain: \"Gradio supplies developers with a library of pre-built components for pretty much all data types.\",\\n            correct: true\\n        }\\n\\t]}\\n/>\\n\\n### 5. Which of the following features are supported by Gradio?\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"Multiple inputs and outputs\",\\n\\t\\t\\texplain: \"Multiple inputs and outputs is possible with gradio. All you need to do is pass in a list of inputs and outputs to their corresponding parameters\",\\n            correct: true\\n        },\\n        {\\n\\t\\t\\ttext: \"State for data persistance\",\\n\\t\\t\\texplain: \"Gradio is capable of adding state to your interface.\",\\n\\t\\t\\tcorrect: true\\n        },\\n        {\\n\\t\\t\\ttext: \"Username and passwords authentication\",\\n\\t\\t\\texplain: \"Pass in a list of username/password tuples to the launch method to add authentication.\",\\n\\t\\t\\tcorrect: true\\n        },\\n        {\\n\\t\\t\\ttext: \"Automatic analytics for who uses your gradio demo\",\\n\\t\\t\\texplain: \"Try again - Gradio does not supply developers analytics on who uses their demos.\"\\n        },\\n        {\\n\\t\\t\\ttext: \"Loading a model from Hugging Face\\'s model hub or Hugging Face Spaces\",\\n\\t\\t\\texplain: \"Absolutely - load any Hugging Face model using the <code>gr.Interface.load()</code> method\",\\n\\t\\t\\tcorrect: true\\n        }\\n\\t]}\\n/>\\n\\n### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"gr.Interface.load(\\'huggingface/{user}/{model_name}\\')\",\\n\\t\\t\\texplain: \"This is a valid method of loading a Hugging Face model from the Hub\",\\n            correct: true\\n        },\\n        {\\n\\t\\t\\ttext: \"gr.Interface.load(\\'model/{user}/{model_name}\\')\",\\n\\t\\t\\texplain: \"This is a valid method of loading a Hugging Face model from the Hub\",\\n\\t\\t\\tcorrect: true\\n        },\\n        {\\n\\t\\t\\ttext: \"gr.Interface.load(\\'demos/{user}/{model_name}\\')\",\\n\\t\\t\\texplain: \"Try again -- you cannot load a model by using the \\'demos\\' prefix.\"\\n        },\\n        {\\n\\t\\t\\ttext: \"gr.Interface.load(\\'spaces/{user}/{model_name}\\')\",\\n\\t\\t\\texplain: \"This is a valid method of loading a Hugging Face model from Spaces\",\\n\\t\\t\\tcorrect: true\\n        }\\n\\t]}\\n/>\\n\\n### 7. Select all the steps necessary for adding state to your Gradio interface\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"Pass in an extra parameter into your prediction function, which represents the state of the interface.\",\\n\\t\\t\\texplain: \"An extra parameter storing history or state of your interface is necessary.\",\\n            correct: true\\n        },\\n        {\\n\\t\\t\\ttext: \"At the end of the prediction function, return the updated value of the state as an extra return value.\",\\n\\t\\t\\texplain: \"This history or state value needs to be returned by your function.\",\\n            correct: true\\n        },\\n        {\\n\\t\\t\\ttext: \"Add the state input and state output components when creating your Interface\",\\n\\t\\t\\texplain: \"Gradio provides a state input and output component to persist data.\",\\n            correct: true\\n        }\\n\\t]}\\n/>\\n\\n### 8. Which of the following are components included in the Gradio library?\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"Textbox.\",\\n\\t\\t\\texplain: \"Yes, you can create textboxes with the Textbox component.\",\\n            correct: true\\n        },\\n        {\\n\\t\\t\\ttext: \"Graph.\",\\n\\t\\t\\texplain: \"There is currently no Graph component.\",\\n        },\\n        {\\n\\t\\t\\ttext: \"Image.\",\\n\\t\\t\\texplain: \"Yes, you can create an image upload widget with the Image component.\",\\n            correct: true\\n        },\\n        {\\n\\t\\t\\ttext: \"Audio.\",\\n\\t\\t\\texplain: \"Yes, you can create an audio upload widget with the Audio component.\",\\n            correct: true\\n        },\\n\\t]}\\n/>\\n\\n### 9. What does Gradio `Blocks` allow you to do?\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"Combine multiple demos into one web app\",\\n\\t\\t\\texplain: \"You can use the `with gradio.Tabs():` to add tabs for multiple demos\",\\n\\t\\t\\tcorrect: true\\n        },\\n        {\\n\\t\\t\\ttext: \"Assign event triggers such as clicked/changed/etc to `Blocks` components\",\\n\\t\\t\\texplain: \"When you assign an event, you pass in three parameters: fn: the function that should be called, inputs: the (list) of input component(s), and outputs: the (list) of output components that should be called.\",\\n\\t\\t\\tcorrect: true\\n        },\\n        {\\n\\t\\t\\ttext: \"Automatically determine which `Blocks` component should be interactive vs. static\",\\n\\t\\t\\texplain: \"Based on the event triggers you define, `Blocks` automatically figures out whether a component should accept user input or not.\",\\n\\t\\t\\tcorrect: true\\n        },\\n\\t\\t {\\n\\t\\t\\ttext: \"Create multi-step demos; meaning allowing you to reuse the output of one component as the input to the next\",\\n\\t\\t\\texplain: \"You can use a component for the input of one event trigger but the output of another.\",\\n            correct: true\\n        },\\n\\t]}\\n/>\\n\\n### 10. You can share a public link to a `Blocks` demo and host a `Blocks` demo on Hugging Face spaces.\\n\\n<Question\\n\\tchoices={[\\n        {\\n\\t\\t\\ttext: \"True\",\\n\\t\\t\\texplain: \"Just like `Interface`, all of the sharing and hosting capabilities are the same for `Blocks` demos!\",\\n\\t\\t\\tcorrect: true\\n        },\\n        {\\n\\t\\t\\ttext: \"False\",\\n\\t\\t\\texplain: \"Just like `Interface`, all of the sharing and hosting capabilities are the same for `Blocks` demos!\",\\n\\t\\t\\tcorrect: false\\n        }\\n\\t]}\\n/>'), Document(metadata={}, page_content=' Tensorflow API\\n\\n[[autodoc]] safetensors.tensorflow.load_file\\n[[autodoc]] safetensors.tensorflow.load\\n[[autodoc]] safetensors.tensorflow.save_file\\n[[autodoc]] safetensors.tensorflow.save\\n'), Document(metadata={}, page_content=' Access and read Logs\\n\\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your Endpoint. \\n\\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\\n\\nThe Container Logs are only available when your Endpoint is in the “Running” state. \\n\\n_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\\n\\n**Build Logs:**\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\\n\\n**Container Logs:**\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />\\n\\n'), Document(metadata={}, page_content='--\\ntitle: Image Classification with AutoTrain \\nthumbnail: /blog/assets/105_autotrain-image-classification/thumbnail.png\\nauthors:\\n- user: nimaboscarino\\n---\\n\\n# Image Classification with AutoTrain\\n\\n\\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\\n\\nSo you’ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There’s just one problem – you don’t know how to code! 😱 Or maybe you’re a seasoned software engineer who wants to add some ML to your side-project, but you don’t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That’s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we’ve just added, we’re making “no-code” machine learning better than ever. Best of all, you can create your first project for ✨ free! ✨\\n\\n[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there\\'s even a good chance that you\\'ll end up with a model that performs better than a model that\\'s been hand-trained by an engineer 🤯 We’ve been expanding the number of tasks that we support, and we’re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we’ve added, with more on the way. But what does this mean for you?\\n\\n[Image Classification](https://huggingface.co/tasks/image-classification) models learn to *categorize* images, meaning that you can train one of these models to label any image. Do you want a model that can recognize signatures? Distinguish bird species? Identify plant diseases? As long as you can find an appropriate dataset, an image classification model has you covered.\\n\\n## How can you train your own image classifier?\\n\\nIf you haven’t [created a Hugging Face account](https://huggingface.co/join) yet, now’s the time! Following that, make your way over to the [AutoTrain homepage](https://huggingface.co/autotrain) and click on “Create new project” to get started. You’ll be asked to fill in some basic info about your project. In the screenshot below you’ll see that I created a project named `butterflies-classification`, and I chose the “Image Classification” task. I’ve also chosen the “Automatic” model option, since I want to let AutoTrain do the work of finding the best model architectures for my project.\\n\\n<div class=\"flex justify-center\">\\n  <figure class=\"image table text-center m-0 w-1/2\">\\n    <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"The \\'New Project\\' form for AutoTrain, filled out for a new Image Classification project named \\'butterflies-classification\\'.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/new-project.png\"></medium-zoom>\\n  </figure>\\n</div>\\n\\nOnce AutoTrain creates your project, you just need to connect your data. If you have the data locally, you can drag and drop the folder into the window. Since we can also use [any of the image classification datasets on the Hugging Face Hub](https://huggingface.co/datasets?task_categories=task_categories:image-classification), in this example I’ve decided to use the [NimaBoscarino/butterflies](https://huggingface.co/datasets/NimaBoscarino/butterflies) dataset. You can select separate training and validation datasets if available, or you can ask AutoTrain to split the data for you.\\n\\n<div class=\"grid grid-cols-2 gap-4\">\\n  <figure class=\"image table text-center m-0 w-full\">\\n  </figure>\\n\\n  <figure class=\"image table text-center m-0 w-full\">\\n    <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"A form showing configurations to select for the imported dataset, including split types and data columns.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/add-dataset.png\"></medium-zoom>\\n  </figure>\\n</div>\\n\\nOnce the data has been added, simply choose the number of model candidates that you’d like AutoModel to try out, review the expected training cost (training with 5 candidate models and less than 500 images is free 🤩), and start training!\\n\\n<div class=\"grid grid-cols-2 gap-4\">\\n  <figure class=\"image table text-center m-0 w-full\">\\n    <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Screenshot showing the model-selection options. Users can choose various numbers of candidate models, and the final training budget is displayed.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/select-models.png\"></medium-zoom>\\n  </figure>\\n  <div>\\n    <figure class=\"image table text-center m-0 w-full\">\\n      <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Five candidate models are being trained, one of which has already completed training.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/training-in-progress.png\"></medium-zoom>\\n    </figure>\\n    <figure class=\"image table text-center m-0 w-full\">\\n      <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"All the candidate models have finished training, with one in the \\'stopped\\' state.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/training-complete.png\"></medium-zoom>\\n    </figure>\\n  </div>\\n</div>\\n\\nIn the screenshots above you can see that my project started 5 different models, which each reached different accuracy scores. One of them wasn’t performing very well at all, so AutoTrain went ahead and stopped it so that it wouldn’t waste resources. The very best model hit 84% accuracy, with effectively zero effort on my end 😍\\xa0 To wrap it all up, you can visit your freshly trained models on the Hub and play around with them through the integrated [inference widget](https://huggingface.co/docs/hub/models-widgets). For example, check out my butterfly classifier model over at [NimaBoscarino/butterflies](https://huggingface.co/NimaBoscarino/butterflies) 🦋\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"An automatically generated model card for the butterflies-classification model, showing validation metrics and an embedded inference widget for image classification. The widget is displaying a picture of a butterfly, which has been identified as a Malachite butterfly.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/model-card.png\"></medium-zoom>\\n</figure>\\n\\nWe’re so excited to see what you build with AutoTrain! Don’t forget to join the community over at [hf.co/join/discord](https://huggingface.co/join/discord), and reach out to us if you need any help 🤗\\n'), Document(metadata={}, page_content='!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Create and manage a repository\\n\\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\\nrepositories on the Hub, especially:\\n\\n- Create and delete a repository.\\n- Manage branches and tags. \\n- Rename your repository.\\n- Update your repository visibility.\\n- Manage a local copy of your repository.\\n\\n<Tip warning={true}>\\n\\nIf you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct\\nmight be to use `git` CLI to clone your repo (`git clone`), commit changes (`git add, git commit`) and push them\\n(`git push`). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do\\nnot share the same requirements and workflows. Model repositories might maintain large model weight files for different\\nframeworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As\\na result, it may be more efficient to use our custom HTTP methods. You can read our [Git vs HTTP paradigm](../concepts/git_vs_http)\\nexplanation page for more details.\\n\\n</Tip>\\n\\nIf you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to\\n[this section](../quick-start#authentication). In the rest of this guide, we will assume that your machine is logged in.\\n\\n## Repo creation and deletion\\n\\nThe first step is to know how to create and delete repositories. You can only manage repositories that you own (under\\nyour username namespace) or from organizations in which you have write permissions.\\n\\n### Create a repository\\n\\nCreate an empty repository with [`create_repo`] and give it a name with the `repo_id` parameter. The `repo_id` is your namespace followed by the repository name: `username_or_org/repo_name`.\\n\\n```py\\n>>> from huggingface_hub import create_repo\\n>>> create_repo(\"lysandre/test-model\")\\n\\'https://huggingface.co/lysandre/test-model\\'\\n```\\n\\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\\n\\n```py\\n>>> from huggingface_hub import create_repo\\n>>> create_repo(\"lysandre/test-dataset\", repo_type=\"dataset\")\\n\\'https://huggingface.co/datasets/lysandre/test-dataset\\'\\n```\\n\\nWhen you create a repository, you can set your repository visibility with the `private` parameter.\\n\\n```py\\n>>> from huggingface_hub import create_repo\\n>>> create_repo(\"lysandre/test-private\", private=True)\\n```\\n\\nIf you want to change the repository visibility at a later time, you can use the [`update_repo_visibility`] function.\\n\\n### Delete a repository\\n\\nDelete a repository with [`delete_repo`]. Make sure you want to delete a repository because this is an irreversible process!\\n\\nSpecify the `repo_id` of the repository you want to delete:\\n\\n```py\\n>>> delete_repo(repo_id=\"lysandre/my-corrupted-dataset\", repo_type=\"dataset\")\\n```\\n\\n### Duplicate a repository (only for Spaces)\\n\\nIn some cases, you want to copy someone else\\'s repo to adapt it to your use case.\\nThis is possible for Spaces using the [`duplicate_space`] method. It will duplicate the whole repository.\\nYou will still need to configure your own settings (hardware, sleep-time, storage, variables and secrets). Check out our [Manage your Space](./manage-spaces) guide for more details.\\n\\n```py\\n>>> from huggingface_hub import duplicate_space\\n>>> duplicate_space(\"multimodalart/dreambooth-training\", private=False)\\nRepoUrl(\\'https://huggingface.co/spaces/nateraw/dreambooth-training\\',...)\\n```\\n\\n## Upload and download files\\n\\nNow that you have created your repository, you are interested in pushing changes to it and downloading files from it.\\n\\nThese 2 topics deserve their own guides. Please refer to the [upload](./upload) and the [download](./download) guides\\nto learn how to use your repository.\\n\\n\\n## Branches and tags\\n\\nGit repositories often make use of branches to store different versions of a same repository.\\nTags can also be used to flag a specific state of your repository, for example, when releasing a version.\\nMore generally, branches and tags are referred as [git references](https://git-scm.com/book/en/v2/Git-Internals-Git-References).\\n\\n### Create branches and tags\\n\\nYou can create new branch and tags using [`create_branch`] and [`create_tag`]:\\n\\n```py\\n>>> from huggingface_hub import create_branch, create_tag\\n\\n# Create a branch on a Space repo from `main` branch\\n>>> create_branch(\"Matthijs/speecht5-tts-demo\", repo_type=\"space\", branch=\"handle-dog-speaker\")\\n\\n# Create a tag on a Dataset repo from `v0.1-release` branch\\n>>> create_branch(\"bigcode/the-stack\", repo_type=\"dataset\", revision=\"v0.1-release\", tag=\"v0.1.1\", tag_message=\"Bump release version.\")\\n```\\n\\nYou can use the [`delete_branch`] and [`delete_tag`] functions in the same way to delete a branch or a tag.\\n\\n### List all branches and tags\\n\\nYou can also list the existing git refs from a repository using [`list_repo_refs`]:\\n\\n```py\\n>>> from huggingface_hub import list_repo_refs\\n>>> list_repo_refs(\"bigcode/the-stack\", repo_type=\"dataset\")\\nGitRefs(\\n   branches=[\\n         GitRefInfo(name=\\'main\\', ref=\\'refs/heads/main\\', target_commit=\\'18edc1591d9ce72aa82f56c4431b3c969b210ae3\\'),\\n         GitRefInfo(name=\\'v1.1.a1\\', ref=\\'refs/heads/v1.1.a1\\', target_commit=\\'f9826b862d1567f3822d3d25649b0d6d22ace714\\')\\n   ],\\n   converts=[],\\n   tags=[\\n         GitRefInfo(name=\\'v1.0\\', ref=\\'refs/tags/v1.0\\', target_commit=\\'c37a8cd1e382064d8aced5e05543c5f7753834da\\')\\n   ]\\n)\\n``` \\n\\n## Change repository settings\\n\\nRepositories come with some settings that you can configure. Most of the time, you will want to do that manually in the\\nrepo settings page in your browser. You must have write access to a repo to configure it (either own it or being part of\\nan organization). In this section, we will see the settings that you can also configure programmatically using `huggingface_hub`.\\n\\nSome settings are specific to Spaces (hardware, environment variables,...). To configure those, please refer to our [Manage your Spaces](../guides/manage-spaces) guide.\\n\\n### Update visibility\\n\\nA repository can be public or private. A private repository is only visible to you or members of the organization in which the repository is located. Change a repository to private as shown in the following:\\n\\n```py\\n>>> from huggingface_hub import update_repo_visibility\\n>>> update_repo_visibility(repo_id=repo_id, private=True)\\n```\\n\\n### Rename your repository\\n\\nYou can rename your repository on the Hub using [`move_repo`]. Using this method, you can also move the repo from a user to\\nan organization. When doing so, there are a [few limitations](https://hf.co/docs/hub/repositories-settings#renaming-or-transferring-a-repo)\\nthat you should be aware of. For example, you can\\'t transfer your repo to another user.\\n\\n```py\\n>>> from huggingface_hub import move_repo\\n>>> move_repo(from_id=\"Wauplin/cool-model\", to_id=\"huggingface/cool-model\")\\n```\\n\\n## Manage a local copy of your repository\\n\\nAll the actions described above can be done using HTTP requests. However, in some cases you might be interested in having\\na local copy of your repository and interact with it using the Git commands you are familiar with.\\n\\nThe [`Repository`] class allows you to interact with files and repositories on the Hub with functions similar to Git commands. It is a wrapper over Git and Git-LFS methods to use the Git commands you already know and love. Before starting, please make sure you have Git-LFS installed (see [here](https://git-lfs.github.com/) for installation instructions).\\n\\n<Tip warning={true}>\\n\\n[`Repository`] is deprecated in favor of the http-based alternatives implemented in [`HfApi`]. Given its large adoption in legacy code, the complete removal of [`Repository`] will only happen in release `v1.0`. For more details, please read [this explanation page](./concepts/git_vs_http).\\n\\n</Tip>\\n\\n### Use a local repository\\n\\nInstantiate a [`Repository`] object with a path to a local repository:\\n\\n```py\\n>>> from huggingface_hub import Repository\\n>>> repo = Repository(local_dir=\"<path>/<to>/<folder>\")\\n```\\n\\n### Clone\\n\\nThe `clone_from` parameter clones a repository from a Hugging Face repository ID to a local directory specified by the `local_dir` argument:\\n\\n```py\\n>>> from huggingface_hub import Repository\\n>>> repo = Repository(local_dir=\"w2v2\", clone_from=\"facebook/wav2vec2-large-960h-lv60\")\\n```\\n\\n`clone_from` can also clone a repository using a URL:\\n\\n```py\\n>>> repo = Repository(local_dir=\"huggingface-hub\", clone_from=\"https://huggingface.co/facebook/wav2vec2-large-960h-lv60\")\\n```\\n\\nYou can combine the `clone_from` parameter with [`create_repo`] to create and clone a repository:\\n\\n```py\\n>>> repo_url = create_repo(repo_id=\"repo_name\")\\n>>> repo = Repository(local_dir=\"repo_local_path\", clone_from=repo_url)\\n```\\n\\nYou can also configure a Git username and email to a cloned repository by specifying the `git_user` and `git_email` parameters when you clone a repository. When users commit to that repository, Git will be aware of the commit author.\\n\\n```py\\n>>> repo = Repository(\\n...   \"my-dataset\",\\n...   clone_from=\"<user>/<dataset_id>\",\\n...   token=True,\\n...   repo_type=\"dataset\",\\n...   git_user=\"MyName\",\\n...   git_email=\"me@cool.mail\"\\n... )\\n```\\n\\n### Branch\\n\\nBranches are important for collaboration and experimentation without impacting your current files and code. Switch between branches with [`~Repository.git_checkout`]. For example, if you want to switch from `branch1` to `branch2`:\\n\\n```py\\n>>> from huggingface_hub import Repository\\n>>> repo = Repository(local_dir=\"huggingface-hub\", clone_from=\"<user>/<dataset_id>\", revision=\\'branch1\\')\\n>>> repo.git_checkout(\"branch2\")\\n```\\n\\n### Pull\\n\\n[`~Repository.git_pull`] allows you to update a current local branch with changes from a remote repository:\\n\\n```py\\n>>> from huggingface_hub import Repository\\n>>> repo.git_pull()\\n```\\n\\nSet `rebase=True` if you want your local commits to occur after your branch is updated with the new commits from the remote:\\n\\n```py\\n>>> repo.git_pull(rebase=True)\\n```\\n'), Document(metadata={}, page_content=' List splits and configurations\\n\\nDatasets typically have splits and may also have configurations. A _split_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A _configuration_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you\\'re interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load_hub)!\\n\\nThis guide shows you how to use Datasets Server\\'s `/splits` endpoint to retrieve a dataset\\'s splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\\n\\nThe `/splits` endpoint accepts the dataset name as its query parameter:\\n\\n<inferencesnippet>\\n<python>\\n```python\\nimport requests\\nheaders = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\\nAPI_URL = \"https://datasets-server.huggingface.co/splits?dataset=duorc\"\\ndef query():\\n    response = requests.get(API_URL, headers=headers)\\n    return response.json()\\ndata = query()\\n```\\n</python>\\n<js>\\n```js\\nimport fetch from \"node-fetch\";\\nasync function query(data) {\\n    const response = await fetch(\\n        \"https://datasets-server.huggingface.co/splits?dataset=duorc\",\\n        {\\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\\n            method: \"GET\"\\n        }\\n    );\\n    const result = await response.json();\\n    return result;\\n}\\nquery().then((response) => {\\n    console.log(JSON.stringify(response));\\n});\\n```\\n</js>\\n<curl>\\n```curl\\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\\\\n        -X GET \\\\\\n        -H \"Authorization: Bearer ${API_TOKEN}\"\\n```\\n</curl>\\n</inferencesnippet>\\n\\nThe endpoint response is a JSON containing a list of the dataset\\'s splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\\n\\n```json\\n{\\n  \"splits\": [\\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"train\" },\\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"validation\" },\\n    { \"dataset\": \"duorc\", \"config\": \"ParaphraseRC\", \"split\": \"test\" },\\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"train\" },\\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"validation\" },\\n    { \"dataset\": \"duorc\", \"config\": \"SelfRC\", \"split\": \"test\" }\\n  ],\\n  \"pending\": [],\\n  \"failed\": []\\n}\\n```\\n'), Document(metadata={}, page_content='!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Fully Sharded Data Parallel\\n\\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\\n\\n<Tip warning={true}>\\n\\nCurrently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\\n\\n</Tip>\\n\\nFSDP is supported in 🤗 Accelerate, and you can use it with 🤗 PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py). You\\'ll configure the script to train a large model for conditional generation.\\n\\n## Configuration\\n\\nBegin by running the following command to [create a FSDP configuration file](https://huggingface.co/docs/accelerate/main/en/usage_guides/fsdp) with 🤗 Accelerate. Use the `--config_file` flag to save the configuration file to a specific location, otherwise it is saved as a `default_config.yaml` file in the 🤗 Accelerate cache.\\n\\nThe configuration file is used to set the default options when you launch the training script.\\n\\n```bash\\naccelerate config --config_file fsdp_config.yaml\\n```\\n\\nYou\\'ll be asked a few questions about your setup, and configure the following arguments. For this example, make sure you fully shard the model parameters, gradients, optimizer states, leverage the CPU for offloading, and wrap model layers based on the Transformer layer class name.\\n\\n```bash\\n`Sharding Strategy`: [1] FULL_SHARD (shards optimizer states, gradients and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD\\n`Offload Params`: Decides Whether to offload parameters and gradients to CPU\\n`Auto Wrap Policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3] NO_WRAP \\n`Transformer Layer Class to Wrap`: When using `TRANSFORMER_BASED_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g, \\n`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`...\\n`Min Num Params`: minimum number of parameters when using `SIZE_BASED_WRAP`\\n`Backward Prefetch`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH\\n`State Dict Type`: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT  \\n```\\n\\nFor example, your FSDP configuration file may look like the following:\\n\\n```yaml\\ncommand_file: null\\ncommands: null\\ncompute_environment: LOCAL_MACHINE\\ndeepspeed_config: {}\\ndistributed_type: FSDP\\ndowncast_bf16: \\'no\\'\\ndynamo_backend: \\'NO\\'\\nfsdp_config:\\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\\n  fsdp_offload_params: true\\n  fsdp_sharding_strategy: 1\\n  fsdp_state_dict_type: FULL_STATE_DICT\\n  fsdp_transformer_layer_cls_to_wrap: T5Block\\ngpu_ids: null\\nmachine_rank: 0\\nmain_process_ip: null\\nmain_process_port: null\\nmain_training_function: main\\nmegatron_lm_config: {}\\nmixed_precision: \\'no\\'\\nnum_machines: 1\\nnum_processes: 2\\nrdzv_backend: static\\nsame_network: true\\ntpu_name: null\\ntpu_zone: null\\nuse_cpu: false\\n```\\n\\n## The important parts\\n\\nLet\\'s dig a bit deeper into the training script to understand how it works.\\n\\nThe [`main()`](https://github.com/huggingface/peft/blob/2822398fbe896f25d4dac5e468624dc5fd65a51b/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py#L14) function begins with initializing an [`~accelerate.Accelerator`] class which handles everything for distributed training, such as automatically detecting your training environment.\\n\\n<Tip>\\n\\n💡 Feel free to change the model and dataset inside the `main` function. If your dataset format is different from the one in the script, you may also need to write your own preprocessing function. \\n\\n</Tip>\\n\\nThe script also creates a configuration corresponding to the 🤗 PEFT method you\\'re using. For LoRA, you\\'ll use [`LoraConfig`] to specify the task type, and several other important parameters such as the dimension of the low-rank matrices, the matrices scaling factor, and the dropout probability of the LoRA layers. If you want to use a different 🤗 PEFT method, replace `LoraConfig` with the appropriate [class](../package_reference/tuners).\\n\\nNext, the script wraps the base model and `peft_config` with the [`get_peft_model`] function to create a [`PeftModel`]. \\n\\n```diff\\n def main():\\n+    accelerator = Accelerator()\\n     model_name_or_path = \"t5-base\"\\n     base_path = \"temp/data/FinancialPhraseBank-v1.0\"\\n+    peft_config = LoraConfig(\\n         task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\\n     )\\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\\n+   model = get_peft_model(model, peft_config)\\n```\\n\\nThroughout the script, you\\'ll see the [`~accelerate.Accelerator.main_process_first`] and [`~accelerate.Accelerator.wait_for_everyone`] functions which help control and synchronize when processes are executed.\\n\\nAfter your dataset is prepared, and all the necessary training components are loaded, the script checks if you\\'re using the `fsdp_plugin`. PyTorch offers two ways for wrapping model layers in FSDP, automatically or manually. The simplest method is to allow FSDP to automatically recursively wrap model layers without changing any other code. You can choose to wrap the model layers based on the layer name or on the size (number of parameters). In the FSDP configuration file, it uses the `TRANSFORMER_BASED_WRAP` option to wrap the [`T5Block`] layer.\\n\\n```py\\nif getattr(accelerator.state, \"fsdp_plugin\", None) is not None:\\n    accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\\n```\\n\\nNext, use 🤗 Accelerate\\'s [`~accelerate.Accelerator.prepare`] function to prepare the model, datasets, optimizer, and scheduler for training.\\n\\n```py\\nmodel, train_dataloader, eval_dataloader, optimizer, lr_scheduler = accelerator.prepare(\\n    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler\\n)\\n```\\n\\nFrom here, the remainder of the script handles the training loop, evaluation, and sharing your model to the Hub.\\n\\n## Train\\n\\nRun the following command to launch the training script. Earlier, you saved the configuration file to `fsdp_config.yaml`, so you\\'ll need to pass the path to the launcher with the `--config_file` argument like this:\\n\\n```bash\\naccelerate launch --config_file fsdp_config.yaml examples/peft_lora_seq2seq_accelerate_fsdp.py\\n```\\n\\nOnce training is complete, the script returns the accuracy and compares the predictions to the labels.'), Document(metadata={}, page_content=\" Convert weights to safetensors\\n\\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\\n\\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\\n\\n<Tip warning={true}>\\n\\nFor larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\\n\\nFeel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\\n\\n</Tip>\\n\"), Document(metadata={}, page_content=' Security\\n\\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\\n\\nHugging Face is GDPR compliant. If a contract or specific data storage is something you\\'ll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). \\n\\nHugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\\n\\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\\n\\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\\n\\n## Contents\\n\\n- [User Access Tokens](./security-tokens)\\n- [Git over SSH](./security-git-ssh)\\n- [Signing commits with GPG](./security-gpg)\\n- [Single Sign-On (SSO)](./security-sso)\\n- [Malware Scanning](./security-malware)\\n- [Pickle Scanning](./security-pickle)\\n- [Secrets Scanning](./security-secrets)\\n\\n'), Document(metadata={}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# RAG\\n\\n<div class=\"flex flex-wrap space-x-1\">\\n<a href=\"https://huggingface.co/models?filter=rag\">\\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\">\\n</a>\\n</div>\\n\\n## Overview\\n\\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\\nsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\\nboth retrieval and generation to adapt to downstream tasks.\\n\\nIt is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.\\n\\nThe abstract from the paper is the following:\\n\\n*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve\\nstate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely\\nmanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind\\ntask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge\\nremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric\\nmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a\\ngeneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained\\nparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\\npre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\\nmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation\\ntasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\\nparametric-only seq2seq baseline.*\\n\\nThis model was contributed by [ola13](https://huggingface.co/ola13).\\n\\n## Usage tips\\n\\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \\nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \\nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \\nto downstream tasks.\\n\\n## RagConfig\\n\\n[[autodoc]] RagConfig\\n\\n## RagTokenizer\\n\\n[[autodoc]] RagTokenizer\\n\\n## Rag specific outputs\\n\\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\\n\\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\\n\\n## RagRetriever\\n\\n[[autodoc]] RagRetriever\\n\\n<frameworkcontent>\\n<pt>\\n\\n## RagModel\\n\\n[[autodoc]] RagModel\\n    - forward\\n\\n## RagSequenceForGeneration\\n\\n[[autodoc]] RagSequenceForGeneration\\n    - forward\\n    - generate\\n\\n## RagTokenForGeneration\\n\\n[[autodoc]] RagTokenForGeneration\\n    - forward\\n    - generate\\n\\n</pt>\\n<tf>\\n\\n## TFRagModel\\n\\n[[autodoc]] TFRagModel\\n    - call\\n\\n## TFRagSequenceForGeneration\\n\\n[[autodoc]] TFRagSequenceForGeneration\\n    - call\\n    - generate\\n\\n## TFRagTokenForGeneration\\n\\n[[autodoc]] TFRagTokenForGeneration\\n    - call\\n    - generate\\n\\n</tf>\\n</frameworkcontent>\\n'), Document(metadata={}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# MarkupLM\\n\\n## Overview\\n\\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\\nperformance, similar to [LayoutLM](layoutlm).\\n\\nThe model can be used for tasks like question answering on web pages or information extraction from web pages. It obtains\\nstate-of-the-art results on 2 important benchmarks:\\n- [WebSRC](https://x-lance.github.io/WebSRC/), a dataset for Web-Based Structural Reading Comprehension (a bit like SQuAD but for web pages)\\n- [SWDE](https://www.researchgate.net/publication/221299838_From_one_tree_to_a_forest_a_unified_solution_for_structured_web_data_extraction), a dataset\\nfor information extraction from web pages (basically named-entity recogntion on web pages)\\n\\nThe abstract from the paper is the following:\\n\\n*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document\\nUnderstanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a\\nlarge number of digital documents where the layout information is not fixed and needs to be interactively and\\ndynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this\\npaper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as\\nHTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the\\npre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding\\ntasks. The pre-trained model and code will be publicly available.*\\n\\nThis model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\\n\\n## Usage tips\\n\\n- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.\\nThese are the XPATH tags and subscripts respectively for each token in the input sequence.\\n- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.\\n\\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\\nalt=\"drawing\" width=\"600\"/> \\n\\n<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\\n\\n## Usage: MarkupLMProcessor\\n\\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\\nif you only want to handle one of the two tasks.\\n\\n```python\\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor\\n\\nfeature_extractor = MarkupLMFeatureExtractor()\\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\\n```\\n\\nIn short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\\nand it will create the inputs expected by the model. Internally, the processor first uses\\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.\\n\\n[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\\n\\nIn total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\\n\\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**\\n\\nThis is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\\n\\n```python\\n>>> from transformers import MarkupLMProcessor\\n\\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n\\n>>> html_string = \"\"\"\\n...  <!DOCTYPE html>\\n...  <html>\\n...  <head>\\n...  <title>Hello world</title>\\n...  </head>\\n...  <body>\\n...  <h1>Welcome</h1>\\n...  <p>Here is my website.</p>\\n...  </body>\\n...  </html>\"\"\"\\n\\n>>> # note that you can also add provide all tokenizer parameters here such as padding, truncation\\n>>> encoding = processor(html_string, return_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict_keys([\\'input_ids\\', \\'token_type_ids\\', \\'attention_mask\\', \\'xpath_tags_seq\\', \\'xpath_subs_seq\\'])\\n```\\n\\n**Use case 2: web page classification (training, inference) + token classification (inference), parse_html=False**\\n\\nIn case one already has obtained all nodes and xpaths, one doesn\\'t need the feature extractor. In that case, one should\\nprovide the nodes and corresponding xpaths themselves to the processor, and make sure to set `parse_html` to `False`.\\n\\n```python\\n>>> from transformers import MarkupLMProcessor\\n\\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n>>> processor.parse_html = False\\n\\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, return_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict_keys([\\'input_ids\\', \\'token_type_ids\\', \\'attention_mask\\', \\'xpath_tags_seq\\', \\'xpath_subs_seq\\'])\\n```\\n\\n**Use case 3: token classification (training), parse_html=False**\\n\\nFor token classification tasks (such as [SWDE](https://paperswithcode.com/dataset/swde)), one can also provide the\\ncorresponding node labels in order to train a model. The processor will then convert these into token-level `labels`.\\nBy default, it will only label the first wordpiece of a word, and label the remaining wordpieces with -100, which is the\\n`ignore_index` of PyTorch\\'s CrossEntropyLoss. In case you want all wordpieces of a word to be labeled, you can\\ninitialize the tokenizer with `only_label_first_subword` set to `False`.\\n\\n```python\\n>>> from transformers import MarkupLMProcessor\\n\\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n>>> processor.parse_html = False\\n\\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\\n>>> node_labels = [1, 2, 2, 1]\\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, return_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict_keys([\\'input_ids\\', \\'token_type_ids\\', \\'attention_mask\\', \\'xpath_tags_seq\\', \\'xpath_subs_seq\\', \\'labels\\'])\\n```\\n\\n**Use case 4: web page question answering (inference), parse_html=True**\\n\\nFor question answering tasks on web pages, you can provide a question to the processor. By default, the\\nprocessor will use the feature extractor to get all nodes and xpaths, and create [CLS] question tokens [SEP] word tokens [SEP].\\n\\n```python\\n>>> from transformers import MarkupLMProcessor\\n\\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n\\n>>> html_string = \"\"\"\\n...  <!DOCTYPE html>\\n...  <html>\\n...  <head>\\n...  <title>Hello world</title>\\n...  </head>\\n...  <body>\\n...  <h1>Welcome</h1>\\n...  <p>My name is Niels.</p>\\n...  </body>\\n...  </html>\"\"\"\\n\\n>>> question = \"What\\'s his name?\"\\n>>> encoding = processor(html_string, questions=question, return_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict_keys([\\'input_ids\\', \\'token_type_ids\\', \\'attention_mask\\', \\'xpath_tags_seq\\', \\'xpath_subs_seq\\'])\\n```\\n\\n**Use case 5: web page question answering (inference), parse_html=False**\\n\\nFor question answering tasks (such as WebSRC), you can provide a question to the processor. If you have extracted\\nall nodes and xpaths yourself, you can provide them directly to the processor. Make sure to set `parse_html` to `False`.\\n\\n```python\\n>>> from transformers import MarkupLMProcessor\\n\\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\\n>>> processor.parse_html = False\\n\\n>>> nodes = [\"hello\", \"world\", \"how\", \"are\"]\\n>>> xpaths = [\"/html/body/div/li[1]/div/span\", \"/html/body/div/li[1]/div/span\", \"html/body\", \"html/body/div\"]\\n>>> question = \"What\\'s his name?\"\\n>>> encoding = processor(nodes=nodes, xpaths=xpaths, questions=question, return_tensors=\"pt\")\\n>>> print(encoding.keys())\\ndict_keys([\\'input_ids\\', \\'token_type_ids\\', \\'attention_mask\\', \\'xpath_tags_seq\\', \\'xpath_subs_seq\\'])\\n```\\n\\n## Resources\\n\\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n\\n## MarkupLMConfig\\n\\n[[autodoc]] MarkupLMConfig\\n    - all\\n\\n## MarkupLMFeatureExtractor\\n\\n[[autodoc]] MarkupLMFeatureExtractor\\n    - __call__\\n\\n## MarkupLMTokenizer\\n\\n[[autodoc]] MarkupLMTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## MarkupLMTokenizerFast\\n\\n[[autodoc]] MarkupLMTokenizerFast\\n    - all\\n\\n## MarkupLMProcessor\\n\\n[[autodoc]] MarkupLMProcessor\\n    - __call__\\n\\n## MarkupLMModel\\n\\n[[autodoc]] MarkupLMModel\\n    - forward\\n\\n## MarkupLMForSequenceClassification\\n\\n[[autodoc]] MarkupLMForSequenceClassification\\n    - forward\\n\\n## MarkupLMForTokenClassification\\n\\n[[autodoc]] MarkupLMForTokenClassification\\n    - forward\\n\\n## MarkupLMForQuestionAnswering\\n\\n[[autodoc]] MarkupLMForQuestionAnswering\\n    - forward\\n'), Document(metadata={}, page_content=\" Managing Spaces with Github Actions\\n\\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\\n\\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\\n\\n```bash\\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\\n```\\n\\nThen force push to sync everything for the first time:\\n\\n```bash\\ngit push --force space main\\n```\\n\\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\\n\\n* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \\n* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.\\n\\n```yaml\\nname: Sync to Hugging Face hub\\non:\\n  push:\\n    branches: [main]\\n\\n  # to run this workflow manually from the Actions tab\\n  workflow_dispatch:\\n\\njobs:\\n  sync-to-hub:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - uses: actions/checkout@v3\\n        with:\\n          fetch-depth: 0\\n          lfs: true\\n      - name: Push to hub\\n        env:\\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\\n        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\\n```\\n\\nFinally, create an Action that automatically checks the file size of any new pull request:\\n\\n\\n```yaml\\nname: Check file size\\non:               # or directly `on: [push]` to run the action on every push on any branch\\n  pull_request:\\n    branches: [main]\\n\\n  # to run this workflow manually from the Actions tab\\n  workflow_dispatch:\\n\\njobs:\\n  sync-to-hub:\\n    runs-on: ubuntu-latest\\n    steps:\\n      - name: Check large files\\n        uses: ActionsDesk/lfs-warning@v2.0\\n        with:\\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\\n```\\n\"), Document(metadata={}, page_content='!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# ByT5\\n\\n## Overview\\n\\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\\nKale, Adam Roberts, Colin Raffel.\\n\\nThe abstract from the paper is the following:\\n\\n*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units.\\nEncoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\\nthe model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\\ncan process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\\nremoving complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\\nsequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of\\noperating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with\\nminimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,\\ntraining FLOPs, and inference speed, and show that byte-level models are competitive with their token-level\\ncounterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on\\ntasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of\\npre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\\nexperiments.*\\n\\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\\nfound [here](https://github.com/google-research/byt5).\\n\\n<Tip>\\n\\nByT5\\'s architecture is based on the T5v1.1 model, refer to [T5v1.1\\'s documentation page](t5v1.1) for the API reference. They\\nonly differ in how inputs should be prepared for the model, see the code examples below.\\n\\n</Tip>\\n\\nSince ByT5 was pre-trained unsupervisedly, there\\'s no real advantage to using a task prefix during single-task\\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\\n\\n\\n## Usage example\\n\\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\\n\\n```python\\n>>> from transformers import T5ForConditionalGeneration\\n>>> import torch\\n\\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\\n\\n>>> num_special_tokens = 3\\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.\\n>>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.\\n\\n>>> input_ids = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + num_special_tokens\\n\\n>>> labels = torch.tensor([list(\"La vie est comme une boîte de chocolat.\".encode(\"utf-8\"))]) + num_special_tokens\\n\\n>>> loss = model(input_ids, labels=labels).loss\\n>>> loss.item()\\n2.66\\n```\\n\\nFor batched inference and training it is however recommended to make use of the tokenizer:\\n\\n```python\\n>>> from transformers import T5ForConditionalGeneration, AutoTokenizer\\n\\n>>> model = T5ForConditionalGeneration.from_pretrained(\"google/byt5-small\")\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-small\")\\n\\n>>> model_inputs = tokenizer(\\n...     [\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\"\\n... )\\n>>> labels_dict = tokenizer(\\n...     [\"La vie est comme une boîte de chocolat.\", \"Aujourd\\'hui c\\'est lundi.\"], padding=\"longest\", return_tensors=\"pt\"\\n... )\\n>>> labels = labels_dict.input_ids\\n\\n>>> loss = model(**model_inputs, labels=labels).loss\\n>>> loss.item()\\n17.9\\n```\\n\\nSimilar to [T5](t5), ByT5 was trained on the span-mask denoising task. However, \\nsince the model works directly on characters, the pretraining task is a bit \\ndifferent. Let\\'s corrupt some characters of the \\ninput sentence `\"The dog chases a ball in the park.\"` and ask ByT5 to predict them \\nfor us.\\n\\n```python\\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\n>>> import torch\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-base\")\\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-base\")\\n\\n>>> input_ids_prompt = \"The dog chases a ball in the park.\"\\n>>> input_ids = tokenizer(input_ids_prompt).input_ids\\n\\n>>> # Note that we cannot add \"{extra_id_...}\" to the string directly\\n>>> # as the Byte tokenizer would incorrectly merge the tokens\\n>>> # For ByT5, we need to work directly on the character level\\n>>> # Contrary to T5, ByT5 does not use sentinel tokens for masking, but instead\\n>>> # uses final utf character ids.\\n>>> # UTF-8 is represented by 8 bits and ByT5 has 3 special tokens.\\n>>> # => There are 2**8+2 = 259 input ids and mask tokens count down from index 258.\\n>>> # => mask to \"The dog [258]a ball [257]park.\"\\n\\n>>> input_ids = torch.tensor([input_ids[:8] + [258] + input_ids[14:21] + [257] + input_ids[28:]])\\n>>> input_ids\\ntensor([[ 87, 107, 104,  35, 103, 114, 106,  35, 258,  35, 100,  35, 101, 100, 111, 111, 257,  35, 115, 100, 117, 110,  49,   1]])\\n\\n>>> # ByT5 produces only one char at a time so we need to produce many more output characters here -> set `max_length=100`.\\n>>> output_ids = model.generate(input_ids, max_length=100)[0].tolist()\\n>>> output_ids\\n[0, 258, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118, 257,  35, 108, 113,  35, 119, 107, 104,  35, 103, 108, 118, 102, 114, 256, 108, 113,  35, 119, 107, 104, 35, 115, 100, 117, 110,  49,  35,  87, 107, 104,  35, 103, 114, 106, 35, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118,  35, 100,  35, 101, 100, 111, 111,  35, 108, 113, 255,  35, 108, 113,  35, 119, 107, 104,  35, 115, 100, 117, 110,  49]\\n\\n>>> # ^- Note how 258 descends to 257, 256, 255\\n\\n>>> # Now we need to split on the sentinel tokens, let\\'s write a short loop for this\\n>>> output_ids_list = []\\n>>> start_token = 0\\n>>> sentinel_token = 258\\n>>> while sentinel_token in output_ids:\\n...     split_idx = output_ids.index(sentinel_token)\\n...     output_ids_list.append(output_ids[start_token:split_idx])\\n...     start_token = split_idx\\n...     sentinel_token -= 1\\n\\n>>> output_ids_list.append(output_ids[start_token:])\\n>>> output_string = tokenizer.batch_decode(output_ids_list)\\n>>> output_string\\n[\\'<pad>\\', \\'is the one who does\\', \\' in the disco\\', \\'in the park. The dog is the one who does a ball in\\', \\' in the park.\\']\\n```\\n\\n\\n## ByT5Tokenizer\\n\\n[[autodoc]] ByT5Tokenizer\\n\\nSee [`ByT5Tokenizer`] for all details.\\n'), Document(metadata={}, page_content='n this video, we\\'ll study the encoder architecture. An example of a popular encoder-only architecture is BERT, which is the most popular model of its kind. Let\\'s first start by understanding how it works. We\\'ll use a small example, using three words. We use these as inputs, and pass them through the encoder. We retrieve a numerical representation of each word. Here, for example, the encoder converts the three words “Welcome to NYC” in these three sequences of numbers. The encoder outputs exactly one sequence of numbers per input word. This numerical representation can also be called a \"Feature vector\", or \"Feature tensor\".\\nLet\\'s dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn\\'t the representation of only the \"to\" word. It also takes into account the words around it, which we call the “context”.As in, it looks to the left context, the word on the left of the one we\\'re studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the \"meaning\" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence. As we\\'ve seen before, this means that the resulting representation of a word has been affected by other words in the sequence. We won\\'t dive into the specifics here, but we\\'ll offer some further readings if you want to get a better understanding at what happens under the hood. So when should one use an encoder? Encoders can be used as standalone models in a wide variety of tasks. For example BERT, arguably the most famous transformer model, is a standalone encoder model and at the time of release, beat the state of the art in many sequence classification tasks, question answering tasks, and masked language modeling, to only cite a few. The idea is that encoders are very powerful at extracting vectors that carry meaningful information about a sequence. This vector can then be handled down the road by additional layers of neurons to make sense of them. Let\\'s take a look at some examples where encoders really shine. First of all, Masked Language Modeling, or MLM.\\xa0It\\'s the task of predicting a hidden word in a sequence of words. Here, for example, we have hidden the word between \"My\" and \"is\". This is one of the objectives with which BERT was trained: it was trained to predict hidden words in a sequence. Encoders shine in this scenario in particular, as bidirectional information is crucial here. If we didn\\'t have the words on the right (is, Sylvain, and the dot), then there is very little chance that BERT would have been able to identify \"name\" as the correct word. The encoder needs to have a good understanding of the sequence in order to predict a masked word, as even if the text is grammatically correct, It does not necessarily make sense in the context of the sequence. As mentioned earlier, encoders are good at doing sequence classification. Sentiment analysis is an example of a sequence classification task. The model\\'s aim is to identify the sentiment of a sequence – it can range from giving a sequence a rating from one to five stars if doing review analysis, to giving a positive or negative rating to a sequence, which is what is shown here. For example here, given the two sequences, we use the model to compute a prediction and to classify the sequences among these two classes: positive and negative. While the two sequences are very similar, containing the same words, the meaning is different – and the encoder model is able to grasp that difference.'), Document(metadata={}, page_content=' Components\\n\\nWhen building a Tokenizer, you can attach various types of components to\\nthis Tokenizer in order to customize its behavior. This page lists most\\nprovided components.\\n\\n## Normalizers\\n\\nA `Normalizer` is in charge of pre-processing the input string in order\\nto normalize it as relevant for a given use case. Some common examples\\nof normalization are the Unicode normalization algorithms (NFD, NFKD,\\nNFC & NFKC), lowercasing etc... The specificity of `tokenizers` is that\\nwe keep track of the alignment while normalizing. This is essential to\\nallow mapping from the generated tokens back to the input text.\\n\\nThe `Normalizer` is optional.\\n\\n<tokenizerslangcontent>\\n<python>\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| NFD | NFD unicode normalization |  |\\n| NFKD | NFKD unicode normalization |  |\\n| NFC | NFC unicode normalization |  |\\n| NFKC | NFKC unicode normalization |  |\\n| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO ὈΔΥΣΣΕΎΣ` <br> Output: `hello`ὀδυσσεύς`  |\\n| Strip | Removes all whitespace characters on the specified sides (left, right or both) of the input | Input: `\"`hi`\"` <br> Output: `\"hi\"`  |\\n| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input: `é` <br> Ouput: `e`  |\\n| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\"a\", \"e\")` will behave like this: <br> Input: `\"banana\"` <br> Ouput: `\"benene\"`  |\\n| BertNormalizer | Provides an implementation of the Normalizer used in the original BERT. Options that can be set are: <ul> <li>clean_text</li> <li>handle_chinese_chars</li> <li>strip_accents</li> <li>lowercase</li> </ul>  |  |\\n| Sequence | Composes multiple normalizers that will run in the provided order | `Sequence([NFKC(), Lowercase()])` |\\n</python>\\n<rust>\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| NFD | NFD unicode normalization |  |\\n| NFKD | NFKD unicode normalization |  |\\n| NFC | NFC unicode normalization |  |\\n| NFKC | NFKC unicode normalization |  |\\n| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO ὈΔΥΣΣΕΎΣ` <br> Output: `hello`ὀδυσσεύς`  |\\n| Strip | Removes all whitespace characters on the specified sides (left, right or both) of the input | Input: `\"`hi`\"` <br> Output: `\"hi\"`  |\\n| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input: `é` <br> Ouput: `e`  |\\n| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\"a\", \"e\")` will behave like this: <br> Input: `\"banana\"` <br> Ouput: `\"benene\"`  |\\n| BertNormalizer | Provides an implementation of the Normalizer used in the original BERT. Options that can be set are: <ul> <li>clean_text</li> <li>handle_chinese_chars</li> <li>strip_accents</li> <li>lowercase</li> </ul>  |  |\\n| Sequence | Composes multiple normalizers that will run in the provided order | `Sequence::new(vec![NFKC, Lowercase])` |\\n</rust>\\n<node>\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| NFD | NFD unicode normalization |  |\\n| NFKD | NFKD unicode normalization |  |\\n| NFC | NFC unicode normalization |  |\\n| NFKC | NFKC unicode normalization |  |\\n| Lowercase | Replaces all uppercase to lowercase | Input: `HELLO ὈΔΥΣΣΕΎΣ` <br> Output: `hello`ὀδυσσεύς`  |\\n| Strip | Removes all whitespace characters on the specified sides (left, right or both) of the input | Input: `\"`hi`\"` <br> Output: `\"hi\"`  |\\n| StripAccents | Removes all accent symbols in unicode (to be used with NFD for consistency) | Input: `é` <br> Ouput: `e`  |\\n| Replace | Replaces a custom string or regexp and changes it with given content | `Replace(\"a\", \"e\")` will behave like this: <br> Input: `\"banana\"` <br> Ouput: `\"benene\"`  |\\n| BertNormalizer | Provides an implementation of the Normalizer used in the original BERT. Options that can be set are: <ul> <li>cleanText</li> <li>handleChineseChars</li> <li>stripAccents</li> <li>lowercase</li> </ul>  |  |\\n| Sequence | Composes multiple normalizers that will run in the provided order | |\\n</node>\\n</tokenizerslangcontent>\\n\\n## Pre-tokenizers\\n\\nThe `PreTokenizer` takes care of splitting the input according to a set\\nof rules. This pre-processing lets you ensure that the underlying\\n`Model` does not build tokens across multiple \"splits\". For example if\\nyou don\\'t want to have whitespaces inside a token, then you can have a\\n`PreTokenizer` that splits on these whitespaces.\\n\\nYou can easily combine multiple `PreTokenizer` together using a\\n`Sequence` (see below). The `PreTokenizer` is also allowed to modify the\\nstring, just like a `Normalizer` does. This is necessary to allow some\\ncomplicated algorithms that require to split before normalizing (e.g.\\nthe ByteLevel)\\n\\n<tokenizerslangcontent>\\n<python>\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel | Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties: <ul> <li>Since it maps on bytes, a tokenizer using this only requires **256** characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.</li> <li>A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! 🎉🎉)</li> <li>For non ascii characters, it gets completely unreadable, but it works nonetheless!</li> </ul> | Input: `\"Hello my friend, how are you?\"` <br> Ouput: `\"Hello\", \"Ġmy\", Ġfriend\", \",\", \"Ġhow\", \"Ġare\", \"Ġyou\", \"?\"`  |\\n| Whitespace | Splits on word boundaries (using the following regular expression: `\\\\w+&#124;[^\\\\w\\\\s]+` | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there\", \"!\"`  |\\n| WhitespaceSplit | Splits on any whitespace character | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there!\"`  |\\n| Punctuation | Will isolate all punctuation characters | Input: `\"Hello?\"` <br> Ouput: `\"Hello\", \"?\"`  |\\n| Metaspace | Splits on whitespaces and replaces them with a special char “▁” (U+2581) | Input: `\"Hello there\"` <br> Ouput: `\"Hello\", \"▁there\"`  |\\n| CharDelimiterSplit | Splits on a given character | Example with `x`: <br> Input: `\"Helloxthere\"` <br> Ouput: `\"Hello\", \"there\"`  |\\n| Digits | Splits the numbers from any other characters. | Input: `\"Hello123there\"` <br>  Output: ``\"Hello\", \"123\", \"there\"``  |\\n| Split | Versatile pre-tokenizer that splits on provided pattern and according to provided behavior. The pattern can be inverted if necessary. <ul> <li>pattern should be either a custom string or regexp.</li> <li>behavior should be one of: <ul><li>removed</li><li>isolated</li><li>merged_with_previous</li><li>merged_with_next</li><li>contiguous</li></ul></li> <li>invert should be a boolean flag.</li> </ul> | Example with pattern = ` `, behavior = `\"isolated\"`, invert = `False`: <br> Input: `\"Hello, how are you?\"` <br> Output: `\"Hello,\", \" \", \"how\", \" \", \"are\", \" \", \"you?\"` |\\n| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the given order | `Sequence([Punctuation(), WhitespaceSplit()])` |\\n</python>\\n<rust>\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel | Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties: <ul> <li>Since it maps on bytes, a tokenizer using this only requires **256** characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.</li> <li>A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! 🎉🎉)</li> <li>For non ascii characters, it gets completely unreadable, but it works nonetheless!</li> </ul> | Input: `\"Hello my friend, how are you?\"` <br> Ouput: `\"Hello\", \"Ġmy\", Ġfriend\", \",\", \"Ġhow\", \"Ġare\", \"Ġyou\", \"?\"`  |\\n| Whitespace | Splits on word boundaries (using the following regular expression: `\\\\w+&#124;[^\\\\w\\\\s]+` | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there\", \"!\"`  |\\n| WhitespaceSplit | Splits on any whitespace character | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there!\"`  |\\n| Punctuation | Will isolate all punctuation characters | Input: `\"Hello?\"` <br> Ouput: `\"Hello\", \"?\"`  |\\n| Metaspace | Splits on whitespaces and replaces them with a special char “▁” (U+2581) | Input: `\"Hello there\"` <br> Ouput: `\"Hello\", \"▁there\"`  |\\n| CharDelimiterSplit | Splits on a given character | Example with `x`: <br> Input: `\"Helloxthere\"` <br> Ouput: `\"Hello\", \"there\"`  |\\n| Digits | Splits the numbers from any other characters. | Input: `\"Hello123there\"` <br>  Output: ``\"Hello\", \"123\", \"there\"``  |\\n| Split | Versatile pre-tokenizer that splits on provided pattern and according to provided behavior. The pattern can be inverted if necessary. <ul> <li>pattern should be either a custom string or regexp.</li> <li>behavior should be one of: <ul><li>Removed</li><li>Isolated</li><li>MergedWithPrevious</li><li>MergedWithNext</li><li>Contiguous</li></ul></li> <li>invert should be a boolean flag.</li> </ul> | Example with pattern = ` `, behavior = `\"isolated\"`, invert = `False`: <br> Input: `\"Hello, how are you?\"` <br> Output: `\"Hello,\", \" \", \"how\", \" \", \"are\", \" \", \"you?\"` |\\n| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the given order | `Sequence::new(vec![Punctuation, WhitespaceSplit])` |\\n</rust>\\n<node>\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| ByteLevel | Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties: <ul> <li>Since it maps on bytes, a tokenizer using this only requires **256** characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.</li> <li>A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! 🎉🎉)</li> <li>For non ascii characters, it gets completely unreadable, but it works nonetheless!</li> </ul> | Input: `\"Hello my friend, how are you?\"` <br> Ouput: `\"Hello\", \"Ġmy\", Ġfriend\", \",\", \"Ġhow\", \"Ġare\", \"Ġyou\", \"?\"`  |\\n| Whitespace | Splits on word boundaries (using the following regular expression: `\\\\w+&#124;[^\\\\w\\\\s]+` | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there\", \"!\"`  |\\n| WhitespaceSplit | Splits on any whitespace character | Input: `\"Hello there!\"` <br> Output: `\"Hello\", \"there!\"`  |\\n| Punctuation | Will isolate all punctuation characters | Input: `\"Hello?\"` <br> Ouput: `\"Hello\", \"?\"`  |\\n| Metaspace | Splits on whitespaces and replaces them with a special char “▁” (U+2581) | Input: `\"Hello there\"` <br> Ouput: `\"Hello\", \"▁there\"`  |\\n| CharDelimiterSplit | Splits on a given character | Example with `x`: <br> Input: `\"Helloxthere\"` <br> Ouput: `\"Hello\", \"there\"`  |\\n| Digits | Splits the numbers from any other characters. | Input: `\"Hello123there\"` <br>  Output: ``\"Hello\", \"123\", \"there\"``  |\\n| Split | Versatile pre-tokenizer that splits on provided pattern and according to provided behavior. The pattern can be inverted if necessary. <ul> <li>pattern should be either a custom string or regexp.</li> <li>behavior should be one of: <ul><li>removed</li><li>isolated</li><li>mergedWithPrevious</li><li>mergedWithNext</li><li>contiguous</li></ul></li> <li>invert should be a boolean flag.</li> </ul> | Example with pattern = ` `, behavior = `\"isolated\"`, invert = `False`: <br> Input: `\"Hello, how are you?\"` <br> Output: `\"Hello,\", \" \", \"how\", \" \", \"are\", \" \", \"you?\"` |\\n| Sequence | Lets you compose multiple `PreTokenizer` that will be run in the given order | |\\n</node>\\n</tokenizerslangcontent>\\n\\n## Models\\n\\nModels are the core algorithms used to actually tokenize, and therefore,\\nthey are the only mandatory component of a Tokenizer.\\n\\n| Name | Description |\\n| :--- | :--- |\\n| WordLevel | This is the “classic” tokenization algorithm. It let’s you simply map words to IDs without anything fancy. This has the advantage of being really simple to use and understand, but it requires extremely large vocabularies for a good coverage. Using this `Model` requires the use of a `PreTokenizer`. No choice will be made by this model directly, it simply maps input tokens to IDs.  |\\n| BPE | One of the most popular subword tokenization algorithm. The Byte-Pair-Encoding works by starting with characters, while merging those that are the most frequently seen together, thus creating new tokens. It then works iteratively to build new tokens out of the most frequent pairs it sees in a corpus. BPE is able to build words it has never seen by using multiple subword tokens, and thus requires smaller vocabularies, with less chances of having “unk” (unknown) tokens.  |\\n| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |\\n| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |\\n\\n## Post-Processors\\n\\nAfter the whole pipeline, we sometimes want to insert some special\\ntokens before feed a tokenized string into a model like \"[CLS] My\\nhorse is amazing [SEP]\". The `PostProcessor` is the component doing\\njust that.\\n\\n| Name | Description | Example |\\n| :--- | :--- | :--- |\\n| TemplateProcessing | Let’s you easily template the post processing, adding special tokens, and specifying the `type_id` for each sequence/special token. The template is given two strings representing the single sequence and the pair of sequences, as well as a set of special tokens to use. | Example, when specifying a template with these values:<br> <ul> <li> single: `\"[CLS] $A [SEP]\"` </li> <li> pair: `\"[CLS] $A [SEP] $B [SEP]\"` </li> <li> special tokens: <ul> <li>`\"[CLS]\"`</li> <li>`\"[SEP]\"`</li> </ul> </li> </ul> <br> Input: `(\"I like this\", \"but not this\")` <br> Output: `\"[CLS] I like this [SEP] but not this [SEP]\"` |\\n\\n## Decoders\\n\\nThe Decoder knows how to go from the IDs used by the Tokenizer, back to\\na readable piece of text. Some `Normalizer` and `PreTokenizer` use\\nspecial characters or identifiers that need to be reverted for example.\\n\\n| Name | Description |\\n| :--- | :--- |\\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\\n| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `▁` to identify whitespaces, and so this Decoder helps with decoding these. |\\n| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |\\n'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Overview\\n\\nWelcome to 🧨 Diffusers! If you\\'re new to diffusion models and generative AI, and want to learn more, then you\\'ve come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how 🧨 Diffusers is meant to be used.\\n\\nYou\\'ll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you\\'ll learn how to train your own diffusion model to generate what you want.\\n\\nAfter completing the tutorials, you\\'ll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.\\n\\nFeel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers!\\n\\nLet\\'s start diffusing! 🧨\\n'), Document(metadata={}, page_content=' 使用标记\\n\\n相关空间：https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\\n标签：标记，数据\\n\\n## 简介\\n\\n当您演示一个机器学习模型时，您可能希望收集试用模型的用户的数据，特别是模型行为不如预期的数据点。捕获这些“困难”数据点是有价值的，因为它允许您改进机器学习模型并使其更可靠和稳健。\\n\\nGradio 通过在每个“界面”中包含一个**标记**按钮来简化这些数据的收集。这使得用户或测试人员可以轻松地将数据发送回运行演示的机器。样本会保存在一个 CSV 日志文件中（默认情况下）。如果演示涉及图像、音频、视频或其他类型的文件，则这些文件会单独保存在一个并行目录中，并且这些文件的路径会保存在 CSV 文件中。\\n\\n## 在 `gradio.Interface` 中使用**标记**按钮\\n\\n使用 Gradio 的 `Interface` 进行标记特别简单。默认情况下，在输出组件下方有一个标记为**标记**的按钮。当用户测试您的模型时，如果看到有趣的输出，他们可以点击标记按钮将输入和输出数据发送回运行演示的机器。样本会保存在一个 CSV 日志文件中（默认情况下）。如果演示涉及图像、音频、视频或其他类型的文件，则这些文件会单独保存在一个并行目录中，并且这些文件的路径会保存在 CSV 文件中。\\n\\n在 `gradio.Interface` 中有[四个参数](https://gradio.app/docs/#interface-header)控制标记的工作方式。我们将详细介绍它们。\\n\\n- `allow_flagging`：此参数可以设置为 `\"manual\"`（默认值），`\"auto\"` 或 `\"never\"`。\\n  - `manual`：用户将看到一个标记按钮，只有在点击按钮时样本才会被标记。\\n  - `auto`：用户将不会看到一个标记按钮，但每个样本都会自动被标记。\\n  - `never`：用户将不会看到一个标记按钮，并且不会标记任何样本。\\n- `flagging_options`：此参数可以是 `None`（默认值）或字符串列表。\\n  - 如果是 `None`，则用户只需点击**标记**按钮，不会显示其他选项。\\n  - 如果提供了一个字符串列表，则用户会看到多个按钮，对应于提供的每个字符串。例如，如果此参数的值为`[\" 错误 \", \" 模糊 \"]`，则会显示标记为**标记为错误**和**标记为模糊**的按钮。这仅适用于 `allow_flagging` 为 `\"manual\"` 的情况。\\n  - 所选选项将与输入和输出一起记录。\\n- `flagging_dir`：此参数接受一个字符串。\\n  - 它表示标记数据存储的目录名称。\\n- `flagging_callback`：此参数接受 `FlaggingCallback` 类的子类的实例\\n  - 使用此参数允许您编写在点击标记按钮时运行的自定义代码\\n  - 默认情况下，它设置为 `gr.CSVLogger` 的一个实例\\n  - 一个示例是将其设置为 `gr.HuggingFaceDatasetSaver` 的一个实例，这样您可以将任何标记的数据导入到 HuggingFace 数据集中（参见下文）。\\n\\n## 标记的数据会发生什么？\\n\\n在 `flagging_dir` 参数提供的目录中，将记录标记的数据的 CSV 文件。\\n\\n以下是一个示例：下面的代码创建了嵌入其中的计算器界面：\\n\\n```python\\nimport gradio as gr\\n\\n\\ndef calculator(num1, operation, num2):\\n    if operation == \"add\":\\n        return num1 + num2\\n    elif operation == \"subtract\":\\n        return num1 - num2\\n    elif operation == \"multiply\":\\n        return num1 * num2\\n    elif operation == \"divide\":\\n        return num1 / num2\\n\\n\\niface = gr.Interface(\\n    calculator,\\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n    \"number\",\\n    allow_flagging=\"manual\"\\n)\\n\\niface.launch()\\n```\\n\\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\\n\\n当您点击上面的标记按钮时，启动界面的目录将包括一个新的标记子文件夹，其中包含一个 CSV 文件。该 CSV 文件包括所有被标记的数据。\\n\\n```directory\\n+-- flagged/\\n|   +-- logs.csv\\n```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nnum1,operation,num2,Output,timestamp\\n5,add,7,12,2022-01-31 11:40:51.093412\\n6,subtract,1.5,4.5,2022-01-31 03:25:32.023542\\n```\\n\\n如果界面涉及文件数据，例如图像和音频组件，还将创建文件夹来存储这些标记的数据。例如，将 `image` 输入到 `image` 输出界面将创建以下结构。\\n\\n```directory\\n+-- flagged/\\n|   +-- logs.csv\\n|   +-- image/\\n|   |   +-- 0.png\\n|   |   +-- 1.png\\n|   +-- Output/\\n|   |   +-- 0.png\\n|   |   +-- 1.png\\n```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nim,Output timestamp\\nim/0.png,Output/0.png,2022-02-04 19:49:58.026963\\nim/1.png,Output/1.png,2022-02-02 10:40:51.093412\\n```\\n\\n如果您希望用户为标记提供一个原因，您可以将字符串列表传递给 Interface 的 `flagging_options` 参数。用户在标记时必须选择其中一项，选项将作为附加列保存在 CSV 文件中。\\n\\n如果我们回到计算器示例，下面的代码将创建嵌入其中的界面。\\n\\n```python\\niface = gr.Interface(\\n    calculator,\\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n    \"number\",\\n    allow_flagging=\"manual\",\\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\\n)\\n\\niface.launch()\\n```\\n\\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\\n\\n当用户点击标记按钮时，CSV 文件现在将包括指示所选选项的列。\\n\\n_flagged/logs.csv_\\n\\n```csv\\nnum1,operation,num2,Output,flag,timestamp\\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\\n```\\n\\n## HuggingFaceDatasetSaver 回调\\n\\n有时，将数据保存到本地 CSV 文件是不合理的。例如，在 Hugging Face Spaces 上\\n，开发者通常无法访问托管 Gradio 演示的底层临时机器。这就是为什么，默认情况下，在 Hugging Face Space 中关闭标记的原因。然而，\\n您可能希望对标记的数据做其他处理。\\nyou may want to do something else with the flagged data.\\n\\n通过 `flagging_callback` 参数，我们使这变得非常简单。\\n\\n例如，下面我们将会将标记的数据从我们的计算器示例导入到 Hugging Face 数据集中，以便我们可以构建一个“众包”数据集：\\n\\n```python\\nimport os\\n\\nHF_TOKEN = os.getenv(\\'HF_TOKEN\\')\\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\\n\\niface = gr.Interface(\\n    calculator,\\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n    \"number\",\\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\\n    allow_flagging=\"manual\",\\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\\n    flagging_callback=hf_writer\\n)\\n\\niface.launch()\\n```\\n\\n注意，我们使用我们的 Hugging Face 令牌和\\n要保存样本的数据集的名称，定义了我们自己的\\n`gradio.HuggingFaceDatasetSaver` 的实例。此外，我们还将 `allow_flagging=\"manual\"` 设置为了\\n，因为在 Hugging Face Spaces 中，`allow_flagging` 默认设置为 `\"never\"`。这是我们的演示：\\n\\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\\n\\n您现在可以在这个[公共的 Hugging Face 数据集](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)中看到上面标记的所有示例。\\n\\n![flagging callback hf](/assets/guides/flagging-callback-hf.png)\\n\\n我们创建了 `gradio.HuggingFaceDatasetSaver` 类，但只要它继承自[此文件](https://github.com/gradio-app/gradio/blob/master/gradio/flagging.py)中定义的 `FlaggingCallback`，您可以传递自己的自定义类。如果您创建了一个很棒的回调，请将其贡献给该存储库！\\n\\n## 使用 Blocks 进行标记\\n\\n如果您正在使用 `gradio.Blocks`，又该怎么办呢？一方面，使用 Blocks 您拥有更多的灵活性\\n--您可以编写任何您想在按钮被点击时运行的 Python 代码，\\n并使用 Blocks 中的内置事件分配它。\\n\\n同时，您可能希望使用现有的 `FlaggingCallback` 来避免编写额外的代码。\\n这需要两个步骤：\\n\\n1. 您必须在代码中的某个位置运行您的回调的 `.setup()` 方法\\n   在第一次标记数据之前\\n2. 当点击标记按钮时，您触发回调的 `.flag()` 方法，\\n   确保正确收集参数并禁用通常的预处理。\\n\\n下面是一个使用默认的 `CSVLogger` 标记图像怀旧滤镜 Blocks 演示的示例：\\ndata using the default `CSVLogger`:\\n\\n$code_blocks_flag\\n$demo_blocks_flag\\n\\n## 隐私\\n\\n重要提示：请确保用户了解他们提交的数据何时被保存以及您计划如何处理它。当您使用 `allow_flagging=auto`（当通过演示提交的所有数据都被标记时），这一点尤为重要\\n\\n### 这就是全部！祝您建设愉快 :)\\n'), Document(metadata={}, page_content='ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main'), Document(metadata={}, page_content='!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# FNet\\n\\n## Overview\\n\\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\\npaper is the following:\\n\\n*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all\\nsequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint\\nand is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models\\noutperform Transformer counterparts.*\\n\\nThis model was contributed by [gchhablani](https://huggingface.co/gchhablani). The original code can be found [here](https://github.com/google-research/google-research/tree/master/f_net).\\n\\n## Usage tips\\n\\nThe model was trained without an attention mask as it is based on Fourier Transform. The model was trained with \\nmaximum sequence length 512 which includes pad tokens. Hence, it is highly recommended to use the same maximum \\nsequence length for fine-tuning and inference.\\n\\n## Resources\\n\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\\n- [Multiple choice task guide](../tasks/multiple_choice)\\n\\n## FNetConfig\\n\\n[[autodoc]] FNetConfig\\n\\n## FNetTokenizer\\n\\n[[autodoc]] FNetTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## FNetTokenizerFast\\n\\n[[autodoc]] FNetTokenizerFast\\n\\n## FNetModel\\n\\n[[autodoc]] FNetModel\\n    - forward\\n\\n## FNetForPreTraining\\n\\n[[autodoc]] FNetForPreTraining\\n    - forward\\n\\n## FNetForMaskedLM\\n\\n[[autodoc]] FNetForMaskedLM\\n    - forward\\n\\n## FNetForNextSentencePrediction\\n\\n[[autodoc]] FNetForNextSentencePrediction\\n    - forward\\n\\n## FNetForSequenceClassification\\n\\n[[autodoc]] FNetForSequenceClassification\\n    - forward\\n\\n## FNetForMultipleChoice\\n\\n[[autodoc]] FNetForMultipleChoice\\n    - forward\\n\\n## FNetForTokenClassification\\n\\n[[autodoc]] FNetForTokenClassification\\n    - forward\\n\\n## FNetForQuestionAnswering\\n\\n[[autodoc]] FNetForQuestionAnswering\\n    - forward\\n'), Document(metadata={}, page_content=\" Test Strategy\\n\\nVery brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to.\\n\\nThis document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\\n\\n## Objectives\\n\\nThe purposes of all testing activities on Gradio fit one of the following objectives:\\n\\n1. Ensure that the Gradio library functions as we expect it to.\\n2. Enable the maintenance team to quickly identify both the presence and source of defects.\\n3. Prevent regressions, i.e. if we fix something it should stay fixed.\\n4. Improve the quality of the codebase in order to ease maintenance efforts.\\n5. Reduce the amount of manual testing required.\\n\\n## Scope\\n\\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas.\\n\\nWe should not focus on code coverage but on test coverage following the below criteria:\\n\\n- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\\n\\n## Types of testing\\n\\nOur tests will broadly fall into one of three categories:\\n\\n- Static Quality checks\\n- Dynamic 'Code' tests\\n- Dynamic Functional tests\\n\\n### Static Quality checks\\n\\nStatic quality checks are generally very fast to run and do not require building the code base. They also provide the least value. These tests would be things like linting, typechecking, and formatting.\\n\\nWhile they offer little in terms of testing functionality they align very closely with objective (4, 5) as they generally help to keep the codebase in good shape and offer very fast feedback. Such check are almost free from an authoring point of view as fixes can be mostly automated (either via scripts or editor integrations).\\n\\n### Dynamic code tests\\n\\nThese tests generally test either isolated pieces of code or test the relationship between parts of the code base. They sometimes test functionality or give indications of working functionality but never offer enough confidence to rely on them solely.\\n\\nThese test are usually either unit or integration tests. They are generally pretty quick to write (especially unit tests) and run and offer a moderate amount of confidence. They align closely with Objectives 2 and 3 and a little bit of 1.\\n\\nThese kind of tests should probably make up the bulk of our handwritten tests.\\n\\n### Dynamic functional tests\\n\\nThese tests give by far the most confidence as they are testing only the functionality of the software and do so by running the entire software itself, exactly as a user would.\\n\\nThis aligns very closely with objective 1 but significantly impacts objective 5, as these tests are costly to both write and run. Despite the value, due to the downside we should try to get as much out of other tests types as we can, reserving functional testing for complex use cases and end-to-end journey.\\n\\nTests in this category could be browser-based end-to-end tests, accessibility tests, or performance tests. They are sometimes called acceptance tests.\\n\\n## Testing tools\\n\\nWe currently use the following tools:\\n\\n### Static quality checks\\n\\n- Python type-checking (python)\\n- Black linting (python)\\n- ruff formatting (python)\\n- prettier formatting (javascript/svelte)\\n- TypeScript type-checking (javascript/svelte)\\n- eslint linting (javascript/svelte) [in progress]\\n\\n### Dynamic code tests\\n\\n- pytest (python unit and integration tests)\\n- vitest (node-based unit and integration tests)\\n- playwright (browser-based unit and integration tests)\\n\\n### Functional/acceptance tests\\n\\n- playwright (full end to end testing)\\n- chromatic (visual testing) [in progress]\\n- Accessibility testing [to do]\\n\\n## Supported environments and versions\\n\\nAll operating systems refer to the current runner variants supported by GitHub actions.\\n\\nAll unspecified version segments (`x`) refer to latest.\\n\\n| Software | Version(s)            | Operating System(s)               |\\n| -------- | --------------------- | --------------------------------- |\\n| Python   | `3.8.x`               | `ubuntu-latest`, `windows-latest` |\\n| Node     | `18.x.x`              | `ubuntu-latest`                   |\\n| Browser  | `playwright-chrome-x` | `ubuntu-latest`                   |\\n\\n## Test execution\\n\\nTests need to be executed in a number of environments and at different stages of the development cycle in order to be useful. The requirements for tests are as follows:\\n\\n- **Locally**: it is important that developers can easily run most tests locally to ensure a passing suite before making a PR. There are some exceptions to this, certain tests may require access to secret values which we cannot make available to all possible contributors for practical security reasons. It is reasonable that it isn't possible to run these tests but they should be disabled by default when running locally.\\n- **CI** - It is _critical_ that all tests run successfully in CI with no exceptions. Not every test is required to pass to satisfy CI checks for practical reasons but it is required that all tests should run in CI and notify us if something unexpected happens in order for the development team to take appropriate action.\\n\\nFor instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\\n\\n## Managing defects\\n\\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\\n\\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.\\n\"), Document(metadata={}, page_content='--\\ntitle: \"Introducing 🤗 Accelerate\"\\nthumbnail: /blog/assets/20_accelerate_library/accelerate_diff.png\\nauthors:\\n- user: sgugger\\n---\\n\\n# Introducing 🤗 Accelerate\\n\\n\\n## 🤗 Accelerate\\n\\nRun your **raw** PyTorch training scripts on any kind of device.\\n\\nMost high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. 🤗 Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism.\\n\\nIt provides two things: a simple and consistent API that abstracts that boilerplate code and a launcher command to easily run those scripts on various setups.\\n\\n### Easy integration!\\n\\nLet\\'s first have a look at an example:\\n\\n```diff\\n  import torch\\n  import torch.nn.functional as F\\n  from datasets import load_dataset\\n+ from accelerate import Accelerator\\n\\n+ accelerator = Accelerator()\\n- device = \\'cpu\\'\\n+ device = accelerator.device\\n\\n  model = torch.nn.Transformer().to(device)\\n  optim = torch.optim.Adam(model.parameters())\\n\\n  dataset = load_dataset(\\'my_dataset\\')\\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\\n\\n+ model, optim, data = accelerator.prepare(model, optim, data)\\n\\n  model.train()\\n  for epoch in range(10):\\n      for source, targets in data:\\n          source = source.to(device)\\n          targets = targets.to(device)\\n\\n          optimizer.zero_grad()\\n\\n          output = model(source)\\n          loss = F.cross_entropy(output, targets)\\n\\n-         loss.backward()\\n+         accelerator.backward(loss)\\n\\n          optimizer.step()\\n```\\n\\nBy just adding five lines of code to any standard PyTorch training script, you can now run said script on any kind of distributed setting, as well as with or without mixed precision. 🤗 Accelerate even handles the device placement for you, so you can simplify the training loop above even further:\\n\\n```diff\\n  import torch\\n  import torch.nn.functional as F\\n  from datasets import load_dataset\\n+ from accelerate import Accelerator\\n\\n+ accelerator = Accelerator()\\n- device = \\'cpu\\'\\n\\n- model = torch.nn.Transformer().to(device)\\n+ model = torch.nn.Transformer()\\n  optim = torch.optim.Adam(model.parameters())\\n\\n  dataset = load_dataset(\\'my_dataset\\')\\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\\n\\n+ model, optim, data = accelerator.prepare(model, optim, data)\\n\\n  model.train()\\n  for epoch in range(10):\\n      for source, targets in data:\\n-         source = source.to(device)\\n-         targets = targets.to(device)\\n\\n          optimizer.zero_grad()\\n\\n          output = model(source)\\n          loss = F.cross_entropy(output, targets)\\n\\n-         loss.backward()\\n+         accelerator.backward(loss)\\n\\n          optimizer.step()\\n```\\n\\nIn contrast, here are the changes needed to have this code run with distributed training are the followings:\\n\\n```diff\\n+ import os\\n  import torch\\n  import torch.nn.functional as F\\n  from datasets import load_dataset\\n+ from torch.utils.data import DistributedSampler\\n+ from torch.nn.parallel import DistributedDataParallel\\n\\n+ local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\\n- device = \\'cpu\\'\\n+ device = device = torch.device(\"cuda\", local_rank)\\n\\n  model = torch.nn.Transformer().to(device)\\n+ model = DistributedDataParallel(model)  \\n  optim = torch.optim.Adam(model.parameters())\\n\\n  dataset = load_dataset(\\'my_dataset\\')\\n+ sampler = DistributedSampler(dataset)\\n- data = torch.utils.data.DataLoader(dataset, shuffle=True)\\n+ data = torch.utils.data.DataLoader(dataset, sampler=sampler)\\n\\n  model.train()\\n  for epoch in range(10):\\n+     sampler.set_epoch(epoch)  \\n      for source, targets in data:\\n          source = source.to(device)\\n          targets = targets.to(device)\\n\\n          optimizer.zero_grad()\\n\\n          output = model(source)\\n          loss = F.cross_entropy(output, targets)\\n\\n          loss.backward()\\n\\n          optimizer.step()\\n```\\n\\nThese changes will make your training script work for multiple GPUs, but your script will then stop working on CPU or one GPU (unless you start adding if statements everywhere). Even more annoying, if you wanted to test your script on TPUs you would need to change different lines of codes. Same for mixed precision training. The promise of 🤗 Accelerate is:\\n- to keep the changes to your training loop to the bare minimum so you have to learn as little as possible.\\n- to have the same functions work for any distributed setup, so only have to learn one API.\\n\\n### How does it work?\\n\\nTo see how the library works in practice, let\\'s have a look at each line of code we need to add to a training loop.\\n\\n```python\\naccelerator = Accelerator()\\n```\\n\\nOn top of giving the main object that you will use, this line will analyze from the environment the type of distributed training run and perform the necessary initialization. You can force a training on CPU or a mixed precision training by passing `cpu=True` or `fp16=True` to this init. Both of those options can also be set using the launcher for your script.\\n\\n```python\\nmodel, optim, data = accelerator.prepare(model, optim, data)\\n```\\n\\nThis is the main bulk of the API and will prepare the three main type of objects: models (`torch.nn.Module`), optimizers (`torch.optim.Optimizer`) and dataloaders (`torch.data.dataloader.DataLoader`).\\n\\n#### Model\\n\\nModel preparation include wrapping it in the proper container (for instance `DistributedDataParallel`) and putting it on the proper device. Like with a regular distributed training, you will need to unwrap your model for saving, or to access its specific methods, which can be done with `accelerator.unwrap_model(model)`.\\n\\n#### Optimizer\\n\\nThe optimizer is also wrapped in a special container that will perform the necessary operations in the step to make mixed precision work. It will also properly handle device placement of the state dict if its non-empty or loaded from a checkpoint.\\n\\n#### DataLoader\\n\\nThis is where most of the magic is hidden. As you have seen in the code example, the library does not rely on a `DistributedSampler`, it will actually work with any sampler you might pass to your dataloader (if you ever had to write a distributed version of your custom sampler, there is no more need for that!). The dataloader is wrapped in a container that will only grab the indices relevant to the current process in the sampler (or skip the batches for the other processes if you use an `IterableDataset`) and put the batches on the proper device.\\n\\nFor this to work, Accelerate provides a utility function that will synchronize the random number generators on each of the processes run during distributed training. By default, it only synchronizes the `generator` of your sampler, so your data augmentation will be different on each process, but the random shuffling will be the same. You can of course use this utility to synchronize more RNGs if you need it.\\n\\n```python\\naccelerator.backward(loss)\\n```\\n\\nThis last line adds the necessary steps for the backward pass (mostly for mixed precision but other integrations will require some custom behavior here).\\n\\n### What about evaluation?\\n\\nEvaluation can either be run normally on all processes, or if you just want it to run on the main process, you can use the handy test:\\n\\n```python\\nif accelerator.is_main_process():\\n    # Evaluation loop\\n```\\n\\nBut you can also very easily run a distributed evaluation using Accelerate, here is what you would need to add to your evaluation loop:\\n\\n```diff\\n+ eval_dataloader = accelerator.prepare(eval_dataloader)\\n  predictions, labels = [], []\\n  for source, targets in eval_dataloader:\\n      with torch.no_grad():\\n          output = model(source)\\n\\n-     predictions.append(output.cpu().numpy())\\n-     labels.append(targets.cpu().numpy())\\n+     predictions.append(accelerator.gather(output).cpu().numpy())\\n+     labels.append(accelerator.gather(targets).cpu().numpy())\\n\\n  predictions = np.concatenate(predictions)\\n  labels = np.concatenate(labels)\\n\\n+ predictions = predictions[:len(eval_dataloader.dataset)]\\n+ labels = label[:len(eval_dataloader.dataset)]\\n\\n  metric_compute(predictions, labels)\\n```\\n\\nLike for the training, you need to add one line to prepare your evaluation dataloader. Then you can just use `accelerator.gather` to gather across processes the tensors of predictions and labels. The last line to add truncates the predictions and labels to the number of examples in your dataset because the prepared evaluation dataloader will return a few more elements to make sure batches all have the same size on each process.\\n\\n### One launcher to rule them all\\n\\nThe scripts using Accelerate will be completely compatible with your traditional launchers, such as `torch.distributed.launch`. But remembering all the arguments to them is a bit annoying and when you\\'ve setup your instance with 4 GPUs, you\\'ll run most of your trainings using them all. Accelerate comes with a handy CLI that works in two steps:\\n\\n```bash\\naccelerate config\\n```\\n\\nThis will trigger a little questionnaire about your setup, which will create a config file you can edit with all the defaults for your training commands. Then\\n\\n```bash\\naccelerate launch path_to_script.py --args_to_the_script\\n```\\n\\nwill launch your training script using those default. The only thing you have to do is provide all the arguments needed by your training script.\\n\\nTo make this launcher even more awesome, you can use it to spawn an AWS instance using SageMaker. Look at [this guide](https://huggingface.co/docs/accelerate/sagemaker.html) to discover how!\\n\\n### How to get involved?\\n\\nTo get started, just `pip install accelerate` or see the [documentation](https://huggingface.co/docs/accelerate/installation.html) for more install options.\\n\\nAccelerate is a fully open-sourced project, you can find it on [GitHub](https://github.com/huggingface/accelerate), have a look at its [documentation](https://huggingface.co/docs/accelerate/) or skim through our [basic examples](https://github.com/huggingface/accelerate/tree/main/examples). Please let us know if you have any issue or feature you would like the library to support. For all questions, the [forums](https://discuss.huggingface.co/c/accelerate) is the place to check!\\n\\nFor more complex examples in situation, you can look at the official [Transformers examples](https://github.com/huggingface/transformers/tree/master/examples). Each folder contains a `run_task_no_trainer.py` that leverages the Accelerate library!\\n'), Document(metadata={}, page_content='FrameworkSwitchCourse {fw} />\\n\\n# Tokenizers[[tokenizers]]\\n\\n{#if fw === \\'pt\\'}\\n\\n<CourseFloatingBanner chapter={2}\\n  classNames=\"absolute z-10 right-0 top-0\"\\n  notebooks={[\\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb\"},\\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_pt.ipynb\"},\\n]} />\\n\\n{:else}\\n\\n<CourseFloatingBanner chapter={2}\\n  classNames=\"absolute z-10 right-0 top-0\"\\n  notebooks={[\\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb\"},\\n]} />\\n\\n{/if}\\n\\n<Youtube id=\"VFp38yj8h3A\"/>\\n\\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we\\'ll explore exactly what happens in the tokenization pipeline. \\n\\nIn NLP tasks, the data that is generally processed is raw text. Here\\'s an example of such text:\\n\\n```\\nJim Henson was a puppeteer\\n```\\n\\nHowever, models can only process numbers, so we need to find a way to convert the raw text to numbers. That\\'s what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation — that is, the one that makes the most sense to the model — and, if possible, the smallest representation.\\n\\nLet\\'s take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.\\n\\n## Word-based[[word-based]]\\n\\n<Youtube id=\"nhJxYji1aho\"/>\\n\\nThe first type of tokenizer that comes to mind is _word-based_. It\\'s generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\\n\\n<div class=\"flex justify-center\">\\n  <img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg\" alt=\"An example of word-based tokenization.\"/>\\n  <img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg\" alt=\"An example of word-based tokenization.\"/>\\n</div>\\n\\nThere are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python\\'s `split()` function:\\n\\n```py\\ntokenized_text = \"Jim Henson was a puppeteer\".split()\\nprint(tokenized_text)\\n```\\n\\n```python out\\n[\\'Jim\\', \\'Henson\\', \\'was\\', \\'a\\', \\'puppeteer\\']\\n```\\n\\nThere are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large \"vocabularies,\" where a vocabulary is defined by the total number of independent tokens that we have in our corpus.\\n\\nEach word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.\\n\\nIf we want to completely cover a language with a word-based tokenizer, we\\'ll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we\\'d need to keep track of that many IDs. Furthermore, words like \"dog\" are represented differently from words like \"dogs\", and the model will initially have no way of knowing that \"dog\" and \"dogs\" are similar: it will identify the two words as unrelated. The same applies to other similar words, like \"run\" and \"running\", which the model will not see as being similar initially.\\n\\nFinally, we need a custom token to represent words that are not in our vocabulary. This is known as the \"unknown\" token, often represented as \"[UNK]\" or \"&lt;unk&gt;\". It\\'s generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn\\'t able to retrieve a sensible representation of a word and you\\'re losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token.\\n\\nOne way to reduce the amount of unknown tokens is to go one level deeper, using a _character-based_ tokenizer.\\n\\n## Character-based[[character-based]]\\n\\n<Youtube id=\"ssLq_EK2jLE\"/>\\n\\nCharacter-based tokenizers split the text into characters, rather than words. This has two primary benefits:\\n\\n- The vocabulary is much smaller.\\n- There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\\n\\nBut here too some questions arise concerning spaces and punctuation:\\n\\n<div class=\"flex justify-center\">\\n  <img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg\" alt=\"An example of character-based tokenization.\"/>\\n  <img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg\" alt=\"An example of character-based tokenization.\"/>\\n</div>\\n\\nThis approach isn\\'t perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it\\'s less meaningful: each character doesn\\'t mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language.\\n\\nAnother thing to consider is that we\\'ll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters.\\n\\nTo get the best of both worlds, we can use a third technique that combines the two approaches: *subword tokenization*.\\n\\n## Subword tokenization[[subword-tokenization]]\\n\\n<Youtube id=\"zHvTiHr506c\"/>\\n\\nSubword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\\n\\nFor instance, \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\". These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\".\\n\\nHere is an example showing how a subword tokenization algorithm would tokenize the sequence \"Let\\'s do tokenization!\":\\n\\n<div class=\"flex justify-center\">\\n  <img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg\" alt=\"A subword tokenization algorithm.\"/>\\n  <img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg\" alt=\"A subword tokenization algorithm.\"/>\\n</div>\\n\\nThese subwords end up providing a lot of semantic meaning: for instance, in the example above \"tokenization\" was split into \"token\" and \"ization\", two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.\\n\\nThis approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\\n\\n### And more![[and-more]]\\n\\nUnsurprisingly, there are many more techniques out there. To name a few:\\n\\n- Byte-level BPE, as used in GPT-2\\n- WordPiece, as used in BERT\\n- SentencePiece or Unigram, as used in several multilingual models\\n\\nYou should now have sufficient knowledge of how tokenizers work to get started with the API.\\n\\n## Loading and saving[[loading-and-saving]]\\n\\nLoading and saving tokenizers is as simple as it is with models. Actually, it\\'s based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the *architecture* of the model) as well as its vocabulary (a bit like the *weights* of the model).\\n\\nLoading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the `BertTokenizer` class:\\n\\n```py\\nfrom transformers import BertTokenizer\\n\\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\\n```\\n\\n{#if fw === \\'pt\\'}\\nSimilar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:\\n\\n{:else}\\nSimilar to `TFAutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:\\n\\n{/if}\\n\\n```py\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\\n```\\n\\nWe can now use the tokenizer as shown in the previous section:\\n\\n```python\\ntokenizer(\"Using a Transformer network is simple\")\\n```\\n\\n```python out\\n{\\'input_ids\\': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],\\n \\'token_type_ids\\': [0, 0, 0, 0, 0, 0, 0, 0, 0],\\n \\'attention_mask\\': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\\n```\\n\\nSaving a tokenizer is identical to saving a model:\\n\\n```py\\ntokenizer.save_pretrained(\"directory_on_my_computer\")\\n```\\n\\nWe\\'ll talk more about `token_type_ids` in [Chapter 3](/course/chapter3), and we\\'ll explain the `attention_mask` key a little later. First, let\\'s see how the `input_ids` are generated. To do this, we\\'ll need to look at the intermediate methods of the tokenizer.\\n\\n## Encoding[[encoding]]\\n\\n<Youtube id=\"Yffk5aydLzg\"/>\\n\\nTranslating text to numbers is known as _encoding_. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.\\n\\nAs we\\'ve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called *tokens*. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\\n\\nThe second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a *vocabulary*, which is the part we download when we instantiate it with the `from_pretrained()` method. Again, we need to use the same vocabulary used when the model was pretrained.\\n\\nTo get a better understanding of the two steps, we\\'ll explore them separately. Note that we will use some methods that perform parts of the tokenization pipeline separately to show you the intermediate results of those steps, but in practice, you should call the tokenizer directly on your inputs (as shown in the section 2).\\n\\n### Tokenization[[tokenization]]\\n\\nThe tokenization process is done by the `tokenize()` method of the tokenizer:\\n\\n```py\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\\n\\nsequence = \"Using a Transformer network is simple\"\\ntokens = tokenizer.tokenize(sequence)\\n\\nprint(tokens)\\n```\\n\\nThe output of this method is a list of strings, or tokens:\\n\\n```python out\\n[\\'Using\\', \\'a\\', \\'transform\\', \\'##er\\', \\'network\\', \\'is\\', \\'simple\\']\\n```\\n\\nThis tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. That\\'s the case here with `transformer`, which is split into two tokens: `transform` and `##er`.\\n\\n### From tokens to input IDs[[from-tokens-to-input-ids]]\\n\\nThe conversion to input IDs is handled by the `convert_tokens_to_ids()` tokenizer method:\\n\\n```py\\nids = tokenizer.convert_tokens_to_ids(tokens)\\n\\nprint(ids)\\n```\\n\\n```python out\\n[7993, 170, 11303, 1200, 2443, 1110, 3014]\\n```\\n\\nThese outputs, once converted to the appropriate framework tensor, can then be used as inputs to a model as seen earlier in this chapter.\\n\\n<Tip>\\n\\n✏️ **Try it out!** Replicate the two last steps (tokenization and conversion to input IDs) on the input sentences we used in section 2 (\"I\\'ve been waiting for a HuggingFace course my whole life.\" and \"I hate this so much!\"). Check that you get the same input IDs we got earlier!\\n\\n</Tip>\\n\\n## Decoding[[decoding]]\\n\\n*Decoding* is going the other way around: from vocabulary indices, we want to get a string. This can be done with the `decode()` method as follows:\\n\\n```py\\ndecoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\\nprint(decoded_string)\\n```\\n\\n```python out\\n\\'Using a Transformer network is simple\\'\\n```\\n\\nNote that the `decode` method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization).\\n\\nBy now you should understand the atomic operations a tokenizer can handle: tokenization, conversion to IDs, and converting IDs back to a string. However, we\\'ve just scraped the tip of the iceberg. In the following section, we\\'ll take our approach to its limits and take a look at how to overcome them.\\n'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# 🧨 Diffusers’ Ethical Guidelines\\n\\n## Preamble\\n\\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\\n\\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, users’ contributions, and usage of the Diffusers library.\\n\\nThe risks associated with using this technology are still being examined, but to name a few: copyrights issues for artists; deep-fake exploitation; sexual content generation in inappropriate contexts; non-consensual impersonation; harmful social biases perpetuating the oppression of marginalized groups.\\nWe will keep tracking risks and adapt the following guidelines based on the community\\'s responsiveness and valuable feedback.\\n\\n\\n## Scope\\n\\nThe Diffusers community will apply the following ethical guidelines to the project’s development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.\\n\\n\\n## Ethical guidelines\\n\\nThe following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.\\n\\n- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.\\n\\n- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.\\n\\n- **Simplicity**: with a desire to make it easy to use and exploit the Diffusers library, we are committed to keeping the project’s goals lean and coherent.\\n\\n- **Accessibility**: the Diffusers project helps lower the entry bar for contributors who can help run it even without technical expertise. Doing so makes research artifacts more accessible to the community.\\n\\n- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\\n\\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology\\'s potential risks and dangers.\\n\\n\\n## Examples of implementations: Safety features and Mechanisms\\n\\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community\\'s input is invaluable in ensuring these features\\' implementation and raising awareness with us.\\n\\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.\\n\\n- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.\\n\\n- **Encouraging safety in deployment**\\n\\n  - [**Safe Stable Diffusion**](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_safe): It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).\\n\\n  - [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\\n\\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository’s authors to have more control over its use.\\n\\n- **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.\\n'), Document(metadata={}, page_content='!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Interact with Discussions and Pull Requests \\n\\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\\n\\n## Retrieve Discussions and Pull Requests from the Hub\\n\\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\\n\\n```python\\n>>> from huggingface_hub import get_repo_discussions\\n>>> for discussion in get_repo_discussions(repo_id=\"bigscience/bloom\"):\\n...     print(f\"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}\")\\n\\n# 11 - Add Flax weights, pr: True\\n# 10 - Update README.md, pr: True\\n# 9 - Training languages in the model card, pr: True\\n# 8 - Update tokenizer_config.json, pr: True\\n# 7 - Slurm training script, pr: False\\n[...]\\n```\\n\\n`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\\n\\n```python\\n>>> from huggingface_hub import get_repo_discussions\\n>>> for discussion in get_repo_discussions(\\n...    repo_id=\"bigscience/bloom\",\\n...    author=\"ArthurZ\",\\n...    discussion_type=\"pull_request\",\\n...    discussion_status=\"open\",\\n... ):\\n...     print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}\")\\n\\n# 19 - Add Flax weights by ArthurZ, pr: True\\n```\\n\\n`HfApi.get_repo_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\\n[`Discussion`] objects. To get all the Discussions in a single list, run:\\n\\n```python\\n>>> from huggingface_hub import get_repo_discussions\\n>>> discussions_list = list(get_repo_discussions(repo_id=\"bert-base-uncased\"))\\n```\\n\\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overview of the\\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get_discussion_details`]:\\n\\n```python\\n>>> from huggingface_hub import get_discussion_details\\n\\n>>> get_discussion_details(\\n...     repo_id=\"bigscience/bloom-1b3\",\\n...     discussion_num=2\\n... )\\nDiscussionWithDetails(\\n    num=2,\\n    author=\\'cakiki\\',\\n    title=\\'Update VRAM memory for the V100s\\',\\n    status=\\'open\\',\\n    is_pull_request=True,\\n    events=[\\n        DiscussionComment(type=\\'comment\\', author=\\'cakiki\\', ...),\\n        DiscussionCommit(type=\\'commit\\', author=\\'cakiki\\', summary=\\'Update VRAM memory for the V100s\\', oid=\\'1256f9d9a33fa8887e1c1bf0e09b4713da96773a\\', ...),\\n    ],\\n    conflicting_files=[],\\n    target_branch=\\'refs/heads/main\\',\\n    merge_commit_oid=None,\\n    diff=\\'diff --git a/README.md b/README.md\\\\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\\\\n--- a/README.md\\\\n+++ b/README.md\\\\n@@ -132,7 +132,7 [...]\\',\\n)\\n```\\n\\n[`HfApi.get_discussion_details`] returns a [`DiscussionWithDetails`] object, which is a subclass of [`Discussion`]\\nwith more detailed information about the Discussion or Pull Request. Information includes all the comments, status changes,\\nand renames of the Discussion via [`DiscussionWithDetails.events`].\\n\\nIn case of a Pull Request, you can retrieve the raw git diff with [`DiscussionWithDetails.diff`]. All the commits of the\\nPull Request are listed in [`DiscussionWithDetails.events`].\\n\\n\\n## Create and edit a Discussion or Pull Request programmatically\\n\\nThe [`HfApi`] class also offers ways to create and edit Discussions and Pull Requests.\\nYou will need an [access token](https://huggingface.co/docs/hub/security-tokens) to create and edit Discussions\\nor Pull Requests.\\n\\nThe simplest way to propose changes on a repo on the Hub is via the [`create_commit`] API: just \\nset the `create_pr` parameter to `True`. This parameter is also available on other methods that wrap [`create_commit`]:\\n\\n    * [`upload_file`]\\n    * [`upload_folder`]\\n    * [`delete_file`]\\n    * [`delete_folder`]\\n    * [`metadata_update`]\\n\\n```python\\n>>> from huggingface_hub import metadata_update\\n\\n>>> metadata_update(\\n...     repo_id=\"username/repo_name\",\\n...     metadata={\"tags\": [\"computer-vision\", \"awesome-model\"]},\\n...     create_pr=True,\\n... )\\n```\\n\\nYou can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to create a Discussion (respectively a Pull Request) on a repo.\\nOpening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\"draft\"` mode.\\n\\n```python\\n>>> from huggingface_hub import create_discussion, create_pull_request\\n\\n>>> create_discussion(\\n...     repo_id=\"username/repo-name\",\\n...     title=\"Hi from the huggingface_hub library!\",\\n...     token=\"<insert your access token here>\",\\n... )\\nDiscussionWithDetails(...)\\n\\n>>> create_pull_request(\\n...     repo_id=\"username/repo-name\",\\n...     title=\"Hi from the huggingface_hub library!\",\\n...     token=\"<insert your access token here>\",\\n... )\\nDiscussionWithDetails(..., is_pull_request=True)\\n```\\n\\nManaging Pull Requests and Discussions can be done entirely with the [`HfApi`] class. For example:\\n\\n    * [`comment_discussion`] to add comments\\n    * [`edit_discussion_comment`] to edit comments\\n    * [`rename_discussion`] to rename a Discussion or Pull Request \\n    * [`change_discussion_status`] to open or close a Discussion / Pull Request \\n    * [`merge_pull_request`] to merge a Pull Request \\n\\n\\nVisit the [`HfApi`] documentation page for an exhaustive reference of all available methods.\\n\\n## Push changes to a Pull Request\\n\\n*Coming soon !*\\n\\n## See also\\n\\nFor a more detailed reference, visit the [Discussions and Pull Requests](../package_reference/community) and the [hf_api](../package_reference/hf_api) documentation page.\\n'), Document(metadata={}, page_content='--\\ntitle: \"Introducing Skops\"\\nthumbnail: /blog/assets/94_skops/introducing_skops.png\\nauthors:\\n- user: merve\\n- user: adrin\\n- user: BenjaminB\\n---\\n\\n# Introducing Skops\\n\\n\\n##\\xa0Introducing Skops\\n\\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.\\n\\nLet\\'s go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production.\\n\\n```python\\n# let\\'s import the libraries first\\nimport sklearn\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\n# Load the data and split\\nX, y = load_breast_cancer(as_frame=True, return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.3, random_state=42\\n)\\n\\n# Train the model\\nmodel = DecisionTreeClassifier().fit(X_train, y_train)\\n```\\n\\nYou can use any model filename and serialization method, like `pickle` or `joblib`. At the moment, our backend uses `joblib` to load the model. `hub_utils.init` creates a local folder containing the model in the given path, and the configuration file containing the specifications of the environment the model is trained in. The data and the task passed to the `init` will help Hugging Face Hub enable the inference widget on the model page as well as discoverability features to find the model.\\n\\n```python\\nfrom skops import hub_utils\\nimport pickle\\n\\n# let\\'s save the model\\nmodel_path = \"example.pkl\"\\nlocal_repo = \"my-awesome-model\"\\nwith open(model_path, mode=\"bw\") as f:\\n    pickle.dump(model, file=f)\\n\\n# we will now initialize a local repository\\nhub_utils.init(\\n    model=model_path, \\n    requirements=[f\"scikit-learn={sklearn.__version__}\"], \\n    dst=local_repo,\\n    task=\"tabular-classification\",\\n    data=X_test,\\n)\\n```\\n\\nThe repository now contains the serialized model and the configuration file. \\nThe configuration contains the following:\\n- features of the model,\\n- the requirements of the model,\\n- an example input taken from `X_test` that we\\'ve passed,\\n- name of the model file,\\n- name of the task to be solved here.\\n\\nWe will now create the model card. The card should match the expected Hugging Face Hub format: a markdown part and a metadata section, which is a `yaml` section at the top. The keys to the metadata section are defined [here](https://huggingface.co/docs/hub/models-cards#model-card-metadata) and are used for the discoverability of the models. \\nThe content of the model card is determined by a template that has a:\\n- `yaml` section on top for metadata (e.g. model license, library name, and more)\\n- markdown section with free text and sections to be filled (e.g. simple description of the model),\\nThe following sections are extracted by `skops` to fill in the model card:\\n- Hyperparameters of the model,\\n- Interactive diagram of the model,\\n- For metadata, library name, task identifier (e.g. tabular-classification), and information required by the inference widget are filled.\\n\\nWe will walk you through how to programmatically pass information to fill the model card. You can check out our documentation on the default template provided by `skops`, and its sections [here](https://skops.readthedocs.io/en/latest/model_card.html) to see what the template expects and what it looks like [here](https://github.com/skops-dev/skops/blob/main/skops/card/default_template.md).\\n\\nYou can create the model card by instantiating the `Card` class from `skops`. During model serialization, the task name and library name are written to the configuration file. This information is also needed in the card\\'s metadata, so you can use the `metadata_from_config` method to extract the metadata from the configuration file and pass it to the card when you create it. You can add information and metadata using `add`.\\n\\n```python\\nfrom skops import card\\n\\n# create the card \\nmodel_card = card.Card(model, metadata=card.metadata_from_config(Path(destination_folder)))\\n\\nlimitations = \"This model is not ready to be used in production.\"\\nmodel_description = \"This is a DecisionTreeClassifier model trained on breast cancer dataset.\"\\nmodel_card_authors = \"skops_user\"\\nget_started_code = \"import pickle \\\\nwith open(dtc_pkl_filename, \\'rb\\') as file: \\\\n    clf = pickle.load(file)\"\\ncitation_bibtex = \"bibtex\\\\n@inproceedings{...,year={2020}}\"\\n\\n# we can add the information using add\\nmodel_card.add(\\n    citation_bibtex=citation_bibtex,\\n    get_started_code=get_started_code,\\n    model_card_authors=model_card_authors,\\n    limitations=limitations,\\n    model_description=model_description,\\n)\\n\\n# we can set the metadata part directly\\nmodel_card.metadata.license = \"mit\"\\n```\\n\\nWe will now evaluate the model and add a description of the evaluation method with `add`. The metrics are added by `add_metrics`, which will be parsed into a table. \\n\\n```python\\nfrom sklearn.metrics import (ConfusionMatrixDisplay, confusion_matrix,\\n                            accuracy_score, f1_score)\\n# let\\'s make a prediction and evaluate the model\\ny_pred = model.predict(X_test)\\n# we can pass metrics using add_metrics and pass details with add\\nmodel_card.add(eval_method=\"The model is evaluated using test split, on accuracy and F1 score with macro average.\")\\nmodel_card.add_metrics(accuracy=accuracy_score(y_test, y_pred))\\nmodel_card.add_metrics(**{\"f1 score\": f1_score(y_test, y_pred, average=\"micro\")})\\n```\\n\\nWe can also add any plot of our choice to the card using `add_plot` like below.\\n\\n```python\\nimport matplotlib.pyplot as plt\\nfrom pathlib import Path\\n# we will create a confusion matrix\\ncm = confusion_matrix(y_test, y_pred, labels=model.classes_)\\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\\ndisp.plot()\\n\\n# save the plot\\nplt.savefig(Path(local_repo) / \"confusion_matrix.png\")\\n\\n# the plot will be written to the model card under the name confusion_matrix\\n#\\xa0we pass the path of the plot itself\\nmodel_card.add_plot(confusion_matrix=\"confusion_matrix.png\")\\n```\\n\\nLet\\'s save the model card in the local repository. The file name here should be `README.md` since it is what Hugging Face Hub expects.\\n```python\\nmodel_card.save(Path(local_repo) / \"README.md\")\\n```\\n\\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either  `notebook_login` if you\\'re logging in from a notebook, or `huggingface-cli login` if you\\'re logging in from the CLI.\\n\\n```python\\n# if the repository doesn\\'t exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\\nrepo_id = \"skops-user/my-awesome-model\"\\nhub_utils.push(\\n    repo_id=repo_id,\\n    source=local_repo,\\n    token=token,\\n    commit_message=\"pushing files to the repo from the example!\",\\n    create_remote=True,\\n)\\n```\\n\\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\\n\\n```python\\ndownload_repo = \"downloaded-model\"\\nhub_utils.download(repo_id=repo_id, dst=download_repo)\\n```\\n\\nThe inference widget is enabled to make predictions in the repository.\\n\\n![Hosted Inference Widget](assets/94_skops/skops_widget.png)\\n\\nIf the requirements of your project have changed, you can use `update_env` to update the environment.\\n\\n```python\\nhub_utils.update_env(path=local_repo, requirements=[\"scikit-learn\"])\\n```\\n\\nYou can see the example repository pushed with above code [here](https://huggingface.co/scikit-learn/skops-blog-example).\\nWe have prepared two examples to show how to save your models and use model card utilities. You can find them in the resources section below.\\n\\n\\n##\\xa0Resources\\n- [Model card tutorial](https://skops.readthedocs.io/en/latest/auto_examples/plot_model_card.html)\\n- [hub_utils tutorial](https://skops.readthedocs.io/en/latest/auto_examples/plot_hf_hub.html)\\n- [skops documentation](https://skops.readthedocs.io/en/latest/modules/classes.html)\\n'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Textual Inversion\\n\\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.\\n\\n[`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings.\\n\\n<Tip>\\n\\nTo learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading_adapters#textual-inversion) loading guide.\\n\\n</Tip>\\n\\n## TextualInversionLoaderMixin\\n\\n[[autodoc]] loaders.textual_inversion.TextualInversionLoaderMixin'), Document(metadata={}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Methods and tools for efficient training on a single GPU\\n\\nThis guide demonstrates practical techniques that you can use to increase the efficiency of your model\\'s training by \\noptimizing memory utilization, speeding up the training, or both. If you\\'d like to understand how GPU is utilized during \\ntraining, please refer to the [Model training anatomy](model_memory_anatomy) conceptual guide first. This guide \\nfocuses on practical techniques.  \\n\\n<Tip>\\n\\nIf you have access to a machine with multiple GPUs, these approaches are still valid, plus you can leverage additional methods outlined in the [multi-GPU section](perf_train_gpu_many).\\n\\n</Tip>\\n\\nWhen training large models, there are two aspects that should be considered at the same time: \\n\\n* Data throughput/training time\\n* Model performance\\n\\nMaximizing the throughput (samples/second) leads to lower training cost. This is generally achieved by utilizing the GPU \\nas much as possible and thus filling GPU memory to its limit. If the desired batch size exceeds the limits of the GPU memory, \\nthe memory optimization techniques, such as gradient accumulation, can help.\\n\\nHowever, if the preferred batch size fits into memory, there\\'s no reason to apply memory-optimizing techniques because they can \\nslow down the training. Just because one can use a large batch size, does not necessarily mean they should. As part of \\nhyperparameter tuning, you should determine which batch size yields the best results and then optimize resources accordingly.\\n\\nThe methods and tools covered in this guide can be classified based on the effect they have on the training process:\\n\\n| Method/tool                                                | Improves training speed | Optimizes memory utilization |\\n|:-----------------------------------------------------------|:------------------------|:-----------------------------|\\n| [Batch size choice](#batch-size-choice)                    | Yes                     | Yes                          |\\n| [Gradient accumulation](#gradient-accumulation)            | No                      | Yes                          |\\n| [Gradient checkpointing](#gradient-checkpointing)          | No                      | Yes                          |\\n| [Mixed precision training](#mixed-precision-training)      | Yes                     | (No)                         |\\n| [Optimizer choice](#optimizer-choice)                      | Yes                     | Yes                          |\\n| [Data preloading](#data-preloading)                        | Yes                     | No                           |\\n| [DeepSpeed Zero](#deepspeed-zero)                          | No                      | Yes                          |\\n| [torch.compile](#using-torchcompile)                       | Yes                     | No                           |\\n\\n<Tip>\\n\\nNote: when using mixed precision with a small model and a large batch size, there will be some memory savings but with a \\nlarge model and a small batch size, the memory use will be larger.\\n\\n</Tip>\\n\\nYou can combine the above methods to get a cumulative effect. These techniques are available to you whether you are \\ntraining your model with [`Trainer`] or writing a pure PyTorch loop, in which case you can [configure these optimizations \\nwith 🤗 Accelerate](#using-accelerate).\\n\\nIf these methods do not result in sufficient gains, you can explore the following options: \\n* [Look into building your own custom Docker container with efficient softare prebuilds](#efficient-software-prebuilds)\\n* [Consider a model that uses Mixture of Experts (MoE)](#mixture-of-experts)\\n* [Convert your model to BetterTransformer to leverage PyTorch native attention](#using-pytorch-native-attention)\\n\\nFinally, if all of the above is still not enough, even after switching to a server-grade GPU like A100, consider moving \\nto a multi-GPU setup. All these approaches are still valid in a multi-GPU setup, plus you can leverage additional parallelism \\ntechniques outlined in the [multi-GPU section](perf_train_gpu_many). \\n\\n## Batch size choice\\n\\nTo achieve optimal performance, start by identifying the appropriate batch size. It is recommended to use batch sizes and \\ninput/output neuron counts that are of size 2^N. Often it\\'s a multiple of 8, but it can be \\nhigher depending on the hardware being used and the model\\'s dtype.\\n\\nFor reference, check out NVIDIA\\'s recommendation for [input/output neuron counts](\\nhttps://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features) and \\n[batch size](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size) for \\nfully connected layers (which are involved in GEMMs (General Matrix Multiplications)).\\n\\n[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) \\ndefine the multiplier based on the dtype and the hardware. For instance, for fp16 data type a multiple of 8 is recommended, unless \\nit\\'s an A100 GPU, in which case use multiples of 64.\\n\\nFor parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). \\nThis is where tiling happens and the right multiplier can have a significant speedup.\\n\\n## Gradient Accumulation\\n\\nThe **gradient accumulation** method aims to calculate gradients in smaller increments instead of computing them for the \\nentire batch at once. This approach involves iteratively calculating gradients in smaller batches by performing forward \\nand backward passes through the model and accumulating the gradients during the process. Once a sufficient number of \\ngradients have been accumulated, the model\\'s optimization step is executed. By employing gradient accumulation, it \\nbecomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU\\'s memory capacity. \\nHowever, it is important to note that the additional forward and backward passes introduced by gradient accumulation can \\nslow down the training process.\\n\\nYou can enable gradient accumulation by adding the `gradient_accumulation_steps` argument to  [`TrainingArguments`]: \\n\\n```py\\ntraining_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\\n```\\n\\nIn the above example, your effective batch size becomes 4. \\n\\nAlternatively, use 🤗 Accelerate to gain full control over the training loop. Find the 🤗 Accelerate example \\n[further down in this guide](#using-accelerate).\\n\\nWhile it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can \\nresult in a more pronounced training slowdown. Consider the following example. Let\\'s say, the `per_device_train_batch_size=4` \\nwithout gradient accumulation hits the GPU\\'s limit. If you would like to train with batches of size 64, do not set the \\n`per_device_train_batch_size` to 1 and `gradient_accumulation_steps` to 64. Instead, keep `per_device_train_batch_size=4` \\nand set `gradient_accumulation_steps=16`. This results in the same effective batch size while making better use of \\nthe available GPU resources.\\n\\nFor additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).\\n\\n## Gradient Checkpointing\\n\\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. \\nThis is because there are other components that also require memory storage.\\n\\nSaving all activations from the forward pass in order to compute the gradients during the backward pass can result in \\nsignificant memory overhead. The alternative approach of discarding the activations and recalculating them when needed \\nduring the backward pass, would introduce a considerable computational overhead and slow down the training process.\\n\\n**Gradient checkpointing** offers a compromise between these two approaches and saves strategically selected activations \\nthroughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For \\nan in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9).\\n\\nTo enable gradient checkpointing in the [`Trainer`], pass the corresponding a flag to [`TrainingArguments`]:\\n\\n```py\\ntraining_args = TrainingArguments(\\n    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args\\n)\\n```\\n\\nAlternatively, use 🤗 Accelerate - find the 🤗 Accelerate example [further in this guide](#using-accelerate). \\n\\n<Tip>\\n\\nWhile gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.\\n\\n</Tip>\\n\\n## Mixed precision training\\n\\n**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models by \\nutilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point \\nprecision (fp32 or float32) to represent and process variables. However, not all variables require this high precision \\nlevel to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit \\nfloating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed \\nin half-precision, while some are still in full precision, the approach is called mixed precision training.\\n\\nMost commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures \\n(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check \\nout the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about \\nthe differences between these data types.\\n\\n### fp16\\n\\nThe main advantage of mixed precision training comes from saving the activations in half precision (fp16). \\nAlthough the gradients are also computed in half precision they are converted back to full precision for the optimization \\nstep so no memory is saved here. \\nWhile mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes.\\nThis is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU).\\n\\nTo enable mixed precision training, set the `fp16` flag to `True`:\\n\\n```py\\ntraining_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\\n```\\n\\nIf you prefer to use 🤗 Accelerate, find the 🤗 Accelerate example [further in this guide](#using-accelerate). \\n\\n### BF16\\n\\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \\nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \\nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \\nis about the same as fp32 - because both have 8-bits used for the numerical range.\\n\\nYou can enable BF16 in the 🤗 Trainer with:\\n\\n```python\\ntraining_args = TrainingArguments(bf16=True, **default_args)\\n```\\n\\n### TF32\\n\\nThe Ampere hardware uses a magical data type called tf32. It has the same numerical range as fp32 (8-bits), but instead \\nof 23 bits precision it has only 10 bits (same as fp16) and uses only 19 bits in total. It\\'s \"magical\" in the sense that \\nyou can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput \\nimprovement. All you need to do is to add the following to your code:\\n\\n```\\nimport torch\\ntorch.backends.cuda.matmul.allow_tf32 = True\\ntorch.backends.cudnn.allow_tf32 = True\\n```\\n\\nCUDA will automatically switch to using tf32 instead of fp32 where possible, assuming that the used GPU is from the Ampere series.\\n\\nAccording to [NVIDIA research](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/), the \\nmajority of machine learning training workloads show the same perplexity and convergence with tf32 training as with fp32. \\nIf you\\'re already using fp16 or bf16 mixed precision it may help with the throughput as well.\\n\\nYou can enable this mode in the 🤗 Trainer:\\n\\n```python\\nTrainingArguments(tf32=True, **default_args)\\n```\\n\\n<Tip>\\n\\ntf32 can\\'t be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an internal CUDA data type. You need `torch>=1.7` to use tf32 data types.\\n\\n</Tip>\\n\\nFor additional information on tf32 vs other precisions, please refer to the following benchmarks: \\n[RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803) and\\n[A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189).\\n\\n## Flash Attention 2\\n\\nYou can speedup the training throughput by using Flash Attention 2 integration in transformers. Check out the appropriate section in the [single GPU section](./perf_infer_gpu_one#Flash-Attention-2) to learn more about how to load a model with Flash Attention 2 modules. \\n\\n## Optimizer choice\\n\\nThe most common optimizer used to train transformer models is Adam or AdamW (Adam with weight decay). Adam achieves \\ngood convergence by storing the rolling average of the previous gradients; however, it adds an additional memory \\nfootprint of the order of the number of model parameters. To remedy this, you can use an alternative optimizer. \\nFor example if you have [NVIDIA/apex](https://github.com/NVIDIA/apex) installed for NVIDIA GPUs, or [ROCmSoftwarePlatform/apex](https://github.com/ROCmSoftwarePlatform/apex) for AMD GPUs, `adamw_apex_fused` will give you the\\nfastest training experience among all supported AdamW optimizers.\\n\\n[`Trainer`] integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch`, `adamw_torch_fused`, \\n`adamw_apex_fused`, `adamw_anyprecision`, `adafactor`, or `adamw_bnb_8bit`. More optimizers can be plugged in via a third-party implementation.\\n\\nLet\\'s take a closer look at two alternatives to AdamW optimizer:\\n1. `adafactor` which is available in [`Trainer`]\\n2. `adamw_bnb_8bit` is also available in Trainer, but a third-party integration is provided below for demonstration.\\n\\nFor comparison, for a 3B-parameter model, like “t5-3b”: \\n* A standard AdamW optimizer will need 24GB of GPU memory because it uses 8 bytes for each parameter (8*3 => 24GB)\\n* Adafactor optimizer will need more than 12GB. It uses slightly more than 4 bytes for each parameter, so 4*3 and then some extra.\\n* 8bit BNB quantized optimizer will use only (2*3) 6GB if all optimizer states are quantized.\\n\\n### Adafactor\\n\\nAdafactor doesn\\'t store rolling averages for each element in weight matrices. Instead, it keeps aggregated information \\n(sums of rolling averages row- and column-wise), significantly reducing its footprint. However, compared to Adam, \\nAdafactor may have slower convergence in certain cases.\\n\\nYou can switch to Adafactor by setting `optim=\"adafactor\"` in [`TrainingArguments`]:\\n\\n```py\\ntraining_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adafactor\", **default_args)\\n```\\n\\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training) \\nyou can notice up to 3x improvement while maintaining the throughput! However, as mentioned before, the convergence of \\nAdafactor can be worse than Adam. \\n\\n### 8-bit Adam\\n\\nInstead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization \\nmeans that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the \\nidea behind mixed precision training.\\n\\nTo use `adamw_bnb_8bit`, you simply need to set `optim=\"adamw_bnb_8bit\"` in [`TrainingArguments`]:\\n\\n```py\\ntraining_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adamw_bnb_8bit\", **default_args)\\n```\\n\\nHowever, we can also use a third-party implementation of the 8-bit optimizer for demonstration purposes to see how that can be integrated.\\n\\nFirst, follow the installation guide in the GitHub [repo](https://github.com/TimDettmers/bitsandbytes) to install the `bitsandbytes` library \\nthat implements the 8-bit Adam optimizer.\\n\\nNext you need to initialize the optimizer. This involves two steps: \\n* First, group the model\\'s parameters into two groups - one where weight decay should be applied, and the other one where it should not. Usually, biases and layer norm parameters are not weight decayed. \\n* Then do some argument housekeeping to use the same parameters as the previously used AdamW optimizer.\\n\\n```py\\nimport bitsandbytes as bnb\\nfrom torch import nn\\nfrom transformers.trainer_pt_utils import get_parameter_names\\n\\ntraining_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\\n\\ndecay_parameters = get_parameter_names(model, [nn.LayerNorm])\\ndecay_parameters = [name for name in decay_parameters if \"bias\" not in name]\\noptimizer_grouped_parameters = [\\n    {\\n        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\\n        \"weight_decay\": training_args.weight_decay,\\n    },\\n    {\\n        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\\n        \"weight_decay\": 0.0,\\n    },\\n]\\n\\noptimizer_kwargs = {\\n    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\\n    \"eps\": training_args.adam_epsilon,\\n}\\noptimizer_kwargs[\"lr\"] = training_args.learning_rate\\nadam_bnb_optim = bnb.optim.Adam8bit(\\n    optimizer_grouped_parameters,\\n    betas=(training_args.adam_beta1, training_args.adam_beta2),\\n    eps=training_args.adam_epsilon,\\n    lr=training_args.learning_rate,\\n)\\n```\\n\\nFinally, pass the custom optimizer as an argument to the `Trainer`:\\n\\n```py\\ntrainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))\\n```\\n\\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training), \\nyou can expect to get about a 3x memory improvement and even slightly higher throughput as using Adafactor. \\n\\n### multi_tensor\\n\\npytorch-nightly introduced `torch.optim._multi_tensor` which should significantly speed up the optimizers for situations \\nwith lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner, take a look at this GitHub [issue](https://github.com/huggingface/transformers/issues/9965).\\n\\n## Data preloading\\n\\nOne of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it \\ncan handle. By default, everything happens in the main process, and it might not be able to read the data from disk fast \\nenough, and thus create a bottleneck, leading to GPU under-utilization. Configure the following arguments to reduce the bottleneck:\\n\\n- `DataLoader(pin_memory=True, ...)` - ensures the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory.\\n- `DataLoader(num_workers=4, ...)` - spawn several workers to preload data faster. During training, watch the GPU utilization stats; if it\\'s far from 100%, experiment with increasing the number of workers. Of course, the problem could be elsewhere, so many workers won\\'t necessarily lead to better performance.\\n\\nWhen using [`Trainer`], the corresponding [`TrainingArguments`] are: `dataloader_pin_memory` (`True` by default), and `dataloader_num_workers` (defaults to `0`).\\n\\n## DeepSpeed ZeRO\\n\\nDeepSpeed is an open-source deep learning optimization library that is integrated with 🤗 Transformers and 🤗 Accelerate.\\nIt provides a wide range of features and optimizations designed to improve the efficiency and scalability of large-scale \\ndeep learning training.\\n\\nIf your model fits onto a single GPU and you have enough space to fit a small batch size, you don\\'t need to use DeepSpeed\\nas it\\'ll only slow things down. However, if the model doesn\\'t fit onto a single GPU or you can\\'t fit a small batch, you can \\nleverage DeepSpeed ZeRO + CPU Offload, or NVMe Offload for much larger models. In this case, you need to separately\\n[install the library](main_classes/deepspeed#installation), then follow one of the guides to create a configuration file \\nand launch DeepSpeed: \\n \\n* For an in-depth guide on DeepSpeed integration with [`Trainer`], review [the corresponding documentation](main_classes/deepspeed), specifically the \\n[section for a single GPU](main_classes/deepspeed#deployment-with-one-gpu). Some adjustments are required to use DeepSpeed in a notebook; please take a look at the [corresponding guide](main_classes/deepspeed#deployment-in-notebooks).\\n* If you prefer to use 🤗 Accelerate, refer to [🤗 Accelerate DeepSpeed guide](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed).\\n\\n## Using torch.compile\\n\\nPyTorch 2.0 introduced a new compile function that doesn\\'t require any modification to existing PyTorch code but can \\noptimize your code by adding a single line of code: `model = torch.compile(model)`.\\n\\nIf using [`Trainer`], you only need `to` pass the `torch_compile` option in the [`TrainingArguments`]: \\n\\n```python\\ntraining_args = TrainingArguments(torch_compile=True, **default_args)\\n```\\n\\n`torch.compile` uses Python\\'s frame evaluation API to automatically create a graph from existing PyTorch programs. After \\ncapturing the graph, different backends can be deployed to lower the graph to an optimized engine. \\nYou can find more details and benchmarks in [PyTorch documentation](https://pytorch.org/get-started/pytorch-2.0/).\\n\\n`torch.compile` has a growing list of backends, which can be found in by calling `torchdynamo.list_backends()`, each of which with its optional dependencies.\\n\\nChoose which backend to use by specifying it via `torch_compile_backend` in the [`TrainingArguments`].  Some of the most commonly used backends are:\\n\\n**Debugging backends**:\\n* `dynamo.optimize(\"eager\")` - Uses PyTorch to run the extracted GraphModule. This is quite useful in debugging TorchDynamo issues.\\n* `dynamo.optimize(\"aot_eager\")` - Uses AotAutograd with no compiler, i.e, just using PyTorch eager for the AotAutograd\\'s extracted forward and backward graphs. This is useful for debugging, and unlikely to give speedups.\\n\\n**Training & inference backends**:\\n* `dynamo.optimize(\"inductor\")` - Uses TorchInductor backend with AotAutograd and cudagraphs by leveraging codegened Triton kernels  [Read more](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)\\n* `dynamo.optimize(\"nvfuser\")` -  nvFuser with TorchScript. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)\\n* `dynamo.optimize(\"aot_nvfuser\")` -  nvFuser with AotAutograd. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)\\n* `dynamo.optimize(\"aot_cudagraphs\")` - cudagraphs with AotAutograd. [Read more](https://github.com/pytorch/torchdynamo/pull/757)\\n\\n**Inference-only backend**s:\\n* `dynamo.optimize(\"ofi\")` -  Uses Torchscript optimize_for_inference.  [Read more](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)\\n* `dynamo.optimize(\"fx2trt\")` -  Uses NVIDIA TensorRT for inference optimizations.  [Read more](https://pytorch.org/TensorRT/tutorials/getting_started_with_fx_path.html)\\n* `dynamo.optimize(\"onnxrt\")` -  Uses ONNXRT for inference on CPU/GPU.  [Read more](https://onnxruntime.ai/)\\n* `dynamo.optimize(\"ipex\")` -  Uses IPEX for inference on CPU.  [Read more](https://github.com/intel/intel-extension-for-pytorch)\\n\\nFor an example of using `torch.compile` with 🤗 Transformers, check out this [blog post on fine-tuning a BERT model for Text Classification using the newest PyTorch 2.0 features](https://www.philschmid.de/getting-started-pytorch-2-0-transformers)\\n\\n## Using 🤗 Accelerate\\n\\nWith [🤗 Accelerate](https://huggingface.co/docs/accelerate/index) you can use the above methods while gaining full \\ncontrol over the training loop and can essentially write the loop in pure PyTorch with some minor modifications. \\n\\nSuppose you have combined the methods in the [`TrainingArguments`] like so:\\n\\n```py\\ntraining_args = TrainingArguments(\\n    per_device_train_batch_size=1,\\n    gradient_accumulation_steps=4,\\n    gradient_checkpointing=True,\\n    fp16=True,\\n    **default_args,\\n)\\n```\\n\\nThe full example training loop with 🤗 Accelerate is only a handful of lines of code long:\\n\\n```py\\nfrom accelerate import Accelerator\\nfrom torch.utils.data.dataloader import DataLoader\\n\\ndataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\\n\\nif training_args.gradient_checkpointing:\\n    model.gradient_checkpointing_enable()\\n\\naccelerator = Accelerator(fp16=training_args.fp16)\\nmodel, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\\n\\nmodel.train()\\nfor step, batch in enumerate(dataloader, start=1):\\n    loss = model(**batch).loss\\n    loss = loss / training_args.gradient_accumulation_steps\\n    accelerator.backward(loss)\\n    if step % training_args.gradient_accumulation_steps == 0:\\n        optimizer.step()\\n        optimizer.zero_grad()\\n```\\n\\nFirst we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \\nThen we can enable gradient checkpointing by calling the model\\'s [`~PreTrainedModel.gradient_checkpointing_enable`] method. \\nWhen we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator) \\nwe can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call. \\nDuring the [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare) \\ncall the dataloader will also be distributed across workers should we use multiple GPUs. We use the same [8-bit optimizer](#8-bit-adam) from the earlier example.\\n\\nFinally, we can add the main training loop. Note that the `backward` call is handled by 🤗 Accelerate. We can also see\\nhow gradient accumulation works: we normalize the loss, so we get the average at the end of accumulation and once we have \\nenough steps we run the optimization. \\n\\nImplementing these optimization techniques with 🤗 Accelerate only takes a handful of lines of code and comes with the \\nbenefit of more flexibility in the training loop. For a full documentation of all features have a look at the \\n[Accelerate documentation](https://huggingface.co/docs/accelerate/index).\\n\\n\\n## Efficient Software Prebuilds\\n\\nPyTorch\\'s [pip and conda builds](https://pytorch.org/get-started/locally/#start-locally) come prebuilt with the cuda toolkit \\nwhich is enough to run PyTorch, but it is insufficient if you need to build cuda extensions.\\n\\nAt times, additional efforts may be required to pre-build some components. For instance, if you\\'re using libraries like `apex` that \\ndon\\'t come pre-compiled. In other situations figuring out how to install the right cuda toolkit system-wide can be complicated. \\nTo address these scenarios PyTorch and NVIDIA released a new version of NGC docker container which already comes with \\neverything prebuilt. You just need to install your programs on it, and it will run out of the box.\\n\\nThis approach is also useful if you want to tweak the pytorch source and/or make a new customized build.\\nTo find the docker image version you want start [with PyTorch release notes](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/), \\nchoose one of the latest monthly releases. Go into the release\\'s notes for the desired release, check that the environment\\'s \\ncomponents are matching your needs (including NVIDIA Driver requirements!) and then at the very top of that document go \\nto the corresponding NGC page. If for some reason you get lost, here is [the index of all PyTorch NGC images](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch).\\n\\nNext follow the instructions to download and deploy the docker image.\\n\\n## Mixture of Experts\\n\\nSome recent papers reported a 4-5x training speedup and a faster inference by integrating\\nMixture of Experts (MoE) into the Transformer models.\\n\\nSince it has been discovered that more parameters lead to better performance, this technique allows to increase the \\nnumber of parameters by an order of magnitude without increasing training costs.\\n\\nIn this approach every other FFN layer is replaced with a MoE Layer which consists of many experts, with a gated function \\nthat trains each expert in a balanced way depending on the input token\\'s position in a sequence.\\n\\n![MoE Transformer 2x block](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perf-moe-transformer.png)\\n\\n(source: [GLAM](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html))\\n\\nYou can find exhaustive details and comparison tables in the papers listed at the end of this section.\\n\\nThe main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude \\nlarger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements.\\n\\nThere is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or \\nhundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the \\nmemory requirements moderately as well.\\n\\nMost related papers and implementations are built around Tensorflow/TPUs:\\n\\n- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)\\n- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)\\n- [GLaM: Generalist Language Model (GLaM)](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)\\n\\nAnd for Pytorch DeepSpeed has built one as well: [DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale](https://arxiv.org/abs/2201.05596), [Mixture of Experts](https://www.deepspeed.ai/tutorials/mixture-of-experts/) - blog posts:  [1](https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/), [2](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/) and specific deployment with large transformer-based natural language generation models: [blog post](https://www.deepspeed.ai/2021/12/09/deepspeed-moe-nlg.html), [Megatron-Deepspeed branch](https://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training).\\n\\n## Using PyTorch native attention and Flash Attention\\n\\nPyTorch 2.0 released a native [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) (SDPA), \\nthat allows using fused GPU kernels such as [memory-efficient attention](https://arxiv.org/abs/2112.05682) and [flash attention](https://arxiv.org/abs/2205.14135).\\n\\nAfter installing the [`optimum`](https://github.com/huggingface/optimum) package, the relevant internal modules can be \\nreplaced to use PyTorch\\'s native attention with:\\n\\n```python\\nmodel = model.to_bettertransformer()\\n```\\n\\nOnce converted, train the model as usual.\\n\\n<Tip warning={true}>\\n\\nThe PyTorch-native `scaled_dot_product_attention` operator can only dispatch to Flash Attention if no `attention_mask` is provided.\\n\\nBy default, in training mode, the BetterTransformer integration **drops the mask support and can only be used for training that does not require a padding mask for batched training**. This is the case, for example, d'), Document(metadata={}, page_content=' Developing Faster with Auto-Reloading\\n\\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\\n\\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\\n\\n## Why Auto-Reloading?\\n\\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\\n\\nTo make it faster and more convenient to write your code, we\\'ve made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We\\'ve also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\\n\\nThis short Guide will cover both of these methods, so no matter how you write Python, you\\'ll leave knowing how to build Gradio apps faster.\\n\\n## Python IDE Reload 🔥\\n\\nIf you are building Gradio Blocks using a Python IDE, your file of code (let\\'s name it `run.py`) might look something like this:\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    gr.Markdown(\"# Greetings from Gradio!\")\\n    inp = gr.Textbox(placeholder=\"What is your name?\")\\n    out = gr.Textbox()\\n\\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\\n               inputs=inp,\\n               outputs=out)\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n```\\n\\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\\n\\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\\n\\nIn the terminal, run `gradio run.py`. That\\'s it!\\n\\nNow, you\\'ll see that after you\\'ll see something like this:\\n\\n```bash\\nWatching: \\'/Users/freddy/sources/gradio/gradio\\', \\'/Users/freddy/sources/gradio/demo/\\'\\n\\nRunning on local URL:  http://127.0.0.1:7860\\n```\\n\\nThe important part here is the line that says `Watching...` What\\'s happening here is that Gradio will be observing the directory where `run.py` file lives, and if the file changes, it will automatically rerun the file for you. So you can focus on writing your code, and your Gradio demo will refresh automatically 🥳\\n\\n⚠️ Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app.\\n\\nThere is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as my_demo:\\n    gr.Markdown(\"# Greetings from Gradio!\")\\n    inp = gr.Textbox(placeholder=\"What is your name?\")\\n    out = gr.Textbox()\\n\\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\\n               inputs=inp,\\n               outputs=out)\\n\\nif __name__ == \"__main__\":\\n    my_demo.launch()\\n```\\n\\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\\n\\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you\\'ve done like this:\\n\\n1. Configure encoding declaration of python script, for example: `# -*- coding: cp1252 -*-`\\n2. Confirm that your code editor has identified that encoding format. \\n3. Run like this: `gradio run.py --encoding cp1252`\\n\\n🔥 If your application accepts command line arguments, you can pass them in as well. Here\\'s an example:\\n\\n```python\\nimport gradio as gr\\nimport argparse\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\"--name\", type=str, default=\"User\")\\nargs, unknown = parser.parse_known_args()\\n\\nwith gr.Blocks() as demo:\\n    gr.Markdown(f\"# Greetings {args.name}!\")\\n    inp = gr.Textbox()\\n    out = gr.Textbox()\\n\\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n```\\n\\nWhich you could run like this: `gradio run.py --name Gretel`\\n\\nAs a small aside, this auto-reloading happens if you change your `run.py` source code or the Gradio source code. Meaning that this can be useful if you decide to [contribute to Gradio itself](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md) ✅\\n\\n## Jupyter Notebook Magic 🔮\\n\\nWhat about if you use Jupyter Notebooks (or Colab Notebooks, etc.) to develop code? We got something for you too!\\n\\nWe\\'ve developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\\n\\n`%load_ext gradio`\\n\\nThen, in the cell that you are developing your Gradio demo, simply write the magic command **`%%blocks`** at the top, and then write the layout and components like you would normally:\\n\\n```py\\n%%blocks\\n\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    gr.Markdown(f\"# Greetings {args.name}!\")\\n    inp = gr.Textbox()\\n    out = gr.Textbox()\\n\\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)\\n```\\n\\nNotice that:\\n\\n- You do not need to launch your demo — Gradio does that for you automatically!\\n\\n- Every time you rerun the cell, Gradio will re-render your app on the same port and using the same underlying web server. This means you\\'ll see your changes _much, much faster_ than if you were rerunning the cell normally.\\n\\nHere\\'s what it looks like in a jupyter notebook:\\n\\n![](https://gradio-builds.s3.amazonaws.com/demo-files/jupyter_reload.gif)\\n\\n🪄 This works in colab notebooks too! [Here\\'s a colab notebook](https://colab.research.google.com/drive/1zAuWoiTIb3O2oitbtVb2_ekv1K6ggtC1?usp=sharing) where you can see the Blocks magic in action. Try making some changes and re-running the cell with the Gradio code!\\n\\nThe Notebook Magic is now the author\\'s preferred way of building Gradio demos. Regardless of how you write Python code, we hope either of these methods will give you a much better development experience using Gradio.\\n\\n---\\n\\n## Next Steps\\n\\nNow that you know how to develop quickly using Gradio, start building your own!\\n\\nIf you are looking for inspiration, try exploring demos other people have built with Gradio, [browse public Hugging Face Spaces](http://hf.space/) 🤗\\n'), Document(metadata={}, page_content='--\\ntitle: \"How to Install and Use the Hugging Face Unity API\"\\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\\nauthors:\\n- user: dylanebert\\n---\\n\\n# How to Install and Use the Hugging Face Unity API\\n\\n<!-- {authors} --> \\n\\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we\\'ll walk through the steps to install and use the Hugging Face Unity API.\\n\\n## Installation\\n\\n1. Open your Unity project\\n2. Go to `Window` -> `Package Manager`\\n3. Click `+` and select `Add Package from git URL`\\n4. Enter `https://github.com/huggingface/unity-api.git`\\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\\n\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\\n</figure> \\n\\n6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\\n7. Test the API key by clicking `Test API key` in the API Wizard.\\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.\\n\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/apiwizard.png\">\\n</figure> \\n\\nNow that the API is set up, you can make calls from your scripts to the API. Let\\'s look at an example of performing a Sentence Similarity task:\\n\\n```\\nusing HuggingFace.API;\\n\\n/* other code */\\n\\n// Make a call to the API\\nvoid Query() {\\n    string inputText = \"I\\'m on my way to the forest.\";\\n    string[] candidates = {\\n        \"The player is going to the city\",\\n        \"The player is going to the wilderness\",\\n        \"The player is wandering aimlessly\"\\n    };\\n    HuggingFaceAPI.SentenceSimilarity(inputText, OnSuccess, OnError, candidates);\\n}\\n\\n// If successful, handle the result\\nvoid OnSuccess(float[] result) {\\n    foreach(float value in result) {\\n        Debug.Log(value);\\n    }\\n}\\n\\n// Otherwise, handle the error\\nvoid OnError(string error) {\\n    Debug.LogError(error);\\n}\\n\\n/* other code */\\n```\\n\\n## Supported Tasks and Custom Models\\n\\nThe Hugging Face Unity API also currently supports the following tasks:\\n\\n- [Conversation](https://huggingface.co/tasks/conversational)\\n- [Text Generation](https://huggingface.co/tasks/text-generation)\\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\\n- [Text Classification](https://huggingface.co/tasks/text-classification)\\n- [Question Answering](https://huggingface.co/tasks/question-answering)\\n- [Translation](https://huggingface.co/tasks/translation)\\n- [Summarization](https://huggingface.co/tasks/summarization)\\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\\n\\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\\n\\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\\n\\n## Usage Tips\\n\\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\\n\\n## Conclusion\\n\\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# How 🤗 Transformers solve tasks\\n\\nIn [What 🤗 Transformers can do](task_summary), you learned about natural language processing (NLP), speech and audio, computer vision tasks, and some important applications of them. This page will look closely at how models solve these tasks and explain what\\'s happening under the hood. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, decoder, or encoder-decoder structure. In addition to Transformer models, our library also has several convolutional neural networks (CNNs), which are still used today for computer vision tasks. We\\'ll also explain how a modern CNN works.\\n\\nTo explain how tasks are solved, we\\'ll walk through what goes on inside the model to output useful predictions.\\n\\n- [Wav2Vec2](model_doc/wav2vec2) for audio classification and automatic speech recognition (ASR)\\n- [Vision Transformer (ViT)](model_doc/vit) and [ConvNeXT](model_doc/convnext) for image classification\\n- [DETR](model_doc/detr) for object detection\\n- [Mask2Former](model_doc/mask2former) for image segmentation\\n- [GLPN](model_doc/glpn) for depth estimation\\n- [BERT](model_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder\\n- [GPT2](model_doc/gpt2) for NLP tasks like text generation that use a decoder\\n- [BART](model_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder\\n\\n<Tip>\\n\\nBefore you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. If you\\'re just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information! \\n\\n</Tip>\\n\\n## Speech and audio\\n\\n[Wav2Vec2](model_doc/wav2vec2) is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. \\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png\"/>\\n</div>\\n\\nThis model has four main components:\\n\\n1. A *feature encoder* takes the raw audio waveform, normalizes it to zero mean and unit variance, and converts it into a sequence of feature vectors that are each 20ms long.\\n\\n2. Waveforms are continuous by nature, so they can\\'t be divided into separate units like a sequence of text can be split into words. That\\'s why the feature vectors are passed to a *quantization module*, which aims to learn discrete speech units. The speech unit is chosen from a collection of codewords, known as a *codebook* (you can think of this as the vocabulary). From the codebook, the vector or speech unit, that best represents the continuous audio input is chosen and forwarded through the model.\\n\\n3. About half of the feature vectors are randomly masked, and the masked feature vector is fed to a *context network*, which is a Transformer encoder that also adds relative positional embeddings.\\n\\n4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\\n\\nNow that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\\n\\n### Audio classification\\n\\nTo use the pretrained model for audio classification, add a sequence classification head on top of the base Wav2Vec2 model. The classification head is a linear layer that accepts the encoder\\'s hidden states. The hidden states represent the learned features from each audio frame which can have varying lengths. To create one vector of fixed-length, the hidden states are pooled first and then transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and target to find the most likely class.\\n\\nReady to try your hand at audio classification? Check out our complete [audio classification guide](tasks/audio_classification) to learn how to finetune Wav2Vec2 and use it for inference!\\n\\n### Automatic speech recognition\\n\\nTo use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder\\'s hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription.\\n\\nReady to try your hand at automatic speech recognition? Check out our complete [automatic speech recognition guide](tasks/asr) to learn how to finetune Wav2Vec2 and use it for inference!\\n\\n## Computer vision\\n\\nThere are two ways to approach computer vision tasks:\\n\\n1. Split an image into a sequence of patches and process them in parallel with a Transformer.\\n2. Use a modern CNN, like [ConvNeXT](model_doc/convnext), which relies on convolutional layers but adopts modern network designs.\\n\\n<Tip>\\n\\nA third approach mixes Transformers with convolutions (for example, [Convolutional Vision Transformer](model_doc/cvt) or [LeViT](model_doc/levit)). We won\\'t discuss those because they just combine the two approaches we examine here.\\n\\n</Tip>\\n\\nViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we\\'ll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks.\\n\\n### Image classification\\n\\nViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions.\\n\\n#### Transformer\\n\\n[ViT](model_doc/vit) replaces convolutions entirely with a pure Transformer architecture. If you\\'re familiar with the original Transformer, then you\\'re already most of the way toward understanding ViT.\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg\"/>\\n</div>\\n\\nThe main change ViT introduced was in how images are fed to a Transformer:\\n\\n1. An image is split into square non-overlapping patches, each of which gets turned into a vector or *patch embedding*. The patch embeddings are generated from a convolutional 2D layer which creates the proper input dimensions (which for a base Transformer is 768 values for each patch embedding). If you had a 224x224 pixel image, you could split it into 196 16x16 image patches. Just like how text is tokenized into words, an image is \"tokenized\" into a sequence of patches.\\n\\n2. A *learnable embedding* - a special `[CLS]` token - is added to the beginning of the patch embeddings just like BERT. The final hidden state of the `[CLS]` token is used as the input to the attached classification head; other outputs are ignored. This token helps the model learn how to encode a representation of the image.\\n\\n3. The last thing to add to the patch and learnable embeddings are the *position embeddings* because the model doesn\\'t know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder.\\n\\n4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer perceptron head (MLP). ViT\\'s pretraining objective is simply classification. Like other classification heads, the MLP head converts the output into logits over the class labels and calculates the cross-entropy loss to find the most likely class.\\n\\nReady to try your hand at image classification? Check out our complete [image classification guide](tasks/image_classification) to learn how to finetune ViT and use it for inference!\\n\\n#### CNN\\n\\n<Tip>\\n\\nThis section briefly explains convolutions, but it\\'d be helpful to have a prior understanding of how they change an image\\'s shape and size. If you\\'re unfamiliar with convolutions, check out the [Convolution Neural Networks chapter](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb) from the fastai book!\\n\\n</Tip>\\n\\n[ConvNeXT](model_doc/convnext) is a CNN architecture that adopts new and modern network designs to improve performance. However, convolutions are still at the core of the model. From a high-level perspective, a [convolution](glossary#convolution) is an operation where a smaller matrix (*kernel*) is multiplied by a small window of the image pixels. It computes some features from it, such as a particular texture or curvature of a line. Then it slides over to the next window of pixels; the distance the convolution travels is known as the *stride*. \\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\\n</div>\\n\\n<small>A basic convolution without padding or stride, taken from <a href=\"https://arxiv.org/abs/1603.07285\">A guide to convolution arithmetic for deep learning.</a></small>\\n\\nYou can feed this output to another convolutional layer, and with each successive layer, the network learns more complex and abstract things like hotdogs or rockets. Between convolutional layers, it is common to add a pooling layer to reduce dimensionality and make the model more robust to variations of a feature\\'s position.\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png\"/>\\n</div>\\n\\nConvNeXT modernizes a CNN in five ways:\\n\\n1. Change the number of blocks in each stage and \"patchify\" an image with a larger stride and corresponding kernel size. The non-overlapping sliding window makes this patchifying strategy similar to how ViT splits an image into patches.\\n\\n2. A *bottleneck* layer shrinks the number of channels and then restores it because it is faster to do a 1x1 convolution, and you can increase the depth. An inverted bottleneck does the opposite by expanding the number of channels and shrinking them, which is more memory efficient.\\n\\n3. Replace the typical 3x3 convolutional layer in the bottleneck layer with *depthwise convolution*, which applies a convolution to each input channel separately and then stacks them back together at the end. This widens the network width for improved performance.\\n\\n4. ViT has a global receptive field which means it can see more of an image at once thanks to its attention mechanism. ConvNeXT attempts to replicate this effect by increasing the kernel size to 7x7.\\n\\n5. ConvNeXT also makes several layer design changes that imitate Transformer models. There are fewer activation and normalization layers,  the activation function is switched to GELU instead of ReLU, and it uses LayerNorm instead of BatchNorm.\\n\\nThe output from the convolution blocks is passed to a classification head which converts the outputs into logits and calculates the cross-entropy loss to find the most likely label.\\n\\n### Object detection\\n\\n[DETR](model_doc/detr), *DEtection TRansformer*, is an end-to-end object detection model that combines a CNN with a Transformer encoder-decoder.\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png\"/>\\n</div>\\n\\n1. A pretrained CNN *backbone* takes an image, represented by its pixel values, and creates a low-resolution feature map of it. A 1x1 convolution is applied to the feature map to reduce dimensionality and it creates a new feature map with a high-level image representation. Since the Transformer is a sequential model, the feature map is flattened into a sequence of feature vectors that are combined with positional embeddings.\\n\\n2. The feature vectors are passed to the encoder, which learns the image representations using its attention layers. Next, the encoder hidden states are combined with *object queries* in the decoder. Object queries are learned embeddings that focus on the different regions of an image, and they\\'re updated as they progress through each attention layer. The decoder hidden states are passed to a feedforward network that predicts the bounding box coordinates and class label for each object query, or `no object` if there isn\\'t one.\\n\\n    DETR decodes each object query in parallel to output *N* final predictions, where *N* is the number of queries. Unlike a typical autoregressive model that predicts one element at a time, object detection is a set prediction task (`bounding box`, `class label`) that makes *N* predictions in a single pass.\\n\\n3. DETR uses a *bipartite matching loss* during training to compare a fixed number of predictions with a fixed set of ground truth labels. If there are fewer ground truth labels in the set of *N* labels, then they\\'re padded with a `no object` class. This loss function encourages DETR to find a one-to-one assignment between the predictions and ground truth labels. If either the bounding boxes or class labels aren\\'t correct, a loss is incurred. Likewise, if DETR predicts an object that doesn\\'t exist, it is penalized. This encourages DETR to find other objects in an image instead of focusing on one really prominent object.\\n\\nAn object detection head is added on top of DETR to find the class label and the coordinates of the bounding box. There are two components to the object detection head: a linear layer to transform the decoder hidden states into logits over the class labels, and a MLP to predict the bounding box.\\n\\nReady to try your hand at object detection? Check out our complete [object detection guide](tasks/object_detection) to learn how to finetune DETR and use it for inference!\\n\\n### Image segmentation\\n\\n[Mask2Former](model_doc/mask2former) is a universal architecture for solving all types of image segmentation tasks. Traditional segmentation models are typically tailored towards a particular subtask of image segmentation, like instance, semantic or panoptic segmentation. Mask2Former frames each of those tasks as a *mask classification* problem. Mask classification groups pixels into *N* segments, and predicts *N* masks and their corresponding class label for a given image. We\\'ll explain how Mask2Former works in this section, and then you can try finetuning SegFormer at the end.\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/mask2former_architecture.png\"/>\\n</div>\\n\\nThere are three main components to Mask2Former:\\n\\n1. A [Swin](model_doc/swin) backbone accepts an image and creates a low-resolution image feature map from 3 consecutive 3x3 convolutions.\\n\\n2. The feature map is passed to a *pixel decoder* which gradually upsamples the low-resolution features into high-resolution per-pixel embeddings. The pixel decoder actually generates multi-scale features (contains both low- and high-resolution features) with resolutions 1/32, 1/16, and 1/8th of the original image.\\n\\n3. Each of these feature maps of differing scales is fed successively to one Transformer decoder layer at a time in order to capture small objects from the high-resolution features. The key to Mask2Former is the *masked attention* mechanism in the decoder. Unlike cross-attention which can attend to the entire image, masked attention only focuses on a certain area of the image. This is faster and leads to better performance because the local features of an image are enough for the model to learn from.\\n\\n4. Like [DETR](tasks_explained#object-detection), Mask2Former also uses learned object queries and combines them with the image features from the pixel decoder to make a set prediction (`class label`, `mask prediction`). The decoder hidden states are passed into a linear layer and transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and class label to find the most likely one.\\n\\n    The mask predictions are generated by combining the pixel-embeddings with the final decoder hidden states. The sigmoid cross-entropy and dice loss is calculated between the logits and the ground truth mask to find the most likely mask.\\n\\nReady to try your hand at object detection? Check out our complete [image segmentation guide](tasks/semantic_segmentation) to learn how to finetune SegFormer and use it for inference!\\n\\n### Depth estimation\\n\\n[GLPN](model_doc/glpn), *Global-Local Path Network*, is a Transformer for depth estimation that combines a [SegFormer](model_doc/segformer) encoder with a lightweight decoder.\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg\"/>\\n</div>\\n\\n1. Like ViT, an image is split into a sequence of patches, except these image patches are smaller. This is better for dense prediction tasks like segmentation or depth estimation. The image patches are transformed into patch embeddings (see the [image classification](#image-classification) section for more details about how patch embeddings are created), which are fed to the encoder.\\n\\n2. The encoder accepts the patch embeddings, and passes them through several encoder blocks. Each block consists of attention and Mix-FFN layers. The purpose of the latter is to provide positional information. At the end of each encoder block is a *patch merging* layer for creating hierarchical representations. The features of each group of neighboring patches are concatenated, and a linear layer is applied to the concatenated features to reduce the number of patches to a resolution of 1/4. This becomes the input to the next encoder block, where this whole process is repeated until you have image features with resolutions of 1/8, 1/16, and 1/32.\\n\\n3. A lightweight decoder takes the last feature map (1/32 scale) from the encoder and upsamples it to 1/16 scale. From here, the feature is passed into a *Selective Feature Fusion (SFF)* module, which selects and combines local and global features from an attention map for each feature and then upsamples it to 1/8th. This process is repeated until the decoded features are the same size as the original image. The output is passed through two convolution layers and then a sigmoid activation is applied to predict the depth of each pixel.\\n\\n## Natural language processing\\n\\nThe Transformer was initially designed for machine translation, and since then, it has practically become the default architecture for solving all NLP tasks. Some tasks lend themselves to the Transformer\\'s encoder structure, while others are better suited for the decoder. Still, other tasks make use of both the Transformer\\'s encoder-decoder structure.\\n\\n### Text classification\\n\\n[BERT](model_doc/bert) is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides.\\n\\n1. BERT uses [WordPiece](tokenizer_summary#wordpiece) tokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special `[SEP]` token is added to differentiate them. A special `[CLS]` token is added to the beginning of every sequence of text. The final output with the `[CLS]` token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences.\\n\\n2. BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and \"predict\" the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word.\\n\\n    The second pretraining object is next-sentence prediction. The model must predict whether sentence B follows sentence A. Half of the time sentence B is the next sentence, and the other half of the time, sentence B is a random sentence. The prediction, whether it is the next sentence or not, is passed to a feedforward network with a softmax over the two classes (`IsNext` and `NotNext`).\\n\\n3. The input embeddings are passed through multiple encoder layers to output some final hidden states.\\n\\nTo use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label.\\n\\nReady to try your hand at text classification? Check out our complete [text classification guide](tasks/sequence_classification) to learn how to finetune DistilBERT and use it for inference!\\n\\n### Token classification\\n\\nTo use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.\\n\\nReady to try your hand at token classification? Check out our complete [token classification guide](tasks/token_classification) to learn how to finetune DistilBERT and use it for inference!\\n\\n### Question answering\\n\\nTo use BERT for question answering, add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer.\\n\\nReady to try your hand at question answering? Check out our complete [question answering guide](tasks/question_answering) to learn how to finetune DistilBERT and use it for inference!\\n\\n<Tip>\\n\\n💡 Notice how easy it is to use BERT for different tasks once it\\'s been pretrained. You only need to add a specific head to the pretrained model to manipulate the hidden states into your desired output!\\n\\n</Tip>\\n\\n### Text generation\\n\\n[GPT-2](model_doc/gpt2) is a decoder-only model pretrained on a large amount of text. It can generate convincing (though not always true!) text given a prompt and complete other NLP tasks like question answering despite not being explicitly trained to.\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png\"/>\\n</div>\\n\\n1. GPT-2 uses [byte pair encoding (BPE)](tokenizer_summary#bytepair-encoding-bpe) to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses a *masked self-attention* layer which means GPT-2 can\\'t attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT\\'s [`mask`] token because, in masked self-attention, an attention mask is used to set the score to `0` for future tokens.\\n\\n2. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The label is the next token in the sequence, which are created by shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits and the labels to output the next most likely token.\\n\\nGPT-2\\'s pretraining objective is based entirely on [causal language modeling](glossary#causal-language-modeling), predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text.\\n\\nReady to try your hand at text generation? Check out our complete [causal language modeling guide](tasks/language_modeling#causal-language-modeling) to learn how to finetune DistilGPT-2 and use it for inference!\\n\\n<Tip>\\n\\nFor more information about text generation, check out the [text generation strategies](generation_strategies) guide!\\n\\n</Tip>\\n\\n### Summarization\\n\\nEncoder-decoder models like [BART](model_doc/bart) and [T5](model_doc/t5) are designed for the sequence-to-sequence pattern of a summarization task. We\\'ll explain how BART works in this section, and then you can try finetuning T5 at the end.\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png\"/>\\n</div>\\n\\n1. BART\\'s encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. The *text infilling* corruption strategy works the best though. In text infilling, a number of text spans are replaced with a **single** [`mask`] token. This is important because the model has to predict the masked tokens, and it teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn\\'t add a final feedforward network at the end to predict a word.\\n\\n2. The encoder\\'s output is passed to the decoder, which must predict the masked tokens and any uncorrupted tokens from the encoder\\'s output. This gives additional context to help the decoder restore the original text. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The cross-entropy loss is calculated between the logits and the label, which is just the token shifted to the right.\\n\\nReady to try your hand at summarization? Check out our complete [summarization guide](tasks/summarization) to learn how to finetune T5 and use it for inference!\\n\\n<Tip>\\n\\nFor more information about text generation, check out the [text generation strategies](generation_strategies) guide!\\n\\n</Tip>\\n\\n### Translation\\n\\nTranslation is another example of a sequence-to-sequence task, which means you can use an encoder-decoder model like [BART](model_doc/bart) or [T5](model_doc/t5) to do it. We\\'ll explain how BART works in this section, and then you can try finetuning T5 at the end.\\n\\nBART adapts to translation by adding a separate randomly initialized encoder to map a source language to an input that can be decoded into the target language. This new encoder\\'s embeddings are passed to the pretrained encoder instead of the original word embeddings. The source encoder is trained by updating the source encoder, positional embeddings, and input embeddings with the cross-entropy loss from the model output. The model parameters are frozen in this first step, and all the model parameters are trained together in the second step.\\n\\nBART has since been followed up by a multilingual version, mBART, intended for translation and pretrained on many different languages.\\n\\nReady to try your hand at translation? Check out our complete [translation guide](tasks/summarization) to learn how to finetune T5 and use it for inference!\\n\\n<Tip>\\n\\nFor more information about text generation, check out the [text generation strategies](generation_strategies) guide!\\n\\n</Tip>'), Document(metadata={}, page_content=\"hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated. Lastly, the return_tensors option tells the tokenizer to return a PyTorch tensor. Looking at the result, we see we have a dictionary with two keys. Input IDs contains the IDs of both sentences, with 0s where the padding is applied. The second key, attention mask, indicates where padding has been applied, so the model does not pay attention to it. This is all what is inside the tokenization step. Now let's have a look at the second step, the model. As for the tokenizer, there is an AutoModel API, with a from_pretrained method. It will download and cache the configuration of the model as well as the pretrained weights. However, the AutoModel API will only instantiate the body of the model, that is, the part of the model that is left once the pretraining head is removed. It will output a high-dimensional tensor that is a representation of the sentences passed, but which is not directly useful for our classification problem. Here the tensor has two sentences, each of sixteen tokens and the last dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.\"), Document(metadata={}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# 🤗 Optimum notebooks\\n\\nYou can find here a list of the notebooks associated with each accelerator in 🤗 Optimum.\\n\\n## Optimum Habana\\n\\n| Notebook                                                                                                                                                                               | Description                                                                                                                                                                       |  Colab                                                                                                                                                                                                          |        Studio Lab                                                                                                                                                                                                   |\\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\\n| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |\\n\\n## Optimum Intel\\n\\n### OpenVINO\\n\\n| Notebook                                                                                                                                                                               | Description                                                                                                                                                                       |                                     Colab                                                                                                                                                                                                          |        Studio Lab                                                                                                                                                                                                   |\\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\\n| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|\\n| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb) | Show how to apply post-training quantization on a question answering model using [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with OpenVINO| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)|\\n| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb) | Show how to load and compare outputs from two Stable Diffusion models with different precision| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)|\\n\\n\\n### Neural Compressor\\n\\n| Notebook                                                                                                                                                                               | Description                                                                                                                                                                       |                                     Colab                                                                                                                                                                                                          |        Studio Lab                                                                                                                                                                                                   |\\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\\n| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb) | Show how to apply quantization while training your model using Intel [Neural Compressor](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb) |\\n\\n\\n## Optimum ONNX Runtime\\n\\n| Notebook                                                                                                                                                                    | Description                                                                                                                                    |                                                                        Colab                                                                                                                                                                                                          |        Studio Lab                                                                                                                                                                                                   |\\n|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\\n| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) | Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) |\\n| [How to fine-tune a model for text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)             | Show how to DistilBERT model on GLUE tasks using [ONNX Runtime](https://github.com/microsoft/onnxruntime).                                     | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)          | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb) |\\n| [How to fine-tune a model for summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)                         | Show how to fine-tune a T5 model on the BBC news corpus.                                                                                       | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)                |                [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb) |\\n| [How to fine-tune DeBERTa for question-answering with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)                         | Show how to fine-tune a DeBERTa model on the squad.                                                                                       | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)                |                [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb) |\\n'), Document(metadata={}, page_content='--\\ntitle: \"From PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease\"\\nthumbnail: /blog/assets/111_pytorch_ddp_accelerate_transformers/thumbnail.png\\nauthors:\\n- user: muellerzr\\n---\\n\\n# From PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease\\n\\n\\n## General Overview\\n\\nThis tutorial assumes you have a basic understanding of PyTorch and how to train a simple model. It will showcase training on multiple GPUs through a process called Distributed Data Parallelism (DDP) through three different levels of increasing abstraction:\\n\\n- Native PyTorch DDP through the `pytorch.distributed` module\\n- Utilizing 🤗 Accelerate\\'s light wrapper around `pytorch.distributed` that also helps ensure the code can be run on a single GPU and TPUs with zero code changes and miminimal code changes to the original code\\n- Utilizing 🤗 Transformer\\'s high-level Trainer API which abstracts all the boilerplate code and supports various devices and distributed scenarios\\n\\n## What is \"Distributed\" training and why does it matter?\\n\\nTake some very basic PyTorch training code below, which sets up and trains a model on MNIST based on the [official MNIST example](https://github.com/pytorch/examples/blob/main/mnist/main.py)\\n\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom torchvision import datasets, transforms\\n\\nclass BasicNet(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\\n        self.dropout1 = nn.Dropout(0.25)\\n        self.dropout2 = nn.Dropout(0.5)\\n        self.fc1 = nn.Linear(9216, 128)\\n        self.fc2 = nn.Linear(128, 10)\\n        self.act = F.relu\\n\\n    def forward(self, x):\\n        x = self.act(self.conv1(x))\\n        x = self.act(self.conv2(x))\\n        x = F.max_pool2d(x, 2)\\n        x = self.dropout1(x)\\n        x = torch.flatten(x, 1)\\n        x = self.act(self.fc1(x))\\n        x = self.dropout2(x)\\n        x = self.fc2(x)\\n        output = F.log_softmax(x, dim=1)\\n        return output\\n```\\n\\nWe define the training device (`cuda`):\\n\\n```python\\ndevice = \"cuda\"\\n```\\n\\nBuild some PyTorch DataLoaders:\\n\\n```python\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Normalize((0.1307), (0.3081))\\n])\\n\\ntrain_dset = datasets.MNIST(\\'data\\', train=True, download=True, transform=transform)\\ntest_dset = datasets.MNIST(\\'data\\', train=False, transform=transform)\\n\\ntrain_loader = torch.utils.data.DataLoader(train_dset, shuffle=True, batch_size=64)\\ntest_loader = torch.utils.data.DataLoader(test_dset, shuffle=False, batch_size=64)\\n```\\n\\nMove the model to the CUDA device:\\n\\n```python\\nmodel = BasicNet().to(device)\\n```\\n\\nBuild a PyTorch optimizer:\\n\\n```python\\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\\n```\\n\\nBefore finally creating a simplistic training and evaluation loop that performs one full iteration over the dataset and calculates the test accuracy:\\n\\n```python\\nmodel.train()\\nfor batch_idx, (data, target) in enumerate(train_loader):\\n    data, target = data.to(device), target.to(device)\\n    output = model(data)\\n    loss = F.nll_loss(output, target)\\n    loss.backward()\\n    optimizer.step()\\n    optimizer.zero_grad()\\n\\nmodel.eval()\\ncorrect = 0\\nwith torch.no_grad():\\n    for data, target in test_loader:\\n        output = model(data)\\n        pred = output.argmax(dim=1, keepdim=True)\\n        correct += pred.eq(target.view_as(pred)).sum().item()\\nprint(f\\'Accuracy: {100. * correct / len(test_loader.dataset)}\\')\\n```\\n\\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook.\\n\\nHowever, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\\n\\n## PyTorch Distributed Data Parallelism\\n\\nAs the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.\\n\\nTo convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)\\n\\nFirst a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through\\n\\n> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity\\n\\n```python\\nimport os\\nimport torch.distributed as dist\\n\\ndef setup(rank, world_size):\\n    \"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\\n    os.environ[\"MASTER_ADDR\"] = \\'localhost\\'\\n    os.environ[\"MASTER_PORT\"] = \"12355\"\\n\\n    # Initialize the process group\\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\\n\\ndef cleanup():\\n    \"Cleans up the distributed environment\"\\n    dist.destroy_process_group()\\n```\\n\\nThe last piece of the puzzle is *how do I send my data and model to another GPU?*\\n\\nThis is where the `DistributedDataParallel` module comes into play. It will copy your model onto each GPU, and when `loss.backward()` is called the backpropagation is performed and the resulting gradients across all these copies of the model will be averaged/reduced. This ensures each device has the same weights post the optimizer step.\\n\\nBelow is an example of our training setup, refactored as a function, with this capability:\\n\\n> Note: Here rank is the overall rank of the current GPU compared to all the other GPUs available, meaning they have a rank of `0 -> n-1`\\n\\n```python\\nfrom torch.nn.parallel import DistributedDataParallel as DDP\\n\\ndef train(model, rank, world_size):\\n    setup(rank, world_size)\\n    model = model.to(rank)\\n    ddp_model = DDP(model, device_ids=[rank])\\n    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\\n    # Train for one epoch\\n    model.train()\\n    for batch_idx, (data, target) in enumerate(train_loader):\\n        data, target = data.to(device), target.to(device)\\n        output = model(data)\\n        loss = F.nll_loss(output, target)\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()\\n    cleanup()\\n```\\n\\nThe optimizer needs to be declared based on the model *on the specific device* (so `ddp_model` and not `model`) for all of the gradients to properly be calculated.\\n\\nLastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\\n\\n```bash\\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\\n```\\n\\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\\n\\nNow let\\'s talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\\n\\n## 🤗 Accelerate\\n\\n[Accelerate](https://huggingface.co/docs/accelerate) is a library designed to allow you to perform what we just did above, without needing to modify your code greatly. On top of this, the data pipeline innate to Accelerate can also improve performance to your code as well.\\n\\nFirst, let\\'s wrap all of the above code we just performed into a single function, to help us visualize the difference:\\n\\n```python\\ndef train_ddp(rank, world_size):\\n    setup(rank, world_size)\\n    # Build DataLoaders\\n    transform = transforms.Compose([\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.1307), (0.3081))\\n    ])\\n\\n    train_dset = datasets.MNIST(\\'data\\', train=True, download=True, transform=transform)\\n    test_dset = datasets.MNIST(\\'data\\', train=False, transform=transform)\\n\\n    train_loader = torch.utils.data.DataLoader(train_dset, shuffle=True, batch_size=64)\\n    test_loader = torch.utils.data.DataLoader(test_dset, shuffle=False, batch_size=64)\\n\\n    # Build model\\n    model = model.to(rank)\\n    ddp_model = DDP(model, device_ids=[rank])\\n\\n    # Build optimizer\\n    optimizer = optim.AdamW(ddp_model.parameters(), lr=1e-3)\\n\\n    # Train for a single epoch\\n    model.train()\\n    for batch_idx, (data, target) in enumerate(train_loader):\\n        data, target = data.to(device), target.to(device)\\n        output = model(data)\\n        loss = F.nll_loss(output, target)\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()\\n    \\n    # Evaluate\\n    model.eval()\\n    correct = 0\\n    with torch.no_grad():\\n        for data, target in test_loader:\\n            data, target = data.to(device), target.to(device)\\n            output = model(data)\\n            pred = output.argmax(dim=1, keepdim=True)\\n            correct += pred.eq(target.view_as(pred)).sum().item()\\n    print(f\\'Accuracy: {100. * correct / len(test_loader.dataset)}\\')\\n```\\n\\nNext let\\'s talk about how Accelerate can help. There\\'s a few issues with the above code:\\n\\n1. This is slightly inefficient, given that `n` dataloaders are made based on each device and pushed.\\n2. This code will **only** work for multi-GPU, so special care would need to be made for it to be ran on a single node again, or on TPU.\\n\\nAccelerate helps this through the [`Accelerator`](https://huggingface.co/docs/accelerate/v0.12.0/en/package_reference/accelerator#accelerator) class. Through it, the code remains much the same except for three lines of code when comparing a single node to multinode, as shown below:\\n\\n```python\\ndef train_ddp_accelerate():\\n    accelerator = Accelerator()\\n    # Build DataLoaders\\n    transform = transforms.Compose([\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.1307), (0.3081))\\n    ])\\n\\n    train_dset = datasets.MNIST(\\'data\\', train=True, download=True, transform=transform)\\n    test_dset = datasets.MNIST(\\'data\\', train=False, transform=transform)\\n\\n    train_loader = torch.utils.data.DataLoader(train_dset, shuffle=True, batch_size=64)\\n    test_loader = torch.utils.data.DataLoader(test_dset, shuffle=False, batch_size=64)\\n\\n    # Build model\\n    model = BasicNet()\\n\\n    # Build optimizer\\n    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\\n\\n    # Send everything through `accelerator.prepare`\\n    train_loader, test_loader, model, optimizer = accelerator.prepare(\\n        train_loader, test_loader, model, optimizer\\n    )\\n\\n    # Train for a single epoch\\n    model.train()\\n    for batch_idx, (data, target) in enumerate(train_loader):\\n        output = model(data)\\n        loss = F.nll_loss(output, target)\\n        accelerator.backward(loss)\\n        optimizer.step()\\n        optimizer.zero_grad()\\n    \\n    # Evaluate\\n    model.eval()\\n    correct = 0\\n    with torch.no_grad():\\n        for data, target in test_loader:\\n            data, target = data.to(device), target.to(device)\\n            output = model(data)\\n            pred = output.argmax(dim=1, keepdim=True)\\n            correct += pred.eq(target.view_as(pred)).sum().item()\\n    print(f\\'Accuracy: {100. * correct / len(test_loader.dataset)}\\')\\n```\\n\\nWith this your PyTorch training loop is now setup to be ran on any distributed setup thanks to the `Accelerator` object. This code can then still be launched through the `torchrun` CLI or through Accelerate\\'s own CLI interface, [`accelerate launch`](https://huggingface.co/docs/accelerate/v0.12.0/en/basic_tutorials/launch).\\n\\nAs a result its now trivialized to perform distributed training with Accelerate and keeping as much of the barebones PyTorch code the same as possible.\\n\\nEarlier it was mentioned that Accelerate also makes the DataLoaders more efficient. This is through custom Samplers that can send parts of the batches automatically to different devices during training allowing for a single copy of the data to be known at one time, rather than four at once into memory depending on the configuration. Along with this, there is only a single full copy of the original dataset in memory total. Subsets of this dataset are split between all of the nodes that are utilized for training, allowing for much larger datasets to be trained on a single instance without an explosion in memory utilized.\\n\\n### Using the `notebook_launcher`\\n\\nEarlier it was mentioned you can start distributed code directly out of your Jupyter Notebook. This comes from Accelerate\\'s [`notebook_launcher`](https://huggingface.co/docs/accelerate/v0.12.0/en/basic_tutorials/notebook) utility, which allows for starting multi-gpu training based on code inside of a Jupyter Notebook.\\n\\nTo use it is as trivial as importing the launcher:\\n\\n```python\\nfrom accelerate import notebook_launcher\\n```\\n\\nAnd passing the training function we declared earlier, any arguments to be passed, and the number of processes to use (such as 8 on a TPU, or 2 for two GPUs). Both of the above training functions can be ran, but do note that after you start a single launch, the instance needs to be restarted before spawning another\\n\\n```python\\nnotebook_launcher(train_ddp, args=(), num_processes=2)\\n```\\n\\nOr:\\n\\n```python\\nnotebook_launcher(train_ddp_accelerate, args=(), num_processes=2)\\n```\\n\\n## Using 🤗 Trainer\\n\\nFinally, we arrive at the highest level of API -- the Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer).\\n\\nThis wraps as much training as possible while still being able to train on distributed systems without the user needing to do anything at all.\\n\\nFirst we need to import the Trainer:\\n\\n```python\\nfrom transformers import Trainer\\n```\\n\\nThen we define some `TrainingArguments` to control all the usual hyper-parameters. The trainer also works through dictionaries, so a custom collate function needs to be made.\\n\\nFinally, we subclass the trainer and write our own `compute_loss`.\\n\\nAfterwards, this code will also work on a distributed setup without any training code needing to be written whatsoever!\\n\\n```python\\nfrom transformers import Trainer, TrainingArguments\\n\\nmodel = BasicNet()\\n\\ntraining_args = TrainingArguments(\\n    \"basic-trainer\",\\n    per_device_train_batch_size=64,\\n    per_device_eval_batch_size=64,\\n    num_train_epochs=1,\\n    evaluation_strategy=\"epoch\",\\n    remove_unused_columns=False\\n)\\n\\ndef collate_fn(examples):\\n    pixel_values = torch.stack([example[0] for example in examples])\\n    labels = torch.tensor([example[1] for example in examples])\\n    return {\"x\":pixel_values, \"labels\":labels}\\n\\nclass MyTrainer(Trainer):\\n    def compute_loss(self, model, inputs, return_outputs=False):\\n        outputs = model(inputs[\"x\"])\\n        target = inputs[\"labels\"]\\n        loss = F.nll_loss(outputs, target)\\n        return (loss, outputs) if return_outputs else loss\\n\\ntrainer = MyTrainer(\\n    model,\\n    training_args,\\n    train_dataset=train_dset,\\n    eval_dataset=test_dset,\\n    data_collator=collate_fn,\\n)\\n```\\n\\n```python\\ntrainer.train()\\n```\\n\\n```python out\\n    ***** Running training *****\\n      Num examples = 60000\\n      Num Epochs = 1\\n      Instantaneous batch size per device = 64\\n      Total train batch size (w. parallel, distributed & accumulation) = 64\\n      Gradient Accumulation steps = 1\\n      Total optimization steps = 938\\n```\\n\\n| Epoch | Training Loss | Validation Loss |\\n|-------|---------------|-----------------|\\n| 1     | 0.875700      | 0.282633        |\\n\\nSimilarly to the above examples with the `notebook_launcher`, this can be done again here by throwing it all into a training function:\\n\\n```python\\ndef train_trainer_ddp():\\n    model = BasicNet()\\n\\n    training_args = TrainingArguments(\\n        \"basic-trainer\",\\n        per_device_train_batch_size=64,\\n        per_device_eval_batch_size=64,\\n        num_train_epochs=1,\\n        evaluation_strategy=\"epoch\",\\n        remove_unused_columns=False\\n    )\\n\\n    def collate_fn(examples):\\n        pixel_values = torch.stack([example[0] for example in examples])\\n        labels = torch.tensor([example[1] for example in examples])\\n        return {\"x\":pixel_values, \"labels\":labels}\\n\\n    class MyTrainer(Trainer):\\n        def compute_loss(self, model, inputs, return_outputs=False):\\n            outputs = model(inputs[\"x\"])\\n            target = inputs[\"labels\"]\\n            loss = F.nll_loss(outputs, target)\\n            return (loss, outputs) if return_outputs else loss\\n\\n    trainer = MyTrainer(\\n        model,\\n        training_args,\\n        train_dataset=train_dset,\\n        eval_dataset=test_dset,\\n        data_collator=collate_fn,\\n    )\\n\\n    trainer.train()\\n\\nnotebook_launcher(train_trainer_ddp, args=(), num_processes=2)\\n```\\n\\n## Resources\\n\\nTo learn more about PyTorch Distributed Data Parallelism, check out the documentation [here](https://pytorch.org/docs/stable/distributed.html)\\n\\nTo learn more about 🤗 Accelerate, check out the documentation [here](https://huggingface.co/docs/accelerate)\\n\\nTo learn more about 🤗 Transformers, check out the documentation [here](https://huggingface.co/docs/transformers)\\n'), Document(metadata={}, page_content=' Image Classification with Vision Transformers\\n\\nRelated spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\\nTags: VISION, TRANSFORMERS, HUB\\n\\n## Introduction\\n\\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.\\n\\nState-of-the-art image classifiers are based on the _transformers_ architectures, originally popularized for NLP tasks. Such architectures are typically called vision transformers (ViT). Such models are perfect to use with Gradio\\'s _image_ input component, so in this tutorial we will build a web demo to classify images using Gradio. We will be able to build the whole web application in a **single line of Python**, and it will look like the demo on the bottom of the page.\\n\\nLet\\'s get started!\\n\\n### Prerequisites\\n\\nMake sure you have the `gradio` Python package already [installed](/getting_started).\\n\\n## Step 1 — Choosing a Vision Image Classification Model\\n\\nFirst, we will need an image classification model. For this tutorial, we will use a model from the [Hugging Face Model Hub](https://huggingface.co/models?pipeline_tag=image-classification). The Hub contains thousands of models covering dozens of different machine learning tasks.\\n\\nExpand the Tasks category on the left sidebar and select \"Image Classification\" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\\n\\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\\n\\n## Step 2 — Loading the Vision Transformer Model with Gradio\\n\\nWhen using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing.\\nAll of these are automatically inferred from the model tags.\\n\\nBesides the import statement, it only takes a single line of Python to load and launch the demo.\\n\\nWe use the `gr.Interface.load()` method and pass in the path to the model including the `huggingface/` to designate that it is from the Hugging Face Hub.\\n\\n```python\\nimport gradio as gr\\n\\ngr.Interface.load(\\n             \"huggingface/google/vit-base-patch16-224\",\\n             examples=[\"alligator.jpg\", \"laptop.jpg\"]).launch()\\n```\\n\\nNotice that we have added one more parameter, the `examples`, which allows us to prepopulate our interfaces with a few predefined examples.\\n\\nThis produces the following interface, which you can try right here in your browser. When you input an image, it is automatically preprocessed and sent to the Hugging Face Hub API, where it is passed through the model and returned as a human-interpretable prediction. Try uploading your own image!\\n\\n<gradio-app space=\"gradio/vision-transformer\">\\n\\n---\\n\\nAnd you\\'re done! In one line of code, you have built a web demo for an image classifier. If you\\'d like to share with others, try setting `share=True` when you `launch()` the Interface!\\n'), Document(metadata={}, page_content=' Using ESPnet at Hugging Face\\n\\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\\n\\n## Exploring ESPnet in the Hub\\n\\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \\n\\nAll models on the Hub come up with useful features:\\n1. An automatically generated model card with a description, a training configuration, licenses and more.\\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\\n3. An interactive widget you can use to play out with the model directly in the browser.\\n4. An Inference API that allows to make inference requests.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png\"/>\\n</div>\\n\\n## Using existing models\\n\\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo)). \\n\\nIf you\\'re interested in doing inference, different classes for different tasks have a `from_pretrained` method that allows loading models from the Hub. For example:\\n* `Speech2Text` for Automatic Speech Recognition.\\n* `Text2Speech` for Text to Speech.\\n* `SeparateSpeech` for Audio Source Separation.\\n\\nHere is an inference example:\\n\\n```py\\nimport soundfile\\nfrom espnet2.bin.tts_inference import Text2Speech\\n\\ntext2speech = Text2Speech.from_pretrained(\"model_name\")\\nspeech = text2speech(\"foobar\")[\"wav\"]\\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\\n```\\n\\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\\n</div>\\n\\n## Sharing your models\\n\\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).\\n\\nThe `run.sh` script allows to upload a given model to a Hugging Face repository.\\n\\n```bash\\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\\n```\\n\\n## Additional resources\\n\\n* ESPnet [docs](https://espnet.github.io/espnet/index.html).\\n* ESPnet model zoo [repository](https://github.com/espnet/espnet_model_zoo).\\n* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).\\n'), Document(metadata={}, page_content=' Add custom Dependencies\\n\\nInference Endpoints’ base image includes all required libraries to run inference on 🤗 Transformers models, but it also supports custom dependencies. This is useful if you want to:\\n\\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).\\n\\nTo add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\\n\\n```bash\\noptimum[onnxruntime]==1.2.3\\nmkl-include\\nmkl\\n```\\n\\nCheck out the `requirements.txt` files in the following model repositories for examples:\\n\\n* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\\n* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\\n\\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom_container) for inference.\\n'), Document(metadata={}, page_content=' Distillation for quantization on Textual Inversion models to personalize text2image\\n\\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\\n\\n## Installing the dependencies\\n\\nBefore running the scripts, make sure to install the library\\'s training dependencies:\\n\\n```bash\\npip install -r requirements.txt\\n```\\n\\n## Prepare Datasets\\n\\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\\n\\n<a href=\"https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg\">\\n    <img src=\"https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg\" width = \"300\" height=\"300\">\\n</a>\\n\\n## Get a FP32 Textual Inversion model\\n\\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\\n\\n```bash\\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\\nexport DATA_DIR=\"./dicoo\"\\n\\naccelerate launch textual_inversion.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\\\\n  --train_data_dir=$DATA_DIR \\\\\\n  --learnable_property=\"object\" \\\\\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\\\\n  --resolution=512 \\\\\\n  --train_batch_size=1 \\\\\\n  --gradient_accumulation_steps=4 \\\\\\n  --max_train_steps=3000 \\\\\\n  --learning_rate=5.0e-04 --scale_lr \\\\\\n  --lr_scheduler=\"constant\" \\\\\\n  --lr_warmup_steps=0 \\\\\\n  --output_dir=\"dicoo_model\"\\n```\\n\\n## Do distillation for quantization\\n\\nDistillation for quantization is a method that combines [intermediate layer knowledge distillation](https://github.com/intel/neural-compressor/blob/master/docs/source/distillation.md#intermediate-layer-knowledge-distillation) and [quantization aware training](https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md#quantization-aware-training) in the same training process to improve the performance of the quantized model. Provided a FP32 model, the distillation for quantization approach will take this model itself as the teacher model and transfer the knowledges of the specified layers to the student model, i.e. quantized version of the FP32 model, during the quantization aware training process.\\n\\nOnce you have the FP32 Textual Inversion model, the following command will take the FP32 Textual Inversion model as input to do distillation for quantization and generate the INT8 Textual Inversion model.\\n\\n```bash\\nexport FP32_MODEL_NAME=\"./dicoo_model\"\\nexport DATA_DIR=\"./dicoo\"\\n\\naccelerate launch textual_inversion.py \\\\\\n  --pretrained_model_name_or_path=$FP32_MODEL_NAME \\\\\\n  --train_data_dir=$DATA_DIR \\\\\\n  --use_ema --learnable_property=\"object\" \\\\\\n  --placeholder_token=\"<dicoo>\" --initializer_token=\"toy\" \\\\\\n  --resolution=512 \\\\\\n  --train_batch_size=1 \\\\\\n  --gradient_accumulation_steps=4 \\\\\\n  --max_train_steps=300 \\\\\\n  --learning_rate=5.0e-04 --max_grad_norm=3 \\\\\\n  --lr_scheduler=\"constant\" \\\\\\n  --lr_warmup_steps=0 \\\\\\n  --output_dir=\"int8_model\" \\\\\\n  --do_quantization --do_distillation --verify_loading\\n```\\n\\nAfter the distillation for quantization process, the quantized UNet would be 4 times smaller (3279MB -> 827MB).\\n\\n## Inference\\n\\nOnce you have trained a INT8 model with the above command, the inference can be done simply using the `text2images.py` script. Make sure to include the `placeholder_token` in your prompt.\\n\\n```bash\\nexport INT8_MODEL_NAME=\"./int8_model\"\\n\\npython text2images.py \\\\\\n  --pretrained_model_name_or_path=$INT8_MODEL_NAME \\\\\\n  --caption \"a lovely <dicoo> in red dress and hat, in the snowly and brightly night, with many brighly buildings.\" \\\\\\n  --images_num 4\\n```\\n\\nHere is the comparison of images generated by the FP32 model (left) and INT8 model (right) respectively:\\n\\n<p float=\"left\">\\n  <img src=\"https://huggingface.co/datasets/Intel/textual_inversion_dicoo_dfq/resolve/main/FP32.png\" width = \"300\" height = \"300\" alt=\"FP32\" align=center />\\n  <img src=\"https://huggingface.co/datasets/Intel/textual_inversion_dicoo_dfq/resolve/main/INT8.png\" width = \"300\" height = \"300\" alt=\"INT8\" align=center />\\n</p>\\n\\n'), Document(metadata={}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Instantiating a big model\\n\\nWhen you want to use a very big pretrained model, one challenge is to minimize the use of the RAM. The usual workflow\\nfrom PyTorch is:\\n\\n1. Create your model with random weights.\\n2. Load your pretrained weights.\\n3. Put those pretrained weights in your random model.\\n\\nStep 1 and 2 both require a full version of the model in memory, which is not a problem in most cases, but if your model starts weighing several GigaBytes, those two copies can make you get out of RAM. Even worse, if you are using `torch.distributed` to launch a distributed training, each process will load the pretrained model and store these two copies in RAM.\\n\\n<Tip>\\n\\nNote that the randomly created model is initialized with \"empty\" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \\n\\n</Tip>\\n\\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\\n\\n## Sharded checkpoints\\n\\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\\n\\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we\\'ll use a normal-size models with a small shard size: let\\'s take a traditional BERT model.\\n\\n```py\\nfrom transformers import AutoModel\\n\\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\\n```\\n\\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\\n\\n```py\\n>>> import os\\n>>> import tempfile\\n\\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir)\\n...     print(sorted(os.listdir(tmp_dir)))\\n[\\'config.json\\', \\'pytorch_model.bin\\']\\n```\\n\\nNow let\\'s use a maximum shard size of 200MB:\\n\\n```py\\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\\n...     print(sorted(os.listdir(tmp_dir)))\\n[\\'config.json\\', \\'pytorch_model-00001-of-00003.bin\\', \\'pytorch_model-00002-of-00003.bin\\', \\'pytorch_model-00003-of-00003.bin\\', \\'pytorch_model.bin.index.json\\']\\n```\\n\\nOn top of the configuration of the model, we see three different weights files, and an `index.json` file which is our index. A checkpoint like this can be fully reloaded using the [`~PreTrainedModel.from_pretrained`] method:\\n\\n```py\\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\\n...     new_model = AutoModel.from_pretrained(tmp_dir)\\n```\\n\\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\\n\\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\\n\\n```py\\n>>> import json\\n\\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\\n...         index = json.load(f)\\n\\n>>> print(index.keys())\\ndict_keys([\\'metadata\\', \\'weight_map\\'])\\n```\\n\\nThe metadata just consists of the total size of the model for now. We plan to add other information in the future:\\n\\n```py\\n>>> index[\"metadata\"]\\n{\\'total_size\\': 433245184}\\n```\\n\\nThe weights map is the main part of this index, which maps each parameter name (as usually found in a PyTorch model `state_dict`) to the file it\\'s stored in:\\n\\n```py\\n>>> index[\"weight_map\"]\\n{\\'embeddings.LayerNorm.bias\\': \\'pytorch_model-00001-of-00003.bin\\',\\n \\'embeddings.LayerNorm.weight\\': \\'pytorch_model-00001-of-00003.bin\\',\\n ...\\n```\\n\\nIf you want to directly load such a sharded checkpoint inside a model without using [`~PreTrainedModel.from_pretrained`] (like you would do `model.load_state_dict()` for a full checkpoint) you should use [`~modeling_utils.load_sharded_checkpoint`]:\\n\\n```py\\n>>> from transformers.modeling_utils import load_sharded_checkpoint\\n\\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\\n...     load_sharded_checkpoint(model, tmp_dir)\\n```\\n\\n## Low memory loading\\n\\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\\n\\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)\\n'), Document(metadata={}, page_content=' Gradio and W&B Integration\\n\\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\\nTags: WANDB, SPACES\\nContributed by Gradio team\\n\\n## Introduction\\n\\nIn this Guide, we\\'ll walk you through:\\n\\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\\n\\n\\n## What is Wandb?\\n\\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\\n\\n<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\\n\\n## What are Hugging Face Spaces & Gradio?\\n\\n### Gradio\\n\\nGradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model\\'s inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\\n\\nGet started [here](https://gradio.app/getting_started)\\n\\n### Hugging Face Spaces\\n\\nHugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\\n\\n## Setting up a Gradio Demo for JoJoGAN\\n\\nNow, let\\'s walk you through how to do this on your own. We\\'ll make the assumption that you\\'re new to W&B and Gradio for the purposes of this tutorial.\\n\\nLet\\'s get started!\\n\\n1. Create a W&B account\\n\\n   Follow [these quick instructions](https://app.wandb.ai/login) to create your free account if you don’t have one already. It shouldn\\'t take more than a couple minutes. Once you\\'re done (or if you\\'ve already got an account), next, we\\'ll run a quick colab.\\n\\n2. Open Colab Install Gradio and W&B\\n\\n   We\\'ll be following along with the colab provided in the JoJoGAN repo with some minor modifications to use Wandb and Gradio more effectively.\\n\\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb)\\n\\n   Install Gradio and Wandb at the top:\\n\\n```sh\\n\\npip install gradio wandb\\n```\\n\\n3. Finetune StyleGAN and W&B experiment tracking\\n\\n   This next step will open a W&B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here\\'s the code you need for that:\\n\\n   ```python\\n\\n   alpha =  1.0\\n   alpha = 1-alpha\\n\\n   preserve_color = True\\n   num_iter = 100\\n   log_interval = 50\\n\\n\\n   samples = []\\n   column_names = [\"Reference (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\\n\\n   wandb.init(project=\"JoJoGAN\")\\n   config = wandb.config\\n   config.num_iter = num_iter\\n   config.preserve_color = preserve_color\\n   wandb.log(\\n   {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\\n   step=0)\\n\\n   # load discriminator for perceptual loss\\n   discriminator = Discriminator(1024, 2).eval().to(device)\\n   ckpt = torch.load(\\'models/stylegan2-ffhq-config-f.pt\\', map_location=lambda storage, loc: storage)\\n   discriminator.load_state_dict(ckpt[\"d\"], strict=False)\\n\\n   # reset generator\\n   del generator\\n   generator = deepcopy(original_generator)\\n\\n   g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\\n\\n   # Which layers to swap for generating a family of plausible real images -> fake image\\n   if preserve_color:\\n       id_swap = [9,11,15,16,17]\\n   else:\\n       id_swap = list(range(7, generator.n_latent))\\n\\n   for idx in tqdm(range(num_iter)):\\n       mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\\n       in_latent = latents.clone()\\n       in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\\n\\n       img = generator(in_latent, input_is_latent=True)\\n\\n       with torch.no_grad():\\n           real_feat = discriminator(targets)\\n       fake_feat = discriminator(img)\\n\\n       loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\\n\\n\\n       wandb.log({\"loss\": loss}, step=idx)\\n       if idx % log_interval == 0:\\n           generator.eval()\\n           my_sample = generator(my_w, input_is_latent=True)\\n           generator.train()\\n           my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\\n           wandb.log(\\n           {\"Current stylization\": [wandb.Image(my_sample)]},\\n           step=idx)\\n       table_data = [\\n               wandb.Image(transforms.ToPILImage()(target_im)),\\n               wandb.Image(img),\\n               wandb.Image(my_sample),\\n           ]\\n       samples.append(table_data)\\n\\n       g_optim.zero_grad()\\n       loss.backward()\\n       g_optim.step()\\n\\n   out_table = wandb.Table(data=samples, columns=column_names)\\n   wandb.log({\"Current Samples\": out_table})\\n   ```\\n\\nalpha = 1.0\\nalpha = 1-alpha\\n\\npreserve_color = True\\nnum_iter = 100\\nlog_interval = 50\\n\\nsamples = []\\ncolumn_names = [\"Referece (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\\n\\nwandb.init(project=\"JoJoGAN\")\\nconfig = wandb.config\\nconfig.num_iter = num_iter\\nconfig.preserve_color = preserve_color\\nwandb.log(\\n{\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\\nstep=0)\\n\\n# load discriminator for perceptual loss\\n\\ndiscriminator = Discriminator(1024, 2).eval().to(device)\\nckpt = torch.load(\\'models/stylegan2-ffhq-config-f.pt\\', map_location=lambda storage, loc: storage)\\ndiscriminator.load_state_dict(ckpt[\"d\"], strict=False)\\n\\n# reset generator\\n\\ndel generator\\ngenerator = deepcopy(original_generator)\\n\\ng_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\\n\\n# Which layers to swap for generating a family of plausible real images -> fake image\\n\\nif preserve_color:\\nid_swap = [9,11,15,16,17]\\nelse:\\nid_swap = list(range(7, generator.n_latent))\\n\\nfor idx in tqdm(range(num_iter)):\\nmean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\\nin_latent = latents.clone()\\nin_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\\n\\n    img = generator(in_latent, input_is_latent=True)\\n\\n    with torch.no_grad():\\n        real_feat = discriminator(targets)\\n    fake_feat = discriminator(img)\\n\\n    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\\n\\n\\n    wandb.log({\"loss\": loss}, step=idx)\\n    if idx % log_interval == 0:\\n        generator.eval()\\n        my_sample = generator(my_w, input_is_latent=True)\\n        generator.train()\\n        my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\\n        wandb.log(\\n        {\"Current stylization\": [wandb.Image(my_sample)]},\\n        step=idx)\\n    table_data = [\\n            wandb.Image(transforms.ToPILImage()(target_im)),\\n            wandb.Image(img),\\n            wandb.Image(my_sample),\\n        ]\\n    samples.append(table_data)\\n\\n    g_optim.zero_grad()\\n    loss.backward()\\n    g_optim.step()\\n\\nout_table = wandb.Table(data=samples, columns=column_names)\\nwandb.log({\"Current Samples\": out_table})\\n\\n````\\n\\n4. Save, Download, and Load Model\\n\\n    Here\\'s how to save and download your model.\\n\\n```python\\n\\nfrom PIL import Image\\nimport torch\\ntorch.backends.cudnn.benchmark = True\\nfrom torchvision import transforms, utils\\nfrom util import *\\nimport math\\nimport random\\nimport numpy as np\\nfrom torch import nn, autograd, optim\\nfrom torch.nn import functional as F\\nfrom tqdm import tqdm\\nimport lpips\\nfrom model import *\\nfrom e4e_projection import projection as e4e_projection\\n\\nfrom copy import deepcopy\\nimport imageio\\n\\nimport os\\nimport sys\\nimport torchvision.transforms as transforms\\nfrom argparse import Namespace\\nfrom e4e.models.psp import pSp\\nfrom util import *\\nfrom huggingface_hub import hf_hub_download\\nfrom google.colab import files\\n\\ntorch.save({\"g\": generator.state_dict()}, \"your-model-name.pt\")\\n\\nfiles.download(\\'your-model-name.pt\\')\\n\\nlatent_dim = 512\\ndevice=\"cuda\"\\nmodel_path_s = hf_hub_download(repo_id=\"akhaliq/jojogan-stylegan2-ffhq-config-f\", filename=\"stylegan2-ffhq-config-f.pt\")\\noriginal_generator = Generator(1024, latent_dim, 8, 2).to(device)\\nckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)\\noriginal_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\\nmean_latent = original_generator.mean_latent(10000)\\n\\ngenerator = deepcopy(original_generator)\\n\\nckpt = torch.load(\"/content/JoJoGAN/your-model-name.pt\", map_location=lambda storage, loc: storage)\\ngenerator.load_state_dict(ckpt[\"g\"], strict=False)\\ngenerator.eval()\\n\\nplt.rcParams[\\'figure.dpi\\'] = 150\\n\\n\\n\\ntransform = transforms.Compose(\\n    [\\n        transforms.Resize((1024, 1024)),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\\n    ]\\n)\\n\\n\\ndef inference(img):\\n    img.save(\\'out.jpg\\')\\n    aligned_face = align_face(\\'out.jpg\\')\\n\\n    my_w = e4e_projection(aligned_face, \"out.pt\", device).unsqueeze(0)\\n    with torch.no_grad():\\n        my_sample = generator(my_w, input_is_latent=True)\\n\\n\\n    npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()\\n    imageio.imwrite(\\'filename.jpeg\\', npimage)\\n    return \\'filename.jpeg\\'\\n````\\n\\n5. Build a Gradio Demo\\n\\n```python\\n\\nimport gradio as gr\\n\\ntitle = \"JoJoGAN\"\\ndescription = \"Gradio Demo for JoJoGAN: One Shot Face Stylization. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\\n\\ndemo = gr.Interface(\\n    inference,\\n    gr.Image(type=\"pil\"),\\n    gr.Image(type=\"file\"),\\n    title=title,\\n    description=description\\n)\\n\\ndemo.launch(share=True)\\n```\\n\\n6. Integrate Gradio into your W&B Dashboard\\n\\n   The last step—integrating your Gradio demo with your W&B dashboard—is just one extra line:\\n\\n```python\\n\\ndemo.integrate(wandb=wandb)\\n```\\n\\n    Once you call integrate, a demo will be created and you can integrate it into your dashboard or report\\n\\n    Outside of W&B with Web components, using the gradio-app tags allows anyone can embed Gradio demos on HF spaces directly into their blogs, websites, documentation, etc.:\\n\\n```html\\n<gradio-app space=\"akhaliq/JoJoGAN\"> </gradio-app>\\n```\\n\\n7. (Optional) Embed W&B plots in your Gradio App\\n\\n   It\\'s also possible to embed W&B plots within Gradio apps. To do so, you can create a W&B Report of your plots and\\n   embed them within your Gradio app within a `gr.HTML` block.\\n\\n   The Report will need to be public and you will need to wrap the URL within an iFrame like this:\\n\\n```python\\n\\nimport gradio as gr\\n\\ndef wandb_report(url):\\n    iframe = f\\'<iframe src={url} style=\"border:none;height:1024px;width:100%\">\\'\\n    return gr.HTML(iframe)\\n\\nwith gr.Blocks() as demo:\\n    report_url = \\'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx\\'\\n    report = wandb_report(report_url)\\n\\ndemo.launch(share=True)\\n```\\n\\n## Conclusion\\n\\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! Thanks for making it to the end. To recap:\\n\\n- Only one single reference image is needed for fine-tuning JoJoGAN which usually takes about 1 minute on a GPU in colab. After training, style can be applied to any input image. Read more in the paper.\\n\\n- W&B tracks experiments with just a few lines of code added to a colab and you can visualize, sort, and understand your experiments in a single, centralized dashboard.\\n\\n- Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web.\\n\\n## How to contribute Gradio demos on HF spaces on the Wandb organization\\n\\n- Create an account on Hugging Face [here](https://huggingface.co/join).\\n- Add Gradio Demo under your username, see this [course](https://huggingface.co/course/chapter9/4?fw=pt) for setting up Gradio Demo on Hugging Face.\\n- Request to join wandb organization [here](https://huggingface.co/wandb).\\n- Once approved transfer model from your username to Wandb organization\\n'), Document(metadata={}, page_content='--\\ntitle: \"Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\"\\nthumbnail: /blog/assets/80_intel/01.png\\nauthors:\\n- user: juliensimon\\n---\\n\\n\\n\\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\\n\\n\\n\\n\\n![image](assets/80_intel/01.png)\\n\\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption.\\n\\nToday, we\\'re excited to announce that Intel has officially joined our [Hardware Partner Program](https://huggingface.co/hardware).  Thanks to the [Optimum](https://github.com/huggingface/optimum-intel) open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers.\\n\\nTransformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.\\n\\nIntel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.\\n\\n“*We’re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.*”, says Wei Li, Intel Vice President & General Manager, AI and Analytics.\\n\\nIn recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs.\\n\\nThe next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let’s take a deeper look at it.\\n\\n## Get Peak Transformers Performance with Optimum Intel\\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance. \\n\\n[Optimum Intel](https://github.com/huggingface/optimum-intel) is part of Optimum and builds on top of the [Intel Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) (INC). INC is an [open-source library](https://github.com/intel/neural-compressor) that delivers unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help users quickly build the best quantized model.\\n\\nWith Optimum Intel, you can apply state-of-the-art optimization techniques to your Transformers with minimal effort. Let’s look at a complete example.\\n\\n## Case study: Quantizing DistilBERT with Optimum Intel\\n\\nIn this example, we will run post-training quantization on a DistilBERT model fine-tuned for classification. Quantization is a process that shrinks memory and compute requirements by reducing the bit width of model parameters. For example, you can often replace 32-bit floating-point parameters with 8-bit integers at the expense of a small drop in prediction accuracy.\\n\\nWe have already fine-tuned the original model to classify product reviews for shoes according to their star rating (from 1 to 5 stars). You can view this [model](https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews) and its [quantized](https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews-quantized?) version on the Hugging Face hub. You can also test the original model in this [Space](https://huggingface.co/spaces/juliensimon/amazon-shoe-reviews-spaces). \\n\\nLet’s get started! All code is available in this [notebook](https://gitlab.com/juliensimon/huggingface-demos/-/blob/main/amazon-shoes/03_optimize_inc_quantize.ipynb). \\n\\nAs usual, the first step is to install all required libraries. It’s worth mentioning that we have to work with a CPU-only version of PyTorch for the quantization process to work correctly.\\n\\n```\\npip -q uninstall torch -y \\npip -q install torch==1.11.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\\npip -q install transformers datasets optimum[neural-compressor] evaluate --upgrade\\n```\\n\\nThen, we prepare an evaluation dataset to assess model performance during quantization. Starting from the dataset we used to fine-tune the original model, we only keep a few thousand reviews and their labels and save them to local storage.\\n\\nNext, we load the original model, its tokenizer, and the evaluation dataset from the Hugging Face hub.\\n\\n```\\nfrom datasets import load_dataset\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\nmodel_name = \"juliensimon/distilbert-amazon-shoe-reviews\"\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\neval_dataset = load_dataset(\"prashantgrao/amazon-shoe-reviews\", split=\"test\").select(range(300))\\n```\\n\\nNext, we define an evaluation function that computes model metrics on the evaluation dataset. This allows the Optimum Intel library to compare these metrics before and after quantization. For this purpose, the Hugging Face [evaluate](https://github.com/huggingface/evaluate/) library is very convenient!\\n\\n```\\nimport evaluate\\n\\ndef eval_func(model):\\n    task_evaluator = evaluate.evaluator(\"text-classification\")\\n    results = task_evaluator.compute(\\n        model_or_pipeline=model,\\n        tokenizer=tokenizer,\\n        data=eval_dataset,\\n        metric=evaluate.load(\"accuracy\"),\\n        label_column=\"labels\",\\n        label_mapping=model.config.label2id,\\n    )\\n    return results[\"accuracy\"]\\n```\\n\\nWe then set up the quantization job using a [configuration]. You can find details on this configuration on the Neural Compressor [documentation](https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md). Here, we go for post-training dynamic quantization with an acceptable accuracy drop of 5%. If accuracy drops more than the allowed 5%, different part of the model will then be quantized until it an acceptable drop in accuracy or if the maximum number of trials, here set to 10, is reached.\\n\\n```\\nfrom neural_compressor.config import AccuracyCriterion, PostTrainingQuantConfig, TuningCriterion\\n\\ntuning_criterion = TuningCriterion(max_trials=10)\\naccuracy_criterion = AccuracyCriterion(tolerable_loss=0.05)\\n# Load the quantization configuration detailing the quantization we wish to apply\\nquantization_config = PostTrainingQuantConfig(\\n    approach=\"dynamic\",\\n    accuracy_criterion=accuracy_criterion,\\n    tuning_criterion=tuning_criterion,\\n)\\n```\\n\\nWe can now launch the quantization job and save the resulting model and its configuration file to local storage.\\n\\n```\\nfrom neural_compressor.config import PostTrainingQuantConfig\\nfrom optimum.intel.neural_compressor import INCQuantizer\\n\\n# The directory where the quantized model will be saved\\nsave_dir = \"./model_inc\"\\nquantizer = INCQuantizer.from_pretrained(model=model, eval_fn=eval_func)\\nquantizer.quantize(quantization_config=quantization_config, save_directory=save_dir)\\n```\\n\\nThe log tells us that Optimum Intel has quantized 38 ```Linear``` and 2 ```Embedding``` operators.\\n\\n```\\n[INFO] |******Mixed Precision Statistics*****|\\n[INFO] +----------------+----------+---------+\\n[INFO] |    Op Type     |  Total   |   INT8  |\\n[INFO] +----------------+----------+---------+\\n[INFO] |   Embedding    |    2     |    2    |\\n[INFO] |     Linear     |    38    |    38   |\\n[INFO] +----------------+----------+---------+\\n```\\n\\nComparing the first layer of the original model (```model.distilbert.transformer.layer[0]```) and its quantized version (```inc_model.distilbert.transformer.layer[0]```), we see that ```Linear``` has indeed been replaced by ```DynamicQuantizedLinear```, its quantized equivalent.\\n\\n```\\n# Original model\\n\\nTransformerBlock(\\n  (attention): MultiHeadSelfAttention(\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (q_lin): Linear(in_features=768, out_features=768, bias=True)\\n    (k_lin): Linear(in_features=768, out_features=768, bias=True)\\n    (v_lin): Linear(in_features=768, out_features=768, bias=True)\\n    (out_lin): Linear(in_features=768, out_features=768, bias=True)\\n  )\\n  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n  (ffn): FFN(\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (lin1): Linear(in_features=768, out_features=3072, bias=True)\\n    (lin2): Linear(in_features=3072, out_features=768, bias=True)\\n  )\\n  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n)\\n```\\n\\n```\\n# Quantized model\\n\\nTransformerBlock(\\n  (attention): MultiHeadSelfAttention(\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)\\n    (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)\\n    (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)\\n    (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)\\n  )\\n  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n  (ffn): FFN(\\n    (dropout): Dropout(p=0.1, inplace=False)\\n    (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_channel_affine)\\n    (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_channel_affine)\\n  )\\n  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\\n)\\n```\\n\\nVery well, but how does this impact accuracy and prediction time?\\n\\nBefore and after each quantization step, Optimum Intel runs the evaluation function on the current model. The accuracy of the quantized model is now a bit lower  (``` 0.546```) than the original model (```0.574```). We also see that the evaluation step of the quantized model was 1.34x faster than the original model. Not bad for a few lines of code!\\n\\n```\\n[INFO] |**********************Tune Result Statistics**********************|\\n[INFO] +--------------------+----------+---------------+------------------+\\n[INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\\n[INFO] +--------------------+----------+---------------+------------------+\\n[INFO] |      Accuracy      | 0.5740   |    0.5460     |     0.5460       |\\n[INFO] | Duration (seconds) | 13.1534  |    9.7695     |     9.7695       |\\n[INFO] +--------------------+----------+---------------+------------------+\\n```\\n\\nYou can find the resulting [model](https://huggingface.co/juliensimon/distilbert-amazon-shoe-reviews-quantized) hosted on the Hugging Face hub. To load a quantized model hosted locally or on the 🤗 hub, you can do as follows :\\n\\n\\n```\\nfrom optimum.intel.neural_compressor import INCModelForSequenceClassification\\n\\ninc_model = INCModelForSequenceClassification.from_pretrained(save_dir)\\n```\\n\\n## We’re only getting started\\n\\nIn this example, we showed you how to easily quantize models post-training with Optimum Intel, and that’s just the beginning. The library supports other types of quantization as well as pruning, a technique that zeroes or removes model parameters that have little or no impact on the predicted outcome.\\n\\nWe are excited to partner with Intel to bring Hugging Face users peak efficiency on the latest Intel Xeon CPUs and Intel AI libraries. Please [give Optimum Intel a star](https://github.com/huggingface/optimum-intel) to get updates, and stay tuned for many upcoming features!\\n\\n*Many thanks to [Ella Charlaix](https://github.com/echarlaix) for her help on this post.*\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'), Document(metadata={}, page_content=' 控制布局 (Controlling Layout)\\n\\n默认情况下，块中的组件是垂直排列的。让我们看看如何重新排列组件。在幕后，这种布局结构使用了[Web 开发的 flexbox 模型](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox)。\\n\\n## Row 行\\n\\n`with gr.Row` 下的元素将水平显示。例如，要并排显示两个按钮：\\n\\n```python\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        btn1 = gr.Button(\"按钮1\")\\n        btn2 = gr.Button(\"按钮2\")\\n```\\n\\n要使行中的每个元素具有相同的高度，请使用 `style` 方法的 `equal_height` 参数。\\n\\n```python\\nwith gr.Blocks() as demo:\\n    with gr.Row(equal_height=True):\\n        textbox = gr.Textbox()\\n        btn2 = gr.Button(\"按钮2\")\\n```\\n\\n可以通过每个组件中存在的 `scale` 和 `min_width` 参数来控制行中元素的宽度。\\n\\n- `scale` 是一个整数，定义了元素在行中的占用空间。如果将 scale 设置为 `0`，则元素不会扩展占用空间。如果将 scale 设置为 `1` 或更大，则元素将扩展。行中的多个元素将按比例扩展。在下面的示例中，`btn1` 将比 `btn2` 扩展两倍，而 `btn0` 将根本不会扩展：\\n\\n```python\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        btn0 = gr.Button(\"按钮0\", scale=0)\\n        btn1 = gr.Button(\"按钮1\", scale=1)\\n        btn2 = gr.Button(\"按钮2\", scale=2)\\n```\\n\\n- `min_width` 将设置元素的最小宽度。如果没有足够的空间满足所有的 `min_width` 值，行将换行。\\n\\n在[文档](https://gradio.app/docs/#row)中了解有关行的更多信息。\\n\\n## 列和嵌套 (Columns and Nesting)\\n\\n列中的组件将垂直放置在一起。由于默认布局对于块应用程序来说是垂直布局，因此为了有用，列通常嵌套在行中。例如：\\n\\n$code_rows_and_columns\\n$demo_rows_and_columns\\n\\n查看第一列如何垂直排列两个文本框。第二列垂直排列图像和按钮。注意两列的相对宽度由 `scale` 参数设置。具有两倍 `scale` 值的列占据两倍的宽度。\\n\\n在[文档](https://gradio.app/docs/#column)中了解有关列的更多信息。\\n\\n## 选项卡和手风琴 (Tabs and Accordions)\\n\\n您还可以使用 `with gr.Tab(\\'tab_name\\'):` 语句创建选项卡。在 `with gr.Tab(\\'tab_name\\'):` 上下文中创建的任何组件都将显示在该选项卡中。连续的 Tab 子句被分组在一起，以便一次只能选择一个选项卡，并且只显示该选项卡上下文中的组件。\\n\\n例如：\\n\\n$code_blocks_flipper\\n$demo_blocks_flipper\\n\\n还请注意本示例中的 `gr.Accordion(\\'label\\')`。手风琴是一种可以切换打开或关闭的布局。与 `Tabs` 一样，它是可以选择性隐藏或显示内容的布局元素。在 `with gr.Accordion(\\'label\\'):` 内定义的任何组件在单击手风琴的切换图标时都会被隐藏或显示。\\n\\n在文档中了解有关[Tabs](https://gradio.app/docs/#tab)和[Accordions](https://gradio.app/docs/#accordion)的更多信息。\\n\\n## 可见性 (Visibility)\\n\\n组件和布局元素都有一个 `visible` 参数，可以在初始时设置，并使用 `gr.update()` 进行更新。在 Column 上设置 `gr.update(visible=...)` 可用于显示或隐藏一组组件。\\n\\n$code_blocks_form\\n$demo_blocks_form\\n\\n## 可变数量的输出 (Variable Number of Outputs)\\n\\n通过以动态方式调整组件的可见性，可以创建支持 _可变数量输出_ 的 Gradio 演示。这是一个非常简单的例子，其中输出文本框的数量由输入滑块控制：\\n\\n例如：\\n\\n$code_variable_outputs\\n$demo_variable_outputs\\n\\n## 分开定义和渲染组件 (Defining and Rendering Components Separately)\\n\\n在某些情况下，您可能希望在实际渲染 UI 之前定义组件。例如，您可能希望在相应的 `gr.Textbox` 输入上方显示示例部分，使用 `gr.Examples`。由于 `gr.Examples` 需要一个参数作为输入组件对象，您需要先定义输入组件，然后在定义 `gr.Examples` 对象之后再渲染它。\\n\\n解决方法是在 `gr.Blocks()` 范围之外定义 `gr.Textbox`，并在 UI 中想要放置它的位置使用组件的 `.render()` 方法。\\n\\n这是一个完整的代码示例：\\n\\n```python\\ninput_textbox = gr.Textbox()\\n\\nwith gr.Blocks() as demo:\\n    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\\n    input_textbox.render()\\n```\\n'), Document(metadata={}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# OpenVINO\\n\\n🤗 [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\\n\\nYou\\'ll need to install 🤗 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\\n\\n```bash\\npip install --upgrade-strategy eager optimum[\"openvino\"]\\n```\\n\\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\\n\\n## Stable Diffusion\\n\\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\\n\\n```python\\nfrom optimum.intel import OVStableDiffusionPipeline\\n\\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\\nprompt = \"sailing ship in storm by Rembrandt\"\\nimage = pipeline(prompt).images[0]\\n\\n# Don\\'t forget to save the exported model\\npipeline.save_pretrained(\"openvino-sd-v1-5\")\\n```\\n\\nTo further speed-up inference, statically reshape the model. If you change any parameters such as the outputs height or width, you’ll need to statically reshape your model again.\\n\\n```python\\n# Define the shapes related to the inputs and desired outputs\\nbatch_size, num_images, height, width = 1, 1, 512, 512\\n\\n# Statically reshape the model\\npipeline.reshape(batch_size, height, width, num_images)\\n# Compile the model before inference\\npipeline.compile()\\n\\nimage = pipeline(\\n    prompt,\\n    height=height,\\n    width=width,\\n    num_images_per_prompt=num_images,\\n).images[0]\\n```\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png\">\\n</div>\\n\\nYou can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion), and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting.\\n\\n## Stable Diffusion XL\\n\\nTo load and run inference with SDXL, use the [`~optimum.intel.OVStableDiffusionXLPipeline`]:\\n\\n```python\\nfrom optimum.intel import OVStableDiffusionXLPipeline\\n\\nmodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\\npipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\\nprompt = \"sailing ship in storm by Rembrandt\"\\nimage = pipeline(prompt).images[0]\\n```\\n\\nTo further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in the Stable Diffusion section.\\n\\nYou can find more examples in the 🤗 Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image.\\n')]\n"
          ]
        }
      ],
      "source": [
        "# Extract contexts from the dataset and create Langchain documents\n",
        "documents = [Document(page_content=context) for context in best_answers_df['context']]  # Assuming we're using the 'train' split\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "71cd06bd22af41d6900ab5d2b99e4e43",
            "d838c58e48464d5cb0ebb5f89be8ba09",
            "735873eafa104bc5b831f9690d018cb0",
            "6e08d98f07c14660b079c46cdd9b0411",
            "bb83c6b7ef98497c83587ee869af72f4",
            "fdbdcad46e474b2eb4ee628816e6d5b7",
            "55207ca74d354853a2c9aa1bf0276cae",
            "d1b2c9c6745f4e23b3ad952bc508f228",
            "8acac0ad0f844511b101579f35f83a75",
            "423b2d0f15974a2a912a71c10e5c604b",
            "3e6f448d710a4982955cdf0ced1ebcda",
            "17b0453ae4354f5bbb6f5ba545b4960e",
            "3c2e8b12704643c1ae783a0542eaf6f0",
            "7d69dace12ee435dbd4947c036838393",
            "ecd41a6fb4874404aac1358c95d32461",
            "acb3ab97078d4052b54c8ae03fb700c4",
            "d9574d761501402d9272b236ac29cadd",
            "10bce8f67bab4d9590d98a5813fbb9bb",
            "1100d6e47288431ab46ea183c9883ab2",
            "2e79735c37f148bfa75eef2656627ab0",
            "2a429c8575284b3687bfc2cda2752663",
            "f75d1d3989184a04baeed8a4a6ac9f50",
            "02580fea91454497b1a9b2b77d91e764",
            "90408b417502430c840d8c16463a5530",
            "5af3f6aa921445beb9420901af8016f8",
            "e98b5ad42df742828afe1a713c4e6a54",
            "af0e3d20839444b88c8ff72097dad79a",
            "0550c3be4feb46e5a2725cff757ee071",
            "44347cb1c85e485b852b70bdd28cbf85",
            "d2f8fad5e36d4eb79f40ab3e027f1ca2",
            "a36a391789684b63a3926881587543f0",
            "3365d2129380429e85da803739b941df",
            "5d0f1c9b1b0f4f45b47c9177c931ed3d",
            "fbc7f68fb2254c44b62e65cfb8f0818e",
            "5191b093fdd4476bbaceae133b015c79",
            "0e32353981a44579ba56576acfecbad2",
            "14c6876839f3484ab32719a882502024",
            "a0e33fba1f0141438cf3c88871f47dbf",
            "e2cc2ab959094626b5ba49dc9d3f324a",
            "302f0c825d0f404cbd99174a8e3ffb54",
            "af310401122147ca92e77e68daf8278d",
            "7c54d7cf9cbd4331933de9f0b49eadbe",
            "379c9e0641da4e72853b199915cd228f",
            "1a959f7eaee54783b1c7ee797d1b6d38",
            "922eaf620b53441c8a9713449cfcfda6",
            "a3cf31a533c34fc5a8bc7f5e88e00a68",
            "618cc59ffe53437c8d4136db07ff053f",
            "7e2cfc63e91d465fb1afe6717fbf3f65",
            "1b24b6e8765043b5b5b1f68065fc44e9",
            "bd2d2b2627724b68b34b27bf180ed2d3",
            "9a81f406468e45e5b29a1a45ee4cbf78",
            "378d464a1d9e4310a1fc65728c9c1139",
            "bf179efbd9554bd8875a2fa7b1f5a01e",
            "3331064bfae04daf87d1a711d4e11621",
            "b134bc01a447487c99d1f7a4c683ba46",
            "b60c62a3900944efa77b22b1da13b0f7",
            "bdd3272dc63f430ea79fa1da3c16e673",
            "08c3d4d0af77405690cc59383fa6d2ac",
            "6f1d7db41e3f4e8a8b1c5f86f436ac42",
            "d14a45bb63e240c986bd40567e0af264",
            "2aac3348bcf747d5a4ac373d6fdc70d6",
            "2064f735a0d145cca6efada3f3957fbd",
            "fc31b219676d47c8899d3653cb93cef8",
            "fdc2c7d46955484a8224606be5a0de87",
            "c535c0fd698143f4afe964f548a8641d",
            "ad9446bed66e42fab514a0be435f7f3c"
          ]
        },
        "id": "yK1mnOMPEg46",
        "outputId": "633f7652-4950-41c8-9dd4-78efbcb65527"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71cd06bd22af41d6900ab5d2b99e4e43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17b0453ae4354f5bbb6f5ba545b4960e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02580fea91454497b1a9b2b77d91e764"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbc7f68fb2254c44b62e65cfb8f0818e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "922eaf620b53441c8a9713449cfcfda6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b60c62a3900944efa77b22b1da13b0f7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "max_seq_length = tokenizer.model_max_length\n",
        "embedding_model = AutoModel.from_pretrained(model_name)\n",
        "#load ' sentence-transformers/all-MiniLM-L6-v2' embedding model from Hugging Face\n",
        "# embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w_Xs4cIPEg46"
      },
      "outputs": [],
      "source": [
        "def get_seq_length(text: str):\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "    return len(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PuWYRYiEg46",
        "outputId": "c7de01a8-02b4-4998-cb47-8f3442715843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n"
          ]
        }
      ],
      "source": [
        "print(max_seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPrfhPe5Eg46"
      },
      "source": [
        "# Defining text splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B3qr_EKEg47",
        "outputId": "792c2df8-2c88-414d-f3c3-a50a053693da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunk_size 462\n"
          ]
        }
      ],
      "source": [
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "# Use RecursiveCharacterTextSplitter to split documents into chunks\n",
        "chunk_overlap = 50\n",
        "chunk_size = max_seq_length - chunk_overlap\n",
        "print('chunk_size',chunk_size)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        "    length_function=get_seq_length,\n",
        "    add_start_index=True,\n",
        "    separators=MARKDOWN_SEPARATORS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "flfZ4zNyEg47"
      },
      "outputs": [],
      "source": [
        "class Chunk:\n",
        "    def __init__(self, text: str):\n",
        "        self.text = text\n",
        "        self.context = None\n",
        "\n",
        "class ProcessedDocument:\n",
        "    def __init__(self, document: Document, chunks: list[Chunk]):\n",
        "        self.document = document\n",
        "        self.chunks = chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8x5T6XzSEg47"
      },
      "outputs": [],
      "source": [
        "docs_processed: list[ProcessedDocument] = []\n",
        "for doc in documents:\n",
        "    text = doc.page_content  # Extract the text content from the Document\n",
        "    chunks = text_splitter.split_text(text)  # Split the text into chunks (strings)\n",
        "    processed_doc = ProcessedDocument(\n",
        "        doc,\n",
        "        [Chunk(chunk_text) for chunk_text in chunks]\n",
        "    )\n",
        "    docs_processed.append(processed_doc)\n",
        "# chunks = [doc.page_content for doc in docs_processed]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for doc in docs_processed:\n",
        "#     for chunk in doc.chunks:\n",
        "#         try:\n",
        "#           chunk_length = get_seq_length(chunk.text)\n",
        "#           if chunk_length > max_seq_length:\n",
        "#               print(f\"Chunk exceeds max length: {chunk_length} tokens\")\n",
        "#         except Exception as e:\n",
        "#           print(f\"Error processing chunk: {e}\")\n",
        "#           print(\"===========================\")\n",
        "#           print(f\"Chunk: {chunk.text}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "uGPIkqc2hJGn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqD1CrhoEg47",
        "outputId": "d81b4783-e8fd-4479-81ee-c773f4067582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of chunks across all documents: 533\n"
          ]
        }
      ],
      "source": [
        "# Count total chunks\n",
        "total_chunks = sum(len(doc.chunks) for doc in docs_processed)\n",
        "print(f\"Total number of chunks across all documents: {total_chunks}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xbm1paDEg47"
      },
      "source": [
        "# Define summary chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uZw6QTXGEg47"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional\n",
        "class Context(BaseModel):\n",
        "    context: Optional[str] = Field(description=\"Summary of the chunk in the context of the document\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "luOsyyoyEg47"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-4cmkTG6Eg48"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# MODEL_GROQ = \"llama-3.1-8b-instant\"\n",
        "MODEL_GROQ = \"llama-3.2-90b-text-preview\"\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "if not groq_api_key:\n",
        "  groq_api_key = input(\"Please enter your GROQ API KEY: \")\n",
        "\n",
        "llm = ChatGroq(api_key=groq_api_key, model=MODEL_GROQ,\n",
        "                        temperature=0,\n",
        "                        max_tokens=None,\n",
        "                        timeout=None,\n",
        "                        max_retries=2,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "C0ZaOjQlEg48"
      },
      "outputs": [],
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"document\", \"chunk\"],\n",
        "    template=\n",
        "       \"\"\"You are an AI assistant specializing in summarization of documents.\n",
        "          Your are provide brief, relevant context for a chunk of text\n",
        "            based on the following document.\n",
        "\n",
        "            Here is the document:\n",
        "            <doc>\n",
        "            {document}\n",
        "            </doc>\n",
        "\n",
        "            Here is the chunk we want to situate within the whole document:\n",
        "            <chunk>\n",
        "            {chunk}\n",
        "            </chunk>\n",
        "\n",
        "            Provide a concise context (3-4 sentences max) for this chunk,\n",
        "            considering the following guidelines:\n",
        "            - Give a short succinct context Be this chunk within the overall\n",
        "            document for the purposes of improving search retrieval of the chunk.\n",
        "            - Answer only with the succinct context and nothing else.\n",
        "            - Context should be mentioned like ‘Focuses on ....'\n",
        "            do not mention 'this chunk or section focuses on...'\n",
        "            \"\"\")\n",
        "\n",
        "def create_context_chain(llm):\n",
        "    # Configure the LLM to produce structured output\n",
        "    structured_llm = llm.with_structured_output(Context)\n",
        "\n",
        "    # Create the chain using the pipe operator\n",
        "    chain = prompt_template | structured_llm\n",
        "    return chain\n",
        "\n",
        "context_chain = create_context_chain(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHIhi0RWWq0z",
        "outputId": "84d6317e-9d01-4a2f-c311-958cc028b0c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page:\n",
            "  Convert weights to safetensors\n",
            "\n",
            "PyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n",
            "\n",
            "The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n",
            "\n",
            "<Tip warning={true}>\n",
            "\n",
            "For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\n",
            "\n",
            "Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\n",
            "\n",
            "</Tip>\n",
            "\n",
            "chunk:\n",
            " Convert weights to safetensors\n",
            "\n",
            "PyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n",
            "\n",
            "The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n",
            "\n",
            "<Tip warning={true}>\n",
            "\n",
            "For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\n",
            "\n",
            "Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\n",
            "\n",
            "</Tip>\n"
          ]
        }
      ],
      "source": [
        "doc = docs_processed[30]\n",
        "print(\"page:\\n\",doc.document.page_content)\n",
        "# for chunk in doc.chunks:\n",
        "chunk = doc.chunks[0]\n",
        "print('chunk:\\n', chunk.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1poXgNkCWMxL"
      },
      "outputs": [],
      "source": [
        "context: Context = context_chain.invoke({\"document\": doc.document.page_content, \"chunk\": chunk})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLtidzjsaMU5",
        "outputId": "37366901-5ce2-453a-e208-1a24c0c18006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunk with context: Context: \n",
            "\n",
            " Converting PyTorch model weights to safetensors format using the Convert Space or a local script. \n",
            "\n",
            " Chunk: Convert weights to safetensors\n",
            "\n",
            "PyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n",
            "\n",
            "The easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n",
            "\n",
            "<Tip warning={true}>\n",
            "\n",
            "For larger models, the Space may be a bit slower because its resources are tied up in converting other models. You can also try running the [convert.py](https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py) script (this is what the Space is running) locally to convert your weights.\n",
            "\n",
            "Feel free to ping [@Narsil](https://huggingface.co/Narsil) for any issues with the Space.\n",
            "\n",
            "</Tip>\n"
          ]
        }
      ],
      "source": [
        "print(f\"chunk with context: Context: \\n\\n {context.context} \\n\\n Chunk: {chunk.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TdEg5IDqJoWe",
        "outputId": "957e648f-17a5-4ced-9069-60183cf625b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed chunk 1 in current minute. Total chunks processed: 1\n",
            "Processed chunk 2 in current minute. Total chunks processed: 2\n",
            "Processed chunk 3 in current minute. Total chunks processed: 3\n",
            "Processed chunk 4 in current minute. Total chunks processed: 4\n",
            "Processed chunk 5 in current minute. Total chunks processed: 5\n",
            "Processed chunk 6 in current minute. Total chunks processed: 6\n",
            "Processed chunk 7 in current minute. Total chunks processed: 7\n",
            "Processed chunk 8 in current minute. Total chunks processed: 8\n",
            "Processed chunk 9 in current minute. Total chunks processed: 9\n",
            "Processed chunk 10 in current minute. Total chunks processed: 10\n",
            "Processed chunk 11 in current minute. Total chunks processed: 11\n",
            "Made 11 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 12\n",
            "Processed chunk 2 in current minute. Total chunks processed: 13\n",
            "Processed chunk 3 in current minute. Total chunks processed: 14\n",
            "Processed chunk 4 in current minute. Total chunks processed: 15\n",
            "Processed chunk 5 in current minute. Total chunks processed: 16\n",
            "Made 5 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 17\n",
            "Processed chunk 2 in current minute. Total chunks processed: 18\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 19\n",
            "Processed chunk 2 in current minute. Total chunks processed: 20\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 21\n",
            "Processed chunk 2 in current minute. Total chunks processed: 22\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 23\n",
            "Processed chunk 2 in current minute. Total chunks processed: 24\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 25\n",
            "Processed chunk 2 in current minute. Total chunks processed: 26\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 27\n",
            "Processed chunk 2 in current minute. Total chunks processed: 28\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 29\n",
            "Processed chunk 2 in current minute. Total chunks processed: 30\n",
            "Processed chunk 3 in current minute. Total chunks processed: 31\n",
            "Made 3 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 32\n",
            "Processed chunk 2 in current minute. Total chunks processed: 33\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 34\n",
            "Processed chunk 2 in current minute. Total chunks processed: 35\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 36\n",
            "Processed chunk 2 in current minute. Total chunks processed: 37\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 38\n",
            "Processed chunk 2 in current minute. Total chunks processed: 39\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 40\n",
            "Processed chunk 2 in current minute. Total chunks processed: 41\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 42\n",
            "Processed chunk 2 in current minute. Total chunks processed: 43\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 44\n",
            "Processed chunk 2 in current minute. Total chunks processed: 45\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 46\n",
            "Processed chunk 2 in current minute. Total chunks processed: 47\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 48\n",
            "Processed chunk 2 in current minute. Total chunks processed: 49\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 50\n",
            "Processed chunk 2 in current minute. Total chunks processed: 51\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 52\n",
            "Processed chunk 2 in current minute. Total chunks processed: 53\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 54\n",
            "Processed chunk 2 in current minute. Total chunks processed: 55\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 56\n",
            "Processed chunk 2 in current minute. Total chunks processed: 57\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 58\n",
            "Processed chunk 2 in current minute. Total chunks processed: 59\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 60\n",
            "Processed chunk 2 in current minute. Total chunks processed: 61\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 62\n",
            "Processed chunk 2 in current minute. Total chunks processed: 63\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 64\n",
            "Processed chunk 2 in current minute. Total chunks processed: 65\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 66\n",
            "Processed chunk 2 in current minute. Total chunks processed: 67\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 68\n",
            "Processed chunk 2 in current minute. Total chunks processed: 69\n",
            "Processed chunk 3 in current minute. Total chunks processed: 70\n",
            "Processed chunk 4 in current minute. Total chunks processed: 71\n",
            "Processed chunk 5 in current minute. Total chunks processed: 72\n",
            "Made 5 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 73\n",
            "Processed chunk 2 in current minute. Total chunks processed: 74\n",
            "Processed chunk 3 in current minute. Total chunks processed: 75\n",
            "Made 3 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 76\n",
            "Processed chunk 2 in current minute. Total chunks processed: 77\n",
            "Processed chunk 3 in current minute. Total chunks processed: 78\n",
            "Processed chunk 4 in current minute. Total chunks processed: 79\n",
            "Processed chunk 5 in current minute. Total chunks processed: 80\n",
            "Processed chunk 6 in current minute. Total chunks processed: 81\n",
            "Made 6 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 82\n",
            "Processed chunk 2 in current minute. Total chunks processed: 83\n",
            "Processed chunk 3 in current minute. Total chunks processed: 84\n",
            "Processed chunk 4 in current minute. Total chunks processed: 85\n",
            "Made 4 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 86\n",
            "Processed chunk 2 in current minute. Total chunks processed: 87\n",
            "Processed chunk 3 in current minute. Total chunks processed: 88\n",
            "Made 3 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 89\n",
            "Processed chunk 2 in current minute. Total chunks processed: 90\n",
            "Processed chunk 3 in current minute. Total chunks processed: 91\n",
            "Made 3 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 92\n",
            "Processed chunk 2 in current minute. Total chunks processed: 93\n",
            "Processed chunk 3 in current minute. Total chunks processed: 94\n",
            "Made 3 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 95\n",
            "Processed chunk 2 in current minute. Total chunks processed: 96\n",
            "Processed chunk 3 in current minute. Total chunks processed: 97\n",
            "Made 3 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 98\n",
            "Processed chunk 2 in current minute. Total chunks processed: 99\n",
            "Processed chunk 3 in current minute. Total chunks processed: 100\n",
            "Made 3 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 101\n",
            "Processed chunk 2 in current minute. Total chunks processed: 102\n",
            "Processed chunk 3 in current minute. Total chunks processed: 103\n",
            "Made 3 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 104\n",
            "Processed chunk 2 in current minute. Total chunks processed: 105\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 106\n",
            "Processed chunk 2 in current minute. Total chunks processed: 107\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 108\n",
            "Processed chunk 2 in current minute. Total chunks processed: 109\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 110\n",
            "Processed chunk 2 in current minute. Total chunks processed: 111\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 112\n",
            "Processed chunk 2 in current minute. Total chunks processed: 113\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 114\n",
            "Processed chunk 2 in current minute. Total chunks processed: 115\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 116\n",
            "Processed chunk 2 in current minute. Total chunks processed: 117\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 118\n",
            "Processed chunk 2 in current minute. Total chunks processed: 119\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 120\n",
            "Processed chunk 2 in current minute. Total chunks processed: 121\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 122\n",
            "Processed chunk 2 in current minute. Total chunks processed: 123\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 124\n",
            "Processed chunk 2 in current minute. Total chunks processed: 125\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 126\n",
            "Processed chunk 2 in current minute. Total chunks processed: 127\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 128\n",
            "Processed chunk 2 in current minute. Total chunks processed: 129\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 130\n",
            "Processed chunk 2 in current minute. Total chunks processed: 131\n",
            "Made 2 calls in the last minute\n",
            "Processed chunk 1 in current minute. Total chunks processed: 132\n",
            "Processed chunk 2 in current minute. Total chunks processed: 133\n",
            "Made 2 calls in the last minute\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-90b-text-preview` in organization `org_01j1ce2962fx1rw1x30eavn9h5` on : Limit 500000, Used 500530, Requested 3799. Please try again in 12m28.17s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-57ee8f9768a1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Make the API call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chunk\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5352\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5353\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5354\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5355\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5356\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_groq/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         }\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         )\n\u001b[0;32m-> 1261\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    954\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.2-90b-text-preview` in organization `org_01j1ce2962fx1rw1x30eavn9h5` on : Limit 500000, Used 500530, Requested 3799. Please try again in 12m28.17s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize counters\n",
        "calls_per_minute = 0\n",
        "last_reset_time = time.time()\n",
        "\n",
        "for doc in docs_processed:\n",
        "    for chunk in doc.chunks:\n",
        "        current_time = time.time()\n",
        "\n",
        "        # Check if a minute has passed since last reset\n",
        "        if current_time - last_reset_time >= 60:\n",
        "            print(f\"Made {calls_per_minute} calls in the last minute\")\n",
        "            calls_per_minute = 0\n",
        "            last_reset_time = current_time\n",
        "        else:\n",
        "            # If we're still within the same minute and hit rate limit\n",
        "            if calls_per_minute >= 30:  # Assuming 30 calls per minute limit\n",
        "                wait_time = 60 - (current_time - last_reset_time)\n",
        "                print(f\"Rate limit reached. Waiting {wait_time:.2f} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "                calls_per_minute = 0\n",
        "                last_reset_time = time.time()\n",
        "\n",
        "        # Make the API call\n",
        "        context: Context = context_chain.invoke({\"document\": doc.document.page_content, \"chunk\": chunk})\n",
        "        doc.context = context.context\n",
        "\n",
        "        # Increment counter\n",
        "        calls_per_minute += 1\n",
        "\n",
        "        # Optional: print progress\n",
        "        print(f\"Processed chunk {calls_per_minute} in current minute. Total chunks processed: {sum(len(d.chunks) for d in docs_processed[:docs_processed.index(doc)]) + len(doc.chunks[:doc.chunks.index(chunk) + 1])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiCoPuZOEg48"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Create lists to store the data\n",
        "chunk_texts = []\n",
        "document_texts = []\n",
        "contexts = []\n",
        "\n",
        "# Extract data from docs_processed\n",
        "for doc in docs_processed:\n",
        "    for chunk in doc.chunks:\n",
        "        chunk_texts.append(chunk.text)\n",
        "        contexts.append(chunk.context)\n",
        "        document_texts.append(doc.document.page_content)\n",
        "\n",
        "# Create dictionary for dataset\n",
        "dataset_dict = {\n",
        "    'chunk': chunk_texts,\n",
        "    'document': document_texts,\n",
        "    'context': contexts\n",
        "}\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "hf_token = userdata.get(\"HuggingFace\")\n",
        "if not hf_token:\n",
        "  # Login to Hugging Face (you'll need your token)\n",
        "  hf_token = input(\"Please enter your Hugging Face token: \")\n",
        "login(hf_token)\n",
        "\n",
        "# Push to Hugging Face Hub\n",
        "dataset.push_to_hub(\n",
        "    f\"AIEnthusiast369/hf_doc_qa_eval_chunk_size_{chunk_size}_llama_3_2_90b\",  # Replace with your username and desired dataset name\n",
        "    private=False  # Set to False if you want it public\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdmYQMFReFlR"
      },
      "source": [
        "## Save processed documents to file\n",
        "## Downloading processed documents in case notebook times out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivDZpzZMeFlR"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def save_download_object(object, filename):\n",
        "    joblib.dump(object, filename)\n",
        "    print(f\"Saved object to {filename}\")\n",
        "    files.download(filename)\n",
        "    print(f\"Downloaded {filename}\")\n",
        "\n",
        "def create_timestamp() -> str:\n",
        "    return datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def create_filename_timestamp(filename, extension = \"joblib\") -> str:\n",
        "    timestamp = create_timestamp()\n",
        "    return f\"{filename}_{timestamp}.{extension}\"\n",
        "\n",
        "def load_bm25_model(filename):\n",
        "    try:\n",
        "        return joblib.load(filename)\n",
        "    except (FileNotFoundError, OSError):\n",
        "        return None\n",
        "\n",
        "def get_latest_bm25_file():\n",
        "    # Look for files matching the pattern bm25_*.joblib\n",
        "    files = glob.glob(\"bm25_*.joblib\")\n",
        "    if not files:\n",
        "        return None\n",
        "    # Return the most recent file\n",
        "    return max(files, key=os.path.getctime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-GbvORIeFlR"
      },
      "outputs": [],
      "source": [
        "# Create filename with timestamp\n",
        "bm25_filename = create_filename_timestamp(\"docs_processed\")\n",
        "\n",
        "# Save the processed documents\n",
        "save_download_object(docs_processed, bm25_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IsJEauoKkxp"
      },
      "outputs": [],
      "source": [
        "# prompt: print chunks from docs_processed where context has value\n",
        "\n",
        "for doc in docs_processed:\n",
        "  for chunk in doc.chunks:\n",
        "    if chunk.context:\n",
        "      print(f\"chunk with context: Context: \\n\\n {chunk.context} \\n\\n Chunk: {chunk.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-rCNTJ7eFlW"
      },
      "outputs": [],
      "source": [
        "# Create list of chunks with their contexts\n",
        "chunks_with_context = []\n",
        "for doc in docs_processed:\n",
        "    for chunk in doc.chunks:\n",
        "        if chunk.context:  # Only include chunks that have a context\n",
        "            chunks_with_context.append(\n",
        "              f\"{chunk.text} \\n\\n {chunk.context}\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i-taFomMI7W"
      },
      "outputs": [],
      "source": [
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "if not pinecone_api_key:\n",
        "  pinecone_api_key = input(\"Please enter your PINECONE API KEY: \")\n",
        "\n",
        "pinecone_env = userdata.get(\"PINECONE_ENV\")\n",
        "if not pinecone_env:\n",
        "  pinecone_env = input(\"Please enter your PINECONE ENV: \")\n",
        "\n",
        "SPARSE_INDEX_NAME: str = \"sparse_index\"\n",
        "EMBEDDING_INDEX_NAME: str = \"embedding_index\"\n",
        "EMBEDING_MODEL = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(EMBEDING_MODEL)\n",
        "\n",
        "pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)\n",
        "\n",
        "def create_bm25(chunks: list[str]):\n",
        "    # Try to load existing BM25 model\n",
        "    latest_bm25_file = get_latest_bm25_file()\n",
        "    if latest_bm25_file:\n",
        "        bm25 = load_bm25_model(latest_bm25_file)\n",
        "        if bm25 is not None:\n",
        "            print(f\"Loaded existing BM25 model from {latest_bm25_file}\")\n",
        "            return bm25\n",
        "\n",
        "    # If no existing model found or loading failed, create a new one\n",
        "    print(\"Creating new BM25 model...\")\n",
        "    tokenized_chunks = [nltk.word_tokenize(chunk) for chunk in chunks]\n",
        "    bm25 = BM25Okapi(tokenized_chunks)\n",
        "\n",
        "    # Save the new model\n",
        "    bm25_filename = create_filename_timestamp(\"bm25\")\n",
        "    save_download_object(bm25, bm25_filename)\n",
        "\n",
        "    return bm25\n",
        "\n",
        "def create_pinecone_indexes(pinecone, embedding_model, bm25, chunks: list[str]):\n",
        "\n",
        "    # Create Pinecone indexes for TF-IDF and Embeddings\n",
        "    max_seq_length = model.max_seq_length\n",
        "\n",
        "    if SPARSE_INDEX_NAME not in pinecone.list_indexes():\n",
        "        pinecone.create_index(SPARSE_INDEX_NAME, dimension=len(bm25.), metric=\"cosine\")\n",
        "        time.sleep(1) # giving time to pinecode to create the index\n",
        "\n",
        "    sparse_index = pinecone.Index(SPARSE_INDEX_NAME)\n",
        "    if EMBEDDING_INDEX_NAME not in pinecone.list_indexes():\n",
        "        pinecone.create_index(EMBEDDING_INDEX_NAME, dimension=max_seq_length, metric=\"cosine\")\n",
        "        time.sleep(1) # giving time to pinecode to create the index\n",
        "\n",
        "    # Connect to Pinecone indexes\n",
        "    sparse_index = pinecone.Index(SPARSE_INDEX_NAME)\n",
        "    embedding_index = pinecone.Index(EMBEDDING_INDEX_NAME)\n",
        "\n",
        "    # Store Vectors/Embeddings in Pinecone with Metadata (Chunk Text)\n",
        "    # Store BM25 vectors\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        bm25_scores = bm25.get_scores(nltk.word_tokenize(chunk))\n",
        "        sparse_index.upsert([(str(i), bm25_scores.tolist(), {\"text\": chunk})])\n",
        "\n",
        "    # Semantic Embeddings using a Pre-trained Transformer Model\n",
        "    embeddings = embedding_model.encode(chunks, convert_to_tensor=False)\n",
        "    # Store embeddings in Pinecone\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "        embedding_index.upsert([(str(i), embedding, {\"text\": chunk})])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqqB_g9TeFlW"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "def fusion_rank_search(\n",
        "    query: str,\n",
        "    bm25,\n",
        "    chunks: list[str],\n",
        "    model,\n",
        "    embedding_index,\n",
        "    reranker_model: CrossEncoder = None,\n",
        "    k: int = 5,\n",
        "    weight_sparse: float = 0.5,\n",
        "    reranker_cutoff: int = 20  # Number of top results to rerank\n",
        "):\n",
        "    # Get BM25 results\n",
        "    tokenized_query = nltk.word_tokenize(query)\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "    bm25_top_indices = np.argsort(bm25_scores)[::-1][:reranker_cutoff]\n",
        "\n",
        "    # Normalize BM25 scores using min-max normalization\n",
        "    bm25_scores_norm = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))\n",
        "    bm25_results = [\n",
        "        {\n",
        "            'id': str(i),\n",
        "            'score': bm25_scores_norm[i],\n",
        "            'metadata': {'text': chunks[i]}\n",
        "        }\n",
        "        for i in bm25_top_indices\n",
        "    ]\n",
        "\n",
        "    # Get embedding results\n",
        "    query_embedding = model.encode(query, convert_to_tensor=False).tolist()\n",
        "    embedding_results = embedding_index.query(query_embedding, top_k=reranker_cutoff, include_metadata=True)\n",
        "\n",
        "    # Extract and normalize embedding scores\n",
        "    dense_scores = np.array([match['score'] for match in embedding_results['matches']])\n",
        "    dense_scores_norm = (dense_scores - np.min(dense_scores)) / (np.max(dense_scores) - np.min(dense_scores))\n",
        "\n",
        "    # Create dictionaries to store normalized scores\n",
        "    fusion_scores = defaultdict(lambda: {'sparse': 0.0, 'dense': 0.0, 'text': ''})\n",
        "\n",
        "    # Store normalized BM25 scores\n",
        "    for result in bm25_results:\n",
        "        doc_id = result['id']\n",
        "        fusion_scores[doc_id]['sparse'] = result['score']\n",
        "        fusion_scores[doc_id]['text'] = result['metadata']['text']\n",
        "\n",
        "    # Store normalized embedding scores\n",
        "    for match, norm_score in zip(embedding_results['matches'], dense_scores_norm):\n",
        "        doc_id = match['id']\n",
        "        fusion_scores[doc_id]['dense'] = norm_score\n",
        "        fusion_scores[doc_id]['text'] = match['metadata']['text']\n",
        "\n",
        "    # Combine scores using weighted average\n",
        "    weight_dense = 1.0 - weight_sparse\n",
        "    initial_results = [\n",
        "        {\n",
        "            'id': doc_id,\n",
        "            'score': (\n",
        "                weight_sparse * scores['sparse'] +\n",
        "                weight_dense * scores['dense']\n",
        "            ),\n",
        "            'metadata': {\n",
        "                'text': scores['text'],\n",
        "                'sparse_score': scores['sparse'],\n",
        "                'dense_score': scores['dense']\n",
        "            }\n",
        "        }\n",
        "        for doc_id, scores in fusion_scores.items()\n",
        "    ]\n",
        "\n",
        "    # Sort by combined score\n",
        "    initial_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "    initial_results = initial_results[:reranker_cutoff]\n",
        "\n",
        "    # Apply reranking if reranker model is provided\n",
        "    if reranker_model is not None:\n",
        "        # Prepare pairs for reranking\n",
        "        pairs = [(query, result['metadata']['text']) for result in initial_results]\n",
        "\n",
        "        # Get reranker scores - use them directly for final ranking\n",
        "        rerank_scores = reranker_model.predict(pairs)\n",
        "\n",
        "        # Update results with reranker scores\n",
        "        for result, rerank_score in zip(initial_results, rerank_scores):\n",
        "            result['metadata']['rerank_score'] = float(rerank_score)\n",
        "            # Use reranker score as the final score\n",
        "            result['score'] = float(rerank_score)\n",
        "\n",
        "        # Resort based on reranker scores\n",
        "        initial_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    return initial_results[:k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE0B0-JweFlW"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_rag_system(\n",
        "    best_answers_df: pd.DataFrame,\n",
        "    bm25,\n",
        "    chunks: list[str],\n",
        "    embedding_model,\n",
        "    embedding_index,\n",
        "    llm_chain,\n",
        "    n_samples: int = None  # Optional: limit number of samples for testing\n",
        "):\n",
        "    # Initialize ROUGE scorer\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Initialize results storage\n",
        "    results = []\n",
        "\n",
        "    # Get subset of dataframe if n_samples is specified\n",
        "    eval_df = best_answers_df.head(n_samples) if n_samples else best_answers_df\n",
        "\n",
        "    # Iterate through questions and answers\n",
        "    for idx, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc=\"Evaluating Questions\"):\n",
        "        query = row['question']\n",
        "        reference_answer = row['answer']\n",
        "\n",
        "        try:\n",
        "            # Get relevant context using fusion ranking\n",
        "            retrieved_results = fusion_rank_search(\n",
        "                query=query,\n",
        "                bm25=bm25,\n",
        "                chunks=chunks,\n",
        "                model=embedding_model,\n",
        "                embedding_index=embedding_index,\n",
        "                k=20\n",
        "            )\n",
        "\n",
        "            # Prepare context for LLM\n",
        "            context = \"\\n\".join([res['metadata']['text'] for res in retrieved_results])\n",
        "\n",
        "            # Generate answer using LLM\n",
        "            llm_response = llm_chain.invoke({\n",
        "                \"context\": context,\n",
        "                \"question\": query\n",
        "            })\n",
        "            generated_answer = llm_response.content if hasattr(llm_response, 'content') else llm_response\n",
        "\n",
        "            # Calculate BLEU score\n",
        "            reference_tokens = [reference_answer.split()]\n",
        "            candidate_tokens = generated_answer.split()\n",
        "            bleu_score = sentence_bleu(reference_tokens, candidate_tokens)\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            rouge_scores = rouge_scorer_instance.score(reference_answer, generated_answer)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'question': query,\n",
        "                'reference_answer': reference_answer,\n",
        "                'generated_answer': generated_answer,\n",
        "                'bleu_score': bleu_score,\n",
        "                'rouge1_f1': rouge_scores['rouge1'].fmeasure,\n",
        "                'rouge2_f1': rouge_scores['rouge2'].fmeasure,\n",
        "                'rougeL_f1': rouge_scores['rougeL'].fmeasure,\n",
        "                'retrieved_contexts': [res['metadata']['text'] for res in retrieved_results],\n",
        "                'context_scores': [res['score'] for res in retrieved_results]\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question {idx}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Calculate and print average scores\n",
        "    avg_scores = {\n",
        "        'Average BLEU': results_df['bleu_score'].mean(),\n",
        "        'Average ROUGE-1': results_df['rouge1_f1'].mean(),\n",
        "        'Average ROUGE-2': results_df['rouge2_f1'].mean(),\n",
        "        'Average ROUGE-L': results_df['rouge2_f1'].mean()\n",
        "    }\n",
        "\n",
        "    return results_df, avg_scores\n",
        "\n",
        "# Example usage:\n",
        "def print_evaluation_results(results_df, avg_scores):\n",
        "    print(\"\\nAverage Scores:\")\n",
        "    for metric, score in avg_scores.items():\n",
        "        print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "    print(\"\\nDetailed Results Sample (first 3):\")\n",
        "    for idx, row in results_df.head(3).iterrows():\n",
        "        print(\"\\nQuestion:\", row['question'])\n",
        "        print(\"Reference Answer:\", row['reference_answer'])\n",
        "        print(\"Generated Answer:\", row['generated_answer'])\n",
        "        print(f\"BLEU Score: {row['bleu_score']:.4f}\")\n",
        "        print(f\"ROUGE-1 F1: {row['rouge1_f1']:.4f}\")\n",
        "        print(f\"ROUGE-2 F1: {row['rouge2_f1']:.4f}\")\n",
        "        print(f\"ROUGE-L F1: {row['rougeL_f1']:.4f}\")\n",
        "        print(\"\\nRetrieved Contexts:\")\n",
        "        for context, score in zip(row['retrieved_contexts'], row['context_scores']):\n",
        "            print(f\"Score: {score:.4f}\")\n",
        "            print(f\"Context: {context[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHONc1uFeFlX"
      },
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "results_df, avg_scores = evaluate_rag_system(\n",
        "    best_answers_df=best_answers_df,\n",
        "    bm25=bm25,\n",
        "    chunks=chunks,\n",
        "    model=embedding_model,\n",
        "    embedding_index=embedding_index,\n",
        "    llm_chain=llm_chain,\n",
        "    n_samples=10  # Optional: start with a small sample for testing\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print_evaluation_results(results_df, avg_scores)\n",
        "\n",
        "# Save results to CSV (optional)\n",
        "results_df.to_csv('rag_evaluation_results.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8056bab4785e425f8556147a8f90d7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f45a15713e54dbda25dd0231581a58f",
              "IPY_MODEL_0e581db8782b4322909118cd7934a853",
              "IPY_MODEL_731076d6ad234d988b97536832899098"
            ],
            "layout": "IPY_MODEL_827176e3e8c540bc9158b4b24de100b1"
          }
        },
        "5f45a15713e54dbda25dd0231581a58f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e229a6919e44fa7b093759655102985",
            "placeholder": "​",
            "style": "IPY_MODEL_a2a41b32407f464aa506f7eea25ddf96",
            "value": "README.md: 100%"
          }
        },
        "0e581db8782b4322909118cd7934a853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f045da443a3540d78753cc0813384e19",
            "max": 893,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da47b2c8d6f3456ca8a10fc88f95a419",
            "value": 893
          }
        },
        "731076d6ad234d988b97536832899098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dff4bd0b03a4855b8142a192f2f8caa",
            "placeholder": "​",
            "style": "IPY_MODEL_60d4f4b6769c40189d072a05c4a1ee94",
            "value": " 893/893 [00:00&lt;00:00, 18.2kB/s]"
          }
        },
        "827176e3e8c540bc9158b4b24de100b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e229a6919e44fa7b093759655102985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a41b32407f464aa506f7eea25ddf96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f045da443a3540d78753cc0813384e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da47b2c8d6f3456ca8a10fc88f95a419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1dff4bd0b03a4855b8142a192f2f8caa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60d4f4b6769c40189d072a05c4a1ee94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87a31898206e40188e569879d70289ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4e976ed60cb4046b1266bb3b5cb2ea8",
              "IPY_MODEL_1cb677f600d5417fa673aaf8cb2cbaca",
              "IPY_MODEL_77e6947746384fd7bbb13eb8f7874bd3"
            ],
            "layout": "IPY_MODEL_536f16f75e904f8b88cb1da3563da37c"
          }
        },
        "d4e976ed60cb4046b1266bb3b5cb2ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d29952ed432541fbb755a2a67dc6c4fb",
            "placeholder": "​",
            "style": "IPY_MODEL_5842b04e603047c1a0f1e3aa8927915a",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "1cb677f600d5417fa673aaf8cb2cbaca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1005f949f24a4dd398646fc88fdef0f3",
            "max": 289016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a573f6b3367f4dc280b91752f3b0c5e7",
            "value": 289016
          }
        },
        "77e6947746384fd7bbb13eb8f7874bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a84419ba5d1e4379b91de8c9527f32bc",
            "placeholder": "​",
            "style": "IPY_MODEL_1a2337aa8aac44a2b1fece87289d9784",
            "value": " 289k/289k [00:00&lt;00:00, 5.19MB/s]"
          }
        },
        "536f16f75e904f8b88cb1da3563da37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d29952ed432541fbb755a2a67dc6c4fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5842b04e603047c1a0f1e3aa8927915a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1005f949f24a4dd398646fc88fdef0f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a573f6b3367f4dc280b91752f3b0c5e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a84419ba5d1e4379b91de8c9527f32bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a2337aa8aac44a2b1fece87289d9784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79a35a3156534e7b9e155b96ca0313d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb8babd540c64fb9b4638ff5c7ffa35b",
              "IPY_MODEL_2074667885f3477aa58c562bb9f30ebb",
              "IPY_MODEL_6d2012f40551430b993899e4836f3307"
            ],
            "layout": "IPY_MODEL_010bdb492c7845c2948baa14080a19c3"
          }
        },
        "fb8babd540c64fb9b4638ff5c7ffa35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fa8e93c28d24a4397fbc832422b5bf5",
            "placeholder": "​",
            "style": "IPY_MODEL_0f5a529098b14d6eab10823e0685b19a",
            "value": "Generating train split: 100%"
          }
        },
        "2074667885f3477aa58c562bb9f30ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76583f5ae0b0413eb701f6f416ff72de",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32669220fd824dbeb68fd6228a5d1438",
            "value": 65
          }
        },
        "6d2012f40551430b993899e4836f3307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37ee0dece8c14c379ed2b5aa499daa47",
            "placeholder": "​",
            "style": "IPY_MODEL_dd0e104a17c1443387f5b059096a0592",
            "value": " 65/65 [00:00&lt;00:00, 846.23 examples/s]"
          }
        },
        "010bdb492c7845c2948baa14080a19c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fa8e93c28d24a4397fbc832422b5bf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5a529098b14d6eab10823e0685b19a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76583f5ae0b0413eb701f6f416ff72de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32669220fd824dbeb68fd6228a5d1438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37ee0dece8c14c379ed2b5aa499daa47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd0e104a17c1443387f5b059096a0592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71cd06bd22af41d6900ab5d2b99e4e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d838c58e48464d5cb0ebb5f89be8ba09",
              "IPY_MODEL_735873eafa104bc5b831f9690d018cb0",
              "IPY_MODEL_6e08d98f07c14660b079c46cdd9b0411"
            ],
            "layout": "IPY_MODEL_bb83c6b7ef98497c83587ee869af72f4"
          }
        },
        "d838c58e48464d5cb0ebb5f89be8ba09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdbdcad46e474b2eb4ee628816e6d5b7",
            "placeholder": "​",
            "style": "IPY_MODEL_55207ca74d354853a2c9aa1bf0276cae",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "735873eafa104bc5b831f9690d018cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1b2c9c6745f4e23b3ad952bc508f228",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8acac0ad0f844511b101579f35f83a75",
            "value": 350
          }
        },
        "6e08d98f07c14660b079c46cdd9b0411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_423b2d0f15974a2a912a71c10e5c604b",
            "placeholder": "​",
            "style": "IPY_MODEL_3e6f448d710a4982955cdf0ced1ebcda",
            "value": " 350/350 [00:00&lt;00:00, 17.7kB/s]"
          }
        },
        "bb83c6b7ef98497c83587ee869af72f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdbdcad46e474b2eb4ee628816e6d5b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55207ca74d354853a2c9aa1bf0276cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1b2c9c6745f4e23b3ad952bc508f228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8acac0ad0f844511b101579f35f83a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "423b2d0f15974a2a912a71c10e5c604b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e6f448d710a4982955cdf0ced1ebcda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17b0453ae4354f5bbb6f5ba545b4960e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c2e8b12704643c1ae783a0542eaf6f0",
              "IPY_MODEL_7d69dace12ee435dbd4947c036838393",
              "IPY_MODEL_ecd41a6fb4874404aac1358c95d32461"
            ],
            "layout": "IPY_MODEL_acb3ab97078d4052b54c8ae03fb700c4"
          }
        },
        "3c2e8b12704643c1ae783a0542eaf6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9574d761501402d9272b236ac29cadd",
            "placeholder": "​",
            "style": "IPY_MODEL_10bce8f67bab4d9590d98a5813fbb9bb",
            "value": "vocab.txt: 100%"
          }
        },
        "7d69dace12ee435dbd4947c036838393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1100d6e47288431ab46ea183c9883ab2",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e79735c37f148bfa75eef2656627ab0",
            "value": 231508
          }
        },
        "ecd41a6fb4874404aac1358c95d32461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a429c8575284b3687bfc2cda2752663",
            "placeholder": "​",
            "style": "IPY_MODEL_f75d1d3989184a04baeed8a4a6ac9f50",
            "value": " 232k/232k [00:00&lt;00:00, 7.37MB/s]"
          }
        },
        "acb3ab97078d4052b54c8ae03fb700c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9574d761501402d9272b236ac29cadd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10bce8f67bab4d9590d98a5813fbb9bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1100d6e47288431ab46ea183c9883ab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e79735c37f148bfa75eef2656627ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a429c8575284b3687bfc2cda2752663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f75d1d3989184a04baeed8a4a6ac9f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02580fea91454497b1a9b2b77d91e764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90408b417502430c840d8c16463a5530",
              "IPY_MODEL_5af3f6aa921445beb9420901af8016f8",
              "IPY_MODEL_e98b5ad42df742828afe1a713c4e6a54"
            ],
            "layout": "IPY_MODEL_af0e3d20839444b88c8ff72097dad79a"
          }
        },
        "90408b417502430c840d8c16463a5530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0550c3be4feb46e5a2725cff757ee071",
            "placeholder": "​",
            "style": "IPY_MODEL_44347cb1c85e485b852b70bdd28cbf85",
            "value": "tokenizer.json: 100%"
          }
        },
        "5af3f6aa921445beb9420901af8016f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f8fad5e36d4eb79f40ab3e027f1ca2",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a36a391789684b63a3926881587543f0",
            "value": 466247
          }
        },
        "e98b5ad42df742828afe1a713c4e6a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3365d2129380429e85da803739b941df",
            "placeholder": "​",
            "style": "IPY_MODEL_5d0f1c9b1b0f4f45b47c9177c931ed3d",
            "value": " 466k/466k [00:00&lt;00:00, 7.04MB/s]"
          }
        },
        "af0e3d20839444b88c8ff72097dad79a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0550c3be4feb46e5a2725cff757ee071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44347cb1c85e485b852b70bdd28cbf85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2f8fad5e36d4eb79f40ab3e027f1ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a36a391789684b63a3926881587543f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3365d2129380429e85da803739b941df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d0f1c9b1b0f4f45b47c9177c931ed3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbc7f68fb2254c44b62e65cfb8f0818e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5191b093fdd4476bbaceae133b015c79",
              "IPY_MODEL_0e32353981a44579ba56576acfecbad2",
              "IPY_MODEL_14c6876839f3484ab32719a882502024"
            ],
            "layout": "IPY_MODEL_a0e33fba1f0141438cf3c88871f47dbf"
          }
        },
        "5191b093fdd4476bbaceae133b015c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2cc2ab959094626b5ba49dc9d3f324a",
            "placeholder": "​",
            "style": "IPY_MODEL_302f0c825d0f404cbd99174a8e3ffb54",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "0e32353981a44579ba56576acfecbad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af310401122147ca92e77e68daf8278d",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c54d7cf9cbd4331933de9f0b49eadbe",
            "value": 112
          }
        },
        "14c6876839f3484ab32719a882502024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_379c9e0641da4e72853b199915cd228f",
            "placeholder": "​",
            "style": "IPY_MODEL_1a959f7eaee54783b1c7ee797d1b6d38",
            "value": " 112/112 [00:00&lt;00:00, 2.91kB/s]"
          }
        },
        "a0e33fba1f0141438cf3c88871f47dbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2cc2ab959094626b5ba49dc9d3f324a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "302f0c825d0f404cbd99174a8e3ffb54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af310401122147ca92e77e68daf8278d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c54d7cf9cbd4331933de9f0b49eadbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "379c9e0641da4e72853b199915cd228f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a959f7eaee54783b1c7ee797d1b6d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "922eaf620b53441c8a9713449cfcfda6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3cf31a533c34fc5a8bc7f5e88e00a68",
              "IPY_MODEL_618cc59ffe53437c8d4136db07ff053f",
              "IPY_MODEL_7e2cfc63e91d465fb1afe6717fbf3f65"
            ],
            "layout": "IPY_MODEL_1b24b6e8765043b5b5b1f68065fc44e9"
          }
        },
        "a3cf31a533c34fc5a8bc7f5e88e00a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd2d2b2627724b68b34b27bf180ed2d3",
            "placeholder": "​",
            "style": "IPY_MODEL_9a81f406468e45e5b29a1a45ee4cbf78",
            "value": "config.json: 100%"
          }
        },
        "618cc59ffe53437c8d4136db07ff053f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_378d464a1d9e4310a1fc65728c9c1139",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf179efbd9554bd8875a2fa7b1f5a01e",
            "value": 612
          }
        },
        "7e2cfc63e91d465fb1afe6717fbf3f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3331064bfae04daf87d1a711d4e11621",
            "placeholder": "​",
            "style": "IPY_MODEL_b134bc01a447487c99d1f7a4c683ba46",
            "value": " 612/612 [00:00&lt;00:00, 35.9kB/s]"
          }
        },
        "1b24b6e8765043b5b5b1f68065fc44e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd2d2b2627724b68b34b27bf180ed2d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a81f406468e45e5b29a1a45ee4cbf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "378d464a1d9e4310a1fc65728c9c1139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf179efbd9554bd8875a2fa7b1f5a01e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3331064bfae04daf87d1a711d4e11621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b134bc01a447487c99d1f7a4c683ba46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b60c62a3900944efa77b22b1da13b0f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdd3272dc63f430ea79fa1da3c16e673",
              "IPY_MODEL_08c3d4d0af77405690cc59383fa6d2ac",
              "IPY_MODEL_6f1d7db41e3f4e8a8b1c5f86f436ac42"
            ],
            "layout": "IPY_MODEL_d14a45bb63e240c986bd40567e0af264"
          }
        },
        "bdd3272dc63f430ea79fa1da3c16e673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aac3348bcf747d5a4ac373d6fdc70d6",
            "placeholder": "​",
            "style": "IPY_MODEL_2064f735a0d145cca6efada3f3957fbd",
            "value": "model.safetensors: 100%"
          }
        },
        "08c3d4d0af77405690cc59383fa6d2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc31b219676d47c8899d3653cb93cef8",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdc2c7d46955484a8224606be5a0de87",
            "value": 90868376
          }
        },
        "6f1d7db41e3f4e8a8b1c5f86f436ac42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c535c0fd698143f4afe964f548a8641d",
            "placeholder": "​",
            "style": "IPY_MODEL_ad9446bed66e42fab514a0be435f7f3c",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 139MB/s]"
          }
        },
        "d14a45bb63e240c986bd40567e0af264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aac3348bcf747d5a4ac373d6fdc70d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2064f735a0d145cca6efada3f3957fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc31b219676d47c8899d3653cb93cef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdc2c7d46955484a8224606be5a0de87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c535c0fd698143f4afe964f548a8641d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad9446bed66e42fab514a0be435f7f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}