question,reference_answer,generated_answer,retrieved_contexts,context_scores,bertscore_precision,bertscore_recall,bertscore_f1
"What architecture is the `tokenizers-linux-x64-musl` binary designed for?
",x86_64-unknown-linux-musl,"The provided context does not contain specific information regarding the architecture for which the `tokenizers-linux-x64-musl` binary is designed. Therefore, the answer based on the given context is: ""I do not have enough information to answer this question based on the provided context.""","['`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`', '*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units.\nEncoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from\nthe model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they\ncan process text in any language out of the box, they are more robust to noise, and they minimize technical debt by\nremoving complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token\nsequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of', '{:else}\n\n<CourseFloatingBanner chapter={2}\n  classNames=""absolute z-10 right-0 top-0""\n  notebooks={[\n    {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb""},\n    {label: ""Aws Studio"", value: ""https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb""},\n]} />\n\n{/if}\n\n<Youtube id=""VFp38yj8h3A""/>\n\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we\'ll explore exactly what happens in the tokenization pipeline.', 'Components\n\nWhen building a Tokenizer, you can attach various types of components to\nthis Tokenizer in order to customize its behavior. This page lists most\nprovided components.\n\n## Normalizers\n\nA `Normalizer` is in charge of pre-processing the input string in order\nto normalize it as relevant for a given use case. Some common examples\nof normalization are the Unicode normalization algorithms (NFD, NFKD,\nNFC & NFKC), lowercasing etc... The specificity of `tokenizers` is that\nwe keep track of the alignment while normalizing. This is essential to\nallow mapping from the generated tokens back to the input text.\n\nThe `Normalizer` is optional.', '## Models\n\nModels are the core algorithms used to actually tokenize, and therefore,\nthey are the only mandatory component of a Tokenizer.']","[-0.09746161103248596, -3.780998706817627, -4.594786643981934, -5.158089637756348, -6.356396198272705]",0.8254179358482361,0.8857962489128113,0.8545418977737427
"What is the purpose of the BLIP-Diffusion model?
",The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.,The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation in the context of text-to-image generation. It aims to overcome limitations such as lengthy fine-tuning and difficulties in preserving subject fidelity by introducing a new subject-driven image generation model that supports multimodal control through subject images and text prompts.,"['encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*', '# BLIP-Diffusion\n\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n\n\nThe abstract from the paper is:', '*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual', ""In Diffusers, we follow this philosophy for both pipelines and schedulers, but only partly for diffusion models. The reason we don't follow this design fully for diffusion models is because almost all diffusion pipelines, such\nas [DDPM](https://huggingface.co/docs/diffusers/api/pipelines/ddpm), [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview#stable-diffusion-pipelines), [unCLIP (DALLÂ·E 2)](https://huggingface.co/docs/diffusers/api/pipelines/unclip) and [Imagen](https://imagen.research.google/) all rely on the same diffusion model, the [UNet](https://huggingface.co/docs/diffusers/api/models/unet2d-cond)."", '</Tip>\n\n\n## BlipDiffusionPipeline\n[[autodoc]] BlipDiffusionPipeline\n    - all\n    - __call__\n\n## BlipDiffusionControlNetPipeline\n[[autodoc]] BlipDiffusionControlNetPipeline\n    - all\n    - __call__']","[3.429678201675415, 2.8743479251861572, 2.6945180892944336, -2.210517406463623, -4.32147741317749]",0.8697982430458069,0.9221949577331543,0.8952305912971497
"How can a user claim authorship of a paper on the Hugging Face Hub?
","By clicking their name on the corresponding Paper page and clicking ""claim authorship"", then confirming the request in paper settings for admin team validation.","To claim authorship of a paper on the Hugging Face Hub, a user can follow these steps as mentioned in the provided context:

""If your paper is not linked to your account, you can click in your name in the corresponding Paper page and click 'claim authorship'. This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.""

Therefore, to claim authorship of a paper on the Hugging Face Hub, a user needs to click on their name on the relevant Paper page and select the option to ""claim authorship."" This action will initiate a request process that, upon validation by the admin team, will verify the Paper page with the user as an author.","['## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email. \n\n<div class=""flex justify-center"">\n<img class=""block dark:hidden"" width=""300"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png""/>\n<img class=""hidden dark:block"" width=""300"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png""/>\n</div>\n\nIf your paper is not linked to your account, you can click in your name in the corresponding Paper page and click ""claim authorship"". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.', '## Linking a Paper to a model, dataset or Space\n\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository\'s tags. Clicking on the arxiv tag will let you:\n\n* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.\n\n<div class=""flex justify-center"">\n<img class=""block dark:hidden"" width=""300"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png""/>\n<img class=""hidden dark:block"" width=""300"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png""/>\n</div>\n\n## Claiming authorship to a Paper', ""### Do you support ACL anthology?\n\nWe're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future.\n\n### Can I have a Paper page even if I have no model/dataset/Space?\n\nYes. You can go to [the main Papers page](https://huggingface.co/papers), click search and write the name of the paper or the full Arxiv id. If the paper does not exist, you will get an option to index it. You can also just visit the page `hf.co/papers/xxxx.yyyyy` replacing with the arxiv id of the paper you wish to index."", ""Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing)."", '<div class=""flex justify-center"">\n<img class=""block dark:hidden"" width=""300"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png""/>\n<img class=""hidden dark:block"" width=""300"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png""/>\n</div>\n\n\n## Frequently Asked Questions\n\n### Can I control which Paper pages show in my profile?\n\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the ""Show on profile"" checkbox to hide/show it in your profile. \n\n### Do you support ACL anthology?']","[6.56227445602417, 2.9058516025543213, -2.2642574310302734, -2.4639511108398438, -3.200122833251953]",0.8483226895332336,0.9080013036727905,0.8771480321884155
"What is the purpose of the /healthcheck endpoint in the Datasets server API?
",Ensure the app is running,The purpose of the `/healthcheck` endpoint in the Datasets server API is to ensure that the application is running.,"['### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset', 'Datasets server API\n\n> API on ð¤ datasets\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server', '```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing a list of the dataset\'s splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n\n```json\n{\n  ""splits"": [\n    { ""dataset"": ""duorc"", ""config"": ""ParaphraseRC"", ""split"": ""train"" },\n    { ""dataset"": ""duorc"", ""config"": ""ParaphraseRC"", ""split"": ""validation"" },\n    { ""dataset"": ""duorc"", ""config"": ""ParaphraseRC"", ""split"": ""test"" },\n    { ""dataset"": ""duorc"", ""config"": ""SelfRC"", ""split"": ""train"" },\n    { ""dataset"": ""duorc"", ""config"": ""SelfRC"", ""split"": ""validation"" },\n    { ""dataset"": ""duorc"", ""config"": ""SelfRC"", ""split"": ""test"" }\n  ],\n  ""pending"": [],\n  ""failed"": []\n}\n```', 'The `/splits` endpoint accepts the dataset name as its query parameter:\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {""Authorization"": f""Bearer {API_TOKEN}""}\nAPI_URL = ""https://datasets-server.huggingface.co/splits?dataset=duorc""\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()', '```\n</python>\n<js>\n```js\nimport fetch from ""node-fetch"";\nasync function query(data) {\n    const response = await fetch(\n        ""https://datasets-server.huggingface.co/splits?dataset=duorc"",\n        {\n            headers: { Authorization: `Bearer ${API_TOKEN}` },\n            method: ""GET""\n        }\n    );\n    const result = await response.json();\n    return result;\n}\nquery().then((response) => {\n    console.log(JSON.stringify(response));\n});\n```\n</js>\n<curl>\n```curl\ncurl https://datasets-server.huggingface.co/splits?dataset=duorc \\\n        -X GET \\\n        -H ""Authorization: Bearer ${API_TOKEN}""']","[2.0400941371917725, -2.726766586303711, -3.783942937850952, -4.13497257232666, -4.499407768249512]",0.8300786018371582,0.8988667726516724,0.8631042242050171
"What is the default context window size for Local Attention in the LongT5 model?
",127 tokens,The default context window size for Local Attention in the LongT5 model is `r=127` tokens.,"['- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`', '- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`\ntokens to the left and right of it (with `r=127` by default). *Local Attention* does not introduce any new parameters\nto the model. The complexity of the mechanism is linear in input sequence length `l`: `O(l*r)`.\n- *Transient Global Attention* is an extension of the *Local Attention*. It, furthermore, allows each input token to\ninteract with all other tokens in the layer. This is achieved via splitting an input sequence into blocks of a fixed\nlength `k` (with a default `k=16`). Then, a global token for such a block is obtained via summing and normalizing the embeddings of every token', ""*Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the\nperformance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we\nexplore the effects of scaling both the input length and model size at the same time. Specifically, we integrated\nattention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training\n(PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global}\n(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are"", ""a few new parameters -- global relative position biases and a layer normalization for global token's embedding.\nThe complexity of this mechanism is `O(l(r + l/k))`.\n- An example showing how to evaluate a fine-tuned LongT5 model on the [pubmed dataset](https://huggingface.co/datasets/scientific_papers) is below."", ""(TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are\nable to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on\nquestion answering tasks.*""]","[1.8962746858596802, 1.0929834842681885, -3.5771865844726562, -4.63636589050293, -5.471528053283691]",0.7943817377090454,0.8669767379760742,0.8290932178497314
"What method is used to load a checkpoint for a task using `AutoPipeline`?
",from_pretrained(),The method used to load a checkpoint for a task using `AutoPipeline` is the `from_pretrained()` method. This method automatically retrieves the relevant pipeline given the name or path to the pretrained weights for the specific task.,"['# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow\n\nBased on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch', '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# AutoPipeline\n\n`AutoPipeline` is designed to:\n\n1. make it easy for you to load a checkpoint for a task without knowing the specific pipeline class to use\n2. use multiple pipelines in your workflow', '```\n\n<Tip>\n\nCheck out the [AutoPipeline](../../tutorials/autopipeline) tutorial to learn how to use this API!\n\n</Tip>\n\n`AutoPipeline` supports text-to-image, image-to-image, and inpainting for the following diffusion models:\n\n- [Stable Diffusion](./stable_diffusion/overview)\n- [ControlNet](./controlnet)\n- [Stable Diffusion XL (SDXL)](./stable_diffusion/stable_diffusion_xl)\n- [DeepFloyd IF](./deepfloyd_if)\n- [Kandinsky 2.1](./kandinsky)\n- [Kandinsky 2.2](./kandinsky_v22)\n\n\n## AutoPipelineForText2Image\n\n[[autodoc]] AutoPipelineForText2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting', '- Pipelines should be used **only** for inference.\n- Pipelines should be very readable, self-explanatory, and easy to tweak.\n- Pipelines should be designed to build on top of each other and be easy to integrate into higher-level APIs.\n- Pipelines are **not** intended to be feature-complete user interfaces. For future complete user interfaces one should rather have a look at [InvokeAI](https://github.com/invoke-ai/InvokeAI), [Diffuzers](https://github.com/abhishekkrthakur/diffuzers), and [lama-cleaner](https://github.com/Sanster/lama-cleaner).\n- Every pipeline should have one and only one way to run it via a `__call__` method. The naming of the `__call__` arguments should be shared across all pipelines.\n- Pipelines should be named after the task they are intended to solve.', '## AutoPipelineForImage2Image\n\n[[autodoc]] AutoPipelineForImage2Image\n\t- all\n\t- from_pretrained\n\t- from_pipe\n\n## AutoPipelineForInpainting\n\n[[autodoc]] AutoPipelineForInpainting\n\t- all\n\t- from_pretrained\n\t- from_pipe']","[3.114692211151123, 1.241852045059204, -2.426175117492676, -4.868200302124023, -5.202479839324951]",0.8128544092178345,0.9059427976608276,0.8568778038024902
"What is the purpose of Diffusers library?
",To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.,"The purpose of the Diffusers library is to provide pre-trained diffusion models and serve as a modular toolbox for inference and training. This is highlighted in the context: ""Diffusers provides pre-trained diffusion models and serves as a modular toolbox for inference and training.""","['- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.', '## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.\n\nGiven its real case applications in the world and potential negative impacts on society, we think it is important to provide the project with ethical guidelines to guide the development, usersâ contributions, and usage of the Diffusers library.', ""For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable."", '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ð§¨ Diffusersâ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.', ""After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.\n\nFeel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers!\n\nLet's start diffusing! ð§¨""]","[3.694605827331543, 3.418553352355957, 2.8632144927978516, -0.8381847739219666, -1.9123960733413696]",0.8879712820053101,0.8996834754943848,0.8937889933586121
"What method does the EulerAncestralDiscreteScheduler use for sampling?
",Ancestral sampling with Euler method steps.,The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps for sampling.,"['# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler\n\n## EulerAncestralDiscreteSchedulerOutput\n[[autodoc]] schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteSchedulerOutput', '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# EulerAncestralDiscreteScheduler', '- Every scheduler has to have a `set_num_inference_steps`, and a `step` function. `set_num_inference_steps(...)` has to be called before every denoising process, *i.e.* before `step(...)` is called.\n- Every scheduler exposes the timesteps to be ""looped over"" via a `timesteps` attribute, which is an array of timesteps the model will be called upon.\n- The `step(...)` function takes a predicted model output and the ""current"" sample (x_t) and returns the ""previous"", slightly more denoised sample (x_t-1).\n- Given the complexity of diffusion schedulers, the `step` function does not expose all the complexity and can be a bit of a ""black box"".\n- In almost all cases, novel schedulers shall be implemented in a new scheduling file.', 'The following design principles are followed:\n- All schedulers are found in [`src/diffusers/schedulers`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers).\n- Schedulers are **not** allowed to import from large utils files and shall be kept very self-contained.\n- One scheduler Python file corresponds to one scheduler algorithm (as might be defined in a paper).\n- If schedulers share similar functionalities, we can make use of the `#Copied from` mechanism.\n- Schedulers all inherit from `SchedulerMixin` and `ConfigMixin`.\n- Schedulers can be easily swapped out with the [`ConfigMixin.from_config`](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config) method as explained in detail [here](../using-diffusers/schedulers.md).', '### Schedulers\n\nSchedulers are responsible to guide the denoising process for inference as well as to define a noise schedule for training. They are designed as individual classes with loadable configuration files and strongly follow the **single-file policy**.']","[4.451257228851318, -3.3830010890960693, -4.155923843383789, -5.112043380737305, -8.24321174621582]",0.8697017431259155,0.9239312410354614,0.8959967494010925
"What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?
",IDEFICS,The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS.,"['[IDEFICS](../model_doc/idefics) is an open-access vision and language model based on [Flamingo](https://huggingface.co/papers/2204.14198), \na state-of-the-art visual language model initially developed by DeepMind. The model accepts arbitrary sequences of image \nand text inputs and generates coherent text as output. It can answer questions about images, describe visual content, \ncreate stories grounded in multiple images, and so on. IDEFICS comes in two variants - [80 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-80b) \nand [9 billion parameters](https://huggingface.co/HuggingFaceM4/idefics-9b), both of which are available on the ð¤ Hub. For each variant, you can also find fine-tuned instructed \nversions of the model adapted for conversational use cases.', '-->\n\n# Image tasks with IDEFICS\n\n[[open-in-colab]]\n\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach \nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. \nFor instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. \nThis approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can \nsolve image-text tasks with a large multimodal model called IDEFICS.', 'This model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However, \nbeing a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether \nthis approach suits your use case better than fine-tuning specialized models for each individual task.', 'Let\'s get a new image for this task: \n\n<div class=""flex justify-center"">\n     <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-vqa.jpg"" alt=""Image of a couple having a picnic""/>\n</div>\n\nPhoto by [Jarritos Mexican Soda](https://unsplash.com/@jarritos). \n\nYou can steer the model from image captioning to visual question answering by prompting it with appropriate instructions:', '- [`FlaxAlbertForMaskedLM`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb).\n- [Masked language modeling](https://huggingface.co/course/chapter7/3?fw=pt) chapter of the ð¤ Hugging Face Course.\n- Check the [Masked language modeling task guide](../tasks/masked_language_modeling) on how to use the model.']","[3.738790988922119, 3.5239651203155518, -2.9268381595611572, -5.565996170043945, -6.907344818115234]",0.8021101355552673,0.8491982221603394,0.8249828219413757
"What is the purpose of the `gradio.Blocks` API?
","The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.","The purpose of the `gradio.Blocks` API is to allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.","[""This simple example above introduces 4 concepts that underlie Blocks:\n\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\n<Tip>\nðIf you're not familiar with the `with` statement in Python, we recommend checking out the excellent [tutorial](https://realpython.com/python-with-statement/) from Real Python. Come back here after reading that ð¤\n</Tip>\nThe order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below)"", 'Introduction to Gradio Blocks[[introduction-to-gradio-blocks]]\n\n<CourseFloatingBanner chapter={9}\n  classNames=""absolute z-10 right-0 top-0""\n  notebooks={[\n    {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb""},\n    {label: ""Aws Studio"", value: ""https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section7.ipynb""},\n]} />\n\nIn the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`.\n\nNow, what\'s the difference between `Interface` and `Blocks`?', '```\n\n<iframe src=""https://course-demos-flip-text.hf.space"" frameBorder=""0"" height=""400"" title=""Gradio app"" class=""container p-0 flex-grow space-iframe"" allow=""accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"" sandbox=""allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads""></iframe>\n\nThis simple example above introduces 4 concepts that underlie Blocks:', 'We\'ve developed a **magic command** that will create and run a Blocks demo for you. To use this, load the gradio extension at the top of your notebook:\n\n`%load_ext gradio`\n\nThen, in the cell that you are developing your Gradio demo, simply write the magic command **`%%blocks`** at the top, and then write the layout and components like you would normally:\n\n```py\n%%blocks\n\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(f""# Greetings {args.name}!"")\n    inp = gr.Textbox()\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: x, inputs=inp, outputs=out)', '### 8. Which of the following are components included in the Gradio library?\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: ""Textbox."",\n\t\t\texplain: ""Yes, you can create textboxes with the Textbox component."",\n            correct: true\n        },\n        {\n\t\t\ttext: ""Graph."",\n\t\t\texplain: ""There is currently no Graph component."",\n        },\n        {\n\t\t\ttext: ""Image."",\n\t\t\texplain: ""Yes, you can create an image upload widget with the Image component."",\n            correct: true\n        },\n        {\n\t\t\ttext: ""Audio."",\n\t\t\texplain: ""Yes, you can create an audio upload widget with the Audio component."",\n            correct: true\n        },\n\t]}\n/>\n\n### 9. What does Gradio `Blocks` allow you to do?']","[1.7282040119171143, 0.27922919392585754, -1.4531080722808838, -1.5357975959777832, -2.8742878437042236]",0.8790749907493591,0.9078633785247803,0.8932372331619263
"What is the purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents""?
",The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.,"The purpose of the two-stage model proposed in the paper ""Hierarchical Text-Conditional Image Generation with CLIP Latents"" is to generate image representations using a prior that generates a CLIP image embedding given a text caption, and then use a decoder to generate an image conditioned on the image embedding. This approach aims to improve image diversity while maintaining photorealism and caption similarity.","['*Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a', ""# unCLIP\n\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in ð¤ Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo).\n\nThe abstract from the paper is following:"", 'its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.*', '## Conclusion\n\nCLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so youâll probably still want to fine-tune a different segmentation model if accuracy is important.', '<figure class=""image table text-center m-0 w-full"">\n  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Overview of the CLIPSeg model"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png""></medium-zoom>\n  <figcaption>Example of image classification using CLIP (<a href=""https://openai.com/blog/clip/"">source</a>).</figcaption>\n</figure>']","[3.206976890563965, -0.4805622696876526, -2.4826879501342773, -2.975679397583008, -4.233980178833008]",0.8988264799118042,0.9661158919334412,0.9312572479248047
"What command is used to install the requirements for a research project using ð¤ Transformers?
",pip install -r requirements.txt,"The command used to install the requirements for a research project using ð¤ Transformers is:

```bash
pip install -q bitsandbytes sentencepiece accelerate transformers
```","['# Research projects\n\nThis folder contains various research projects using ð¤ Transformers. They are not maintained and require a specific\nversion of ð¤ Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command', 'Add custom Dependencies\n\nInference Endpointsâ base image includes all required libraries to run inference on ð¤ Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).', 'Before you begin, make sure you have all the necessary libraries installed. \n\n```bash\npip install -q bitsandbytes sentencepiece accelerate transformers', '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâ ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# How ð¤ Transformers solve tasks', 'With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity']","[1.6791223287582397, -2.1248269081115723, -2.3956961631774902, -3.271254062652588, -7.084390163421631]",0.8065667748451233,0.8883365392684937,0.8454791307449341
"What task does the `roberta-large-mnli` checkpoint perform?
",Text classification,"The `roberta-large-mnli` checkpoint performs the task of text classification by classifying if two sentences are logically linked across three labels (contradiction, neutral, entailment) â a task also called natural language inference.","['First, though, let\'s test what you learned in this chapter!\n\n\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?\n\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: ""Summarization"",\n\t\t\texplain: ""Look again on the <a href=\\""https://huggingface.co/roberta-large-mnli\\"">roberta-large-mnli page</a>.""\n\t\t},\n\t\t{\n\t\t\ttext: ""Text classification"",\n\t\t\texplain: ""More precisely, it classifies if two sentences are logically linked across three labels (contradiction, neutral, entailment) â a task also called <em>natural language inference</em>."",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: ""Text generation"",\n\t\t\texplain: ""Look again on the <a href=\\""https://huggingface.co/roberta-large-mnli\\"">roberta-large-mnli page</a>.""\n\t\t}\n\t]}\n/>\n\n### 2. What will the following code return?', '!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=""absolute z-10 right-0 top-0""\n/>\n\nThis chapter covered a lot of ground! Don\'t worry if you didn\'t grasp all the details; the next chapters will help you understand how things work under the hood.\n\nFirst, though, let\'s test what you learned in this chapter!\n\n\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?', ""This model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources"", '```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)', ""NystrÃ¶mformer for Masked Language Modeling (MLM) is available on HuggingFace. Currently, there are 4 checkpoints, corresponding to various sequence lengths: [`nystromformer-512`](https://huggingface.co/uw-madison/nystromformer-512), [`nystromformer-1024`](https://huggingface.co/uw-madison/nystromformer-1024), [`nystromformer-2048`](https://huggingface.co/uw-madison/nystromformer-2048), and [`nystromformer-4096`](https://huggingface.co/uw-madison/nystromformer-4096). The number of landmarks, \\\\(m\\\\), can be controlled using the `num_landmarks` parameter in the [`NystromformerConfig`](https://huggingface.co/docs/transformers/v4.18.0/en/model_doc/nystromformer#transformers.NystromformerConfig). Let's take a look at a minimal example of NystrÃ¶mformer for MLM:""]","[4.744812965393066, 1.774989366531372, -1.6209709644317627, -6.542631149291992, -6.94545841217041]",0.7806244492530823,0.861201286315918,0.8189356327056885
"What service is replacing the Paid tier of the Inference API at Hugging Face?
",Inference Endpoints,"The service replacing the Paid tier of the Inference API at Hugging Face is the ""Inference Endpoints,"" as mentioned in the provided context: ""But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: Inference Endpoints.""","[""--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n\n# Introducing our new pricing\n\n\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n\nFirst of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints)."", ""Along with Inference Endpoints, we've recently introduced hardware upgrades for [Spaces](https://huggingface.co/spaces/launch), which allows running ML demos with the hardware of your choice. No subscription is required to use these services; you only need to add a credit card to your account from your [billing settings](https://huggingface.co/settings/billing). You can also attach a payment method to any of [your organizations](https://huggingface.co/settings/organizations)."", 'If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources\n\n* [Hugging Face Infinity](https://huggingface.co/infinity)\n* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i) \n* [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n* [ð¤ Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)', 'In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs.', 'Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources']","[1.924665927886963, -2.514176845550537, -3.005640983581543, -3.9285922050476074, -4.234669208526611]",0.7892374992370605,0.8052475452423096,0.7971621155738831
"What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?
",Grouped convolutions,"SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers.","[""â ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:"", 'highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with\nBERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods\nsuch as grouped convolutions have yielded significant speedups for computer vision networks, but many of these\ntechniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*', ""This model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources"", '## SqueezeBertTokenizerFast\n\n[[autodoc]] SqueezeBertTokenizerFast\n\n## SqueezeBertModel\n\n[[autodoc]] SqueezeBertModel\n\n## SqueezeBertForMaskedLM\n\n[[autodoc]] SqueezeBertForMaskedLM\n\n## SqueezeBertForSequenceClassification\n\n[[autodoc]] SqueezeBertForSequenceClassification\n\n## SqueezeBertForMultipleChoice\n\n[[autodoc]] SqueezeBertForMultipleChoice\n\n## SqueezeBertForTokenClassification\n\n[[autodoc]] SqueezeBertForTokenClassification\n\n## SqueezeBertForQuestionAnswering\n\n[[autodoc]] SqueezeBertForQuestionAnswering', '!--Copyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n\nâ ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# SqueezeBERT\n\n## Overview']","[5.897789001464844, 0.44092226028442383, -1.3245198726654053, -3.4286093711853027, -3.4763684272766113]",0.8148932456970215,0.8778588771820068,0.8452050089836121
"What type of license is the HuggingFace Team's software distributed under?
","Apache License, Version 2.0","The HuggingFace Team's software is distributed under the Apache License, Version 2.0. 

","['!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview', '!--Copyright 2022 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Under construction', '!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Research projects', '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# OpenVINO', 'Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n\n<img width=""150"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg"">\n\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\n\n## Contents\n\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)']","[6.19050931930542, 6.075645446777344, 5.961357593536377, 5.895658493041992, -1.4656929969787598]",0.874743640422821,0.9189367890357971,0.8962958455085754
"What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?
",Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.,"The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are:

1. Splitting the embedding matrix into two smaller matrices.
2. Using repeating layers split among groups. 

These techniques are mentioned in the provided context from the ALBERT model overview.","['## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:', '*Increasing model size when pretraining natural language representations often results in improved performance on\ndownstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations,\nlonger training times, and unexpected model degradation. To address these problems, we present two parameter-reduction\ntechniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows\nthat our proposed methods lead to models that scale much better compared to the original BERT. We also use a\nself-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks', '## AlbertModel\n\n[[autodoc]] AlbertModel\n    - forward\n\n## AlbertForPreTraining\n\n[[autodoc]] AlbertForPreTraining\n    - forward\n\n## AlbertForMaskedLM\n\n[[autodoc]] AlbertForMaskedLM\n    - forward\n\n## AlbertForSequenceClassification\n\n[[autodoc]] AlbertForSequenceClassification\n    - forward\n\n## AlbertForMultipleChoice\n\n[[autodoc]] AlbertForMultipleChoice\n\n## AlbertForTokenClassification\n\n[[autodoc]] AlbertForTokenClassification\n    - forward\n\n## AlbertForQuestionAnswering\n\n[[autodoc]] AlbertForQuestionAnswering\n    - forward\n\n</pt>\n\n<tf>\n\n## TFAlbertModel\n\n[[autodoc]] TFAlbertModel\n    - call\n\n## TFAlbertForPreTraining\n\n[[autodoc]] TFAlbertForPreTraining\n    - call\n\n## TFAlbertForMaskedLM\n\n[[autodoc]] TFAlbertForMaskedLM\n    - call\n\n## TFAlbertForSequenceClassification', '- Check the  [Multiple choice task guide](../tasks/multiple_choice) on how to use the model.\n\n\n## AlbertConfig\n\n[[autodoc]] AlbertConfig\n\n## AlbertTokenizer\n\n[[autodoc]] AlbertTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary\n\n## AlbertTokenizerFast\n\n[[autodoc]] AlbertTokenizerFast\n\n## Albert specific outputs\n\n[[autodoc]] models.albert.modeling_albert.AlbertForPreTrainingOutput\n\n[[autodoc]] models.albert.modeling_tf_albert.TFAlbertForPreTrainingOutput\n\n<frameworkcontent>\n<pt>\n\n## AlbertModel\n\n[[autodoc]] AlbertModel\n    - forward\n\n## AlbertForPreTraining\n\n[[autodoc]] AlbertForPreTraining\n    - forward\n\n## AlbertForMaskedLM\n\n[[autodoc]] AlbertForMaskedLM\n    - forward', '| Method/tool                                                | Improves training speed | Optimizes memory utilization |\n|:-----------------------------------------------------------|:------------------------|:-----------------------------|\n| [Batch size choice](#batch-size-choice)                    | Yes                     | Yes                          |\n| [Gradient accumulation](#gradient-accumulation)            | No                      | Yes                          |\n| [Gradient checkpointing](#gradient-checkpointing)          | No                      | Yes                          |\n| [Mixed precision training](#mixed-precision-training)      | Yes                     | (No)                         |']","[8.975837707519531, 1.6817626953125, -2.175797939300537, -2.2850780487060547, -3.73201060295105]",0.8593320846557617,0.9418754577636719,0.8987124562263489
"What are the three main steps for fine-tuning a model with the ð¤ Datasets library?
",1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.,"The three main steps for fine-tuning a model with the ð¤ Datasets library are as follows:
1. Load a dataset from the Hugging Face Hub.
2. Preprocess the data with `Dataset.map()`.
3. Load and compute metrics.

These steps were outlined in Chapter 3 of the course material provided in the context.","['Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=""absolute z-10 right-0 top-0""\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the ð¤ Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what ð¤ Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we\'ll find answers to the following questions:', '```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=""https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg"">\n    <img src=""https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg"" width = ""300"" height=""300"">\n</a>\n\n## Get a FP32 Textual Inversion model\n\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\n\n```bash\nexport MODEL_NAME=""CompVis/stable-diffusion-v1-4""\nexport DATA_DIR=""./dicoo""', '| [How to fine-tune a model for summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)                         | Show how to fine-tune a T5 model on the BBC news corpus.                                                                                       | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)                |                [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb) |', '<div class=""grid grid-cols-2 gap-4"">\n  <figure class=""image table text-center m-0 w-full"">\n  </figure>\n\n  <figure class=""image table text-center m-0 w-full"">\n    <medium-zoom background=""rgba(0,0,0,.7)"" alt=""A form showing configurations to select for the imported dataset, including split types and data columns."" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/autotrain-image-classification/add-dataset.png""></medium-zoom>\n  </figure>\n</div>\n\nOnce the data has been added, simply choose the number of model candidates that youâd like AutoModel to try out, review the expected training cost (training with 5 candidate models and less than 500 images is free ð¤©), and start training!', '| [How to fine-tune DeBERTa for question-answering with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)                         | Show how to fine-tune a DeBERTa model on the squad.                                                                                       | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)                |                [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb) |']","[8.358916282653809, -1.3373134136199951, -4.362672805786133, -4.990054607391357, -5.780527114868164]",0.881207287311554,0.9683210849761963,0.9227126240730286
"What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?
",+800%,The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is up to 800%.,"['Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources', '---\n\n## Conclusion\n\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.', 'These two metrics will be used to benchmark Hugging Face Infinity across different setups to understand the benefits and tradeoffs in this blog post.\n\n---\n\n## Results\n\nTo run the benchmark, we created an infinity container for the [EC2 C6i instance](https://aws.amazon.com/ec2/instance-types/c6i) (Ice-lake) and optimized a [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) model for sequence classification using Infinity Multiverse. \n\nThis ice-lake optimized Infinity Container can achieve up to 34% better latency & throughput compared to existing cascade-lake-based instances, and up to 800% better latency & throughput compared to vanilla transformers running on ice-lake.', 'With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity', 'In recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference ([part 1](https://huggingface.co/blog/bert-cpu-scaling-part-1), [part 2](https://huggingface.co/blog/bert-cpu-scaling-part-2)) and achieved [single-digit millisecond latency](https://huggingface.co/blog/infinity-cpu-performance) for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for [Habana Gaudi](https://huggingface.co/blog/getting-started-habana) accelerators, which deliver up to 40% better price-performance than GPUs.']","[6.4006195068359375, 6.135651588439941, 5.209621906280518, 0.849901556968689, -1.0956666469573975]",0.8044542670249939,0.8772130608558655,0.839259684085846
"What is the command to upload a spaCy pipeline to the Hugging Face Hub?
",python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl,"The command to upload a spaCy pipeline to the Hugging Face Hub is as follows:

```bash
python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl
```

This command is part of the process described in the context for sharing spaCy models via the `spacy-huggingface-hub` library on the Hugging Face Hub.","['```\n\nAnd for larger-scale use cases, you can click ""Deploy > Accelerated Inference"" and see how to do this with Python.\n\n\n### Sharing your models\n\nBut probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. \n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl', ""Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models"", '### Using existing models\n\nAll models from the Hub can be directly installed using `pip install`. \n\n\n```bash\npip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl', '--\ntitle: ""Welcome spaCy to the Hugging Face Hub""\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.', '```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main']","[2.8803045749664307, 2.0470573902130127, 1.1296229362487793, -0.30074214935302734, -1.3362653255462646]",0.8512731194496155,0.9467048645019531,0.8964563608169556
"What is the time and memory complexity of the NystrÃ¶mformer's approximation of self-attention?
",O(n),"The time and memory complexity of NystrÃ¶mformer's approximation of self-attention is linear. This is because NystrÃ¶mformer approximates the standard self-attention with \\(O(n)\\) complexity, as mentioned in the provided context.","['The [NystrÃ¶mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. NystrÃ¶mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the NystrÃ¶m method and how it can be adapted to approximate self-attention.\n\n\n## NystrÃ¶m method for matrix approximation', '</div>\n\n## Conclusion\n\nNystrÃ¶mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the NystrÃ¶m method and how it can be leveraged for self-attention. Readers interested in deploying or fine-tuning NystrÃ¶mformer for downstream tasks can find the HuggingFace documentation [here](https://huggingface.co/docs/transformers/model_doc/nystromformer).', '## Can we approximate self-attention with the NystrÃ¶m method?\n\nOur goal is to ultimately approximate the softmax matrix in standard self attention: S = softmax \\\\( \\frac{QK^T}{\\sqrt{d}} \\\\)\n\nHere, \\\\(Q\\\\) and \\\\(K\\\\) denote the queries and keys respectively. Following the procedure discussed above, we would sample \\\\(m\\\\) rows and columns from \\\\(S\\\\), form four submatrices, and obtain \\\\(\\hat{S}\\\\):\n\n<figure class=""image table text-center m-0 w-full"">\n  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""NystrÃ¶m approximation of S"" src=""assets/86_nystromformer/s_hat.png""></medium-zoom>\n  <figcaption>NystrÃ¶m approximation of S</figcaption>\n</figure>', '--\ntitle: ""NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m method""\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# NystrÃ¶mformer: Approximating self-attention in linear time and memory via the NystrÃ¶m method\n\n\n<script async defer src=""https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js""></script>\n\n## Introduction', 'The overall algorithm is summarised by the following figure from the paper:\n\n<figure class=""image table text-center m-0 w-full"">\n  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Efficient self-attention with the NystrÃ¶m method"" src=""assets/86_nystromformer/paper_figure.png""></medium-zoom>\n  <figcaption>Efficient self-attention with the NystrÃ¶m method</figcaption>\n</figure>\n\nThe three orange matrices above correspond to the three matrices we constructed using the key and query landmarks. Also, notice that there is a DConv box. This corresponds to a skip connection added to the values using a 1D depthwise convolution.\n\n\n## How is NystrÃ¶mformer implemented?']","[2.725971221923828, 0.15460354089736938, -0.09656041860580444, -0.6728003025054932, -1.974501132965088]",0.7825191617012024,0.8861281871795654,0.8311070799827576
"What is the goal of the Named Entity Recognition task in token classification?
","The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.","The goal of the Named Entity Recognition task in token classification is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labeling each token with one class for each entity, and another class for tokens that have no entity.","['Welcome to the Hugging Face tasks series! In this video weâll take a look at the token classification task.\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\nLetâs take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.', 'Another token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.\nToken classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\nAn example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.', '### Token classification\n\nTo use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.\n\nReady to try your hand at token classification? Check out our complete [token classification guide](tasks/token_classification) to learn how to finetune DistilBERT and use it for inference!\n\n### Question answering', 'An example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\nYou can extract important information from invoices using named entity recognition models, such as date, organization name or address.\nFor more information about the Token classification task, check out the Hugging Face course.', ""ote: the following transcripts are associated with Merve Noyan's videos in the Hugging Face Tasks playlist: https://www.youtube.com/playlist?list=PLo2EIpI_JMQtyEr-sLJSy5_SnLCb4vtQf\n\nToken Classification video""]","[8.799345016479492, 4.593922138214111, 2.4031221866607666, 1.6643130779266357, -5.01120662689209]",0.9152060151100159,0.984557569026947,0.9486159682273865
"What is the resolution of images used by the CLIPSeg model?
",352 x 352 pixels,"The resolution of images used by the CLIPSeg model is 352 x 352 pixels, as mentioned in the context: ""For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution.""","['Currently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, letâs first take a look at how CLIPSeg works.\n\n## CLIP: the magic model behind CLIPSeg', '## Conclusion\n\nCLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so youâll probably still want to fine-tune a different segmentation model if accuracy is important.', '<figure class=""image table text-center m-0 w-full"">\n  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Overview of the CLIPSeg model"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clipseg-overview.png""></medium-zoom>\n  <figcaption><a href=""https://arxiv.org/abs/2112.10003"">Source</a></figcaption>\n</figure>', '<figure class=""image table text-center m-0 w-full"">\n  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Overview of the CLIPSeg model"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png""></medium-zoom>\n  <figcaption>Example of image classification using CLIP (<a href=""https://openai.com/blog/clip/"">source</a>).</figcaption>\n</figure>', '--\ntitle: Zero-shot image segmentation with CLIPSeg\nthumbnail: /blog/assets/123_clipseg-zero-shot/thumb.png\nauthors:\n- user: segments-tobias\n  guest: true\n- user: nielsr\n---\n\n# Zero-shot image segmentation with CLIPSeg\n\n\n<script async defer src=""https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js""></script>\n\n<a target=""_blank"" href=""https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/123_clipseg-zero-shot.ipynb"">\n    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>\n</a>']","[5.635388374328613, 0.6760971546173096, 0.6169716715812683, 0.5840951800346375, -2.929421901702881]",0.8176261186599731,0.8761526346206665,0.8458782434463501
"What can you use Gradio for?
","Create a demo for your machine learning model, share your machine learning model with others, and debug your model.","Gradio can be used for demoing machine learning models as a web app. It wraps Python functions, including machine learning models' inference functions, into a user interface. The demos can be launched inside Jupyter notebooks, Colab notebooks, and embedded in websites.","['<img alt=""Screen Shot 2022-08-01 at 5 54 59 PM"" src=""https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png"">\n\n## What are Hugging Face Spaces & Gradio?\n\n### Gradio\n\nGradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model\'s inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\n\nGet started [here](https://gradio.app/getting_started)\n\n### Hugging Face Spaces', '### 3. Where can you launch a Gradio demo from?\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: ""Standard python IDEs"",\n\t\t\texplain: ""Gradio works great with your favorite IDE."",\n            correct: true\n        },\n        {\n\t\t\ttext: ""Google Colab notebooks"",\n\t\t\texplain: ""You can create and launch a demo within your Google colab notebook."",\n\t\t\tcorrect: true\n        },\n        {\n\t\t\ttext: ""Jupyter notebooks"",\n\t\t\texplain: ""Good choice - You can create and launch a demo within your Jupyter notebook."",\n\t\t\tcorrect: true\n        }\n\t]}\n/>\n\n### 4. Gradio is designed primarily for NLP models', '### 2. Gradio ONLY works with PyTorch models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: ""True"",\n\t\t\texplain: ""Gradio works with PyTorch models, but also works for any type of machine learning model!""\n        },\n        {\n\t\t\ttext: ""False"",\n\t\t\texplain: ""Gradio is model agnostic, meaning you can create a demo for any type of machine learning model."",\n\t\t\tcorrect: true\n        }\n\t]}\n/>\n\n### 3. Where can you launch a Gradio demo from?', '### 4. Gradio is designed primarily for NLP models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: ""True"",\n\t\t\texplain: ""Gradio works with pretty much any data type, not just NLP.""\n        },\n        {\n\t\t\ttext: ""False"",\n\t\t\texplain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",\n            correct: true\n        }\n\t]}\n/>\n\n### 5. Which of the following features are supported by Gradio?', ""- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)""]","[2.0726726055145264, 1.0046883821487427, -0.2995167076587677, -0.5403167009353638, -1.3391644954681396]",0.8533488512039185,0.9011619091033936,0.876603901386261
"What TensorFlow API function is used to load a saved tensor file?
",safetensors.tensorflow.load_file,The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`.,"['Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save', '```\n\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training), \nyou can expect to get about a 3x memory improvement and even slightly higher throughput as using Adafactor. \n\n### multi_tensor\n\npytorch-nightly introduced `torch.optim._multi_tensor` which should significantly speed up the optimizers for situations \nwith lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner, take a look at this GitHub [issue](https://github.com/huggingface/transformers/issues/9965).\n\n## Data preloading', ""```\n\n<Tip>\n\ntf32 can't be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an internal CUDA data type. You need `torch>=1.7` to use tf32 data types.\n\n</Tip>\n\nFor additional information on tf32 vs other precisions, please refer to the following benchmarks: \n[RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803) and\n[A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189).\n\n## Flash Attention 2\n\nYou can speedup the training throughput by using Flash Attention 2 integration in transformers. Check out the appropriate section in the [single GPU section](./perf_infer_gpu_one#Flash-Attention-2) to learn more about how to load a model with Flash Attention 2 modules. \n\n## Optimizer choice"", ""- Byte-level BPE, as used in GPT-2\n- WordPiece, as used in BERT\n- SentencePiece or Unigram, as used in several multilingual models\n\nYou should now have sufficient knowledge of how tokenizers work to get started with the API.\n\n## Loading and saving[[loading-and-saving]]\n\nLoading and saving tokenizers is as simple as it is with models. Actually, it's based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the *architecture* of the model) as well as its vocabulary (a bit like the *weights* of the model).\n\nLoading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the `BertTokenizer` class:"", ""transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will""]","[1.7084121704101562, -4.653308868408203, -4.716238498687744, -5.600337028503418, -6.0092315673828125]",0.8800260424613953,0.9358482360839844,0.9070791006088257
"Where can you access the logs of your Endpoints in Hugging Face Endpoints?
","In the ""Logs"" tab of your Endpoint through the UI.","You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the âLogsâ tab of your Endpoint. The Container Logs are available when your Endpoint is in the âRunningâ state, and you can also check the Build Logs if your Endpoint creation is in the âFailedâ state to determine the reason.","['Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the âLogsâ tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=""https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png"" alt=""select logs"" />\n\nThe Container Logs are only available when your Endpoint is in the âRunningâ state. \n\n_Note: If your Endpoint creation is in the âFailedâ state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**\n\n<img src=""https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png"" alt=""build logs"" />', '**Build Logs:**\n\n<img src=""https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png"" alt=""build logs"" />\n\n**Container Logs:**\n\n<img src=""https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png"" alt=""container logs"" />', '### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server\n\n- /healthcheck: Ensure the app is running\n- /metrics: Return a list of metrics in the Prometheus format\n- /webhook: Add, update or remove a dataset\n- /is-valid: Tell if a dataset is [valid](https://huggingface.co/docs/datasets-server/valid)\n- /splits: List the [splits](https://huggingface.co/docs/datasets-server/splits) names for a dataset\n- /first-rows: Extract the [first rows](https://huggingface.co/docs/datasets-server/first_rows) for a dataset split\n- /parquet: List the [parquet files](https://huggingface.co/docs/datasets-server/parquet) auto-converted for a dataset', 'Datasets server API\n\n> API on ð¤ datasets\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server', '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview']","[7.704747200012207, 1.164287805557251, -0.9097878336906433, -2.070384979248047, -3.1253437995910645]",0.8638705611228943,0.9279212951660156,0.8947511315345764
"What is the latest task added to Hugging Face AutoTrain for Computer Vision?
",Image Classification,The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification.,"[""[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer ð¤¯ Weâve been expanding the number of tasks that we support, and weâre proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task weâve added, with more on the way. But what does this mean for you?"", 'Welcome to the Hugging Face tasks series. In this video, we will take a look at the Question Answering task.\nQuestion answering is the task of extracting an answer in a given document.\nQuestion answering models take a context, which is the document you want to search in, and a question and return an answer. Note that the answer is not generated, but extracted from the context. This type of question answering is called extractive.\nThe task is evaluated on two metrics, exact match and F1-Score.\nAs the name implies, exact match looks for an exact match between the predicted answer and the correct answer.', '## How can you train your own image classifier?\n\nIf you havenât [created a Hugging Face account](https://huggingface.co/join) yet, nowâs the time! Following that, make your way over to the [AutoTrain homepage](https://huggingface.co/autotrain) and click on âCreate new projectâ to get started. Youâll be asked to fill in some basic info about your project. In the screenshot below youâll see that I created a project named `butterflies-classification`, and I chose the âImage Classificationâ task. Iâve also chosen the âAutomaticâ model option, since I want to let AutoTrain do the work of finding the best model architectures for my project.', '```\n\n## Supported Tasks and Custom Models\n\nThe Hugging Face Unity API also currently supports the following tasks:\n\n- [Conversation](https://huggingface.co/tasks/conversational)\n- [Text Generation](https://huggingface.co/tasks/text-generation)\n- [Text to Image](https://huggingface.co/tasks/text-to-image)\n- [Text Classification](https://huggingface.co/tasks/text-classification)\n- [Question Answering](https://huggingface.co/tasks/question-answering)\n- [Translation](https://huggingface.co/tasks/translation)\n- [Summarization](https://huggingface.co/tasks/summarization)\n- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)\n\nUse the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.', '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# OpenVINO']","[6.7532758712768555, 0.7315505743026733, 0.43546175956726074, -0.5282979011535645, -4.184412002563477]",0.8157225847244263,0.8782259821891785,0.8458212018013
"What is the default repository type created by the `create_repo` function on Hugging Face Hub?
",model,"The default repository type created by the `create_repo` function on Hugging Face Hub is a model repository. 

","['```\n\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(""lysandre/test-dataset"", repo_type=""dataset"")\n\'https://huggingface.co/datasets/lysandre/test-dataset\'\n```\n\nWhen you create a repository, you can set your repository visibility with the `private` parameter.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(""lysandre/test-private"", private=True)', 'Create an empty repository with [`create_repo`] and give it a name with the `repo_id` parameter. The `repo_id` is your namespace followed by the repository name: `username_or_org/repo_name`.\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(""lysandre/test-model"")\n\'https://huggingface.co/lysandre/test-model\'', '```\n\nWe can now push the repository to the Hugging Face Hub. For this, we will use `push` from `hub_utils`. Hugging Face Hub requires tokens for authentication, therefore you need to pass your token in either  `notebook_login` if you\'re logging in from a notebook, or `huggingface-cli login` if you\'re logging in from the CLI.\n\n```python\n# if the repository doesn\'t exist remotely on the Hugging Face Hub, it will be created when we set create_remote to True\nrepo_id = ""skops-user/my-awesome-model""\nhub_utils.push(\n    repo_id=repo_id,\n    source=local_repo,\n    token=token,\n    commit_message=""pushing files to the repo from the example!"",\n    create_remote=True,\n)', '<Tip warning={true}>\n\n[`Repository`] is deprecated in favor of the http-based alternatives implemented in [`HfApi`]. Given its large adoption in legacy code, the complete removal of [`Repository`] will only happen in release `v1.0`. For more details, please read [this explanation page](./concepts/git_vs_http).\n\n</Tip>\n\n### Use a local repository\n\nInstantiate a [`Repository`] object with a path to a local repository:\n\n```py\n>>> from huggingface_hub import Repository\n>>> repo = Repository(local_dir=""<path>/<to>/<folder>"")', '## Change repository settings\n\nRepositories come with some settings that you can configure. Most of the time, you will want to do that manually in the\nrepo settings page in your browser. You must have write access to a repo to configure it (either own it or being part of\nan organization). In this section, we will see the settings that you can also configure programmatically using `huggingface_hub`.\n\nSome settings are specific to Spaces (hardware, environment variables,...). To configure those, please refer to our [Manage your Spaces](../guides/manage-spaces) guide.\n\n### Update visibility']","[4.896388530731201, 2.25909423828125, 0.27706828713417053, -0.21877527236938477, -2.7165889739990234]",0.7755994200706482,0.8201712965965271,0.7972628474235535
"How many splits does the ""duorc"" dataset have?
",Six,"The ""duorc"" dataset has six splits, as mentioned in the provided context:
- ""ParaphraseRC"" configuration has splits for ""train"", ""validation"", and ""test"".
- ""SelfRC"" configuration also has splits for ""train"", ""validation"", and ""test"". 

Therefore, the ""duorc"" dataset has a total of six splits.","['```\n</curl>\n</inferencesnippet>\n\nThe endpoint response is a JSON containing a list of the dataset\'s splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:\n\n```json\n{\n  ""splits"": [\n    { ""dataset"": ""duorc"", ""config"": ""ParaphraseRC"", ""split"": ""train"" },\n    { ""dataset"": ""duorc"", ""config"": ""ParaphraseRC"", ""split"": ""validation"" },\n    { ""dataset"": ""duorc"", ""config"": ""ParaphraseRC"", ""split"": ""test"" },\n    { ""dataset"": ""duorc"", ""config"": ""SelfRC"", ""split"": ""train"" },\n    { ""dataset"": ""duorc"", ""config"": ""SelfRC"", ""split"": ""validation"" },\n    { ""dataset"": ""duorc"", ""config"": ""SelfRC"", ""split"": ""test"" }\n  ],\n  ""pending"": [],\n  ""failed"": []\n}\n```', 'The `/splits` endpoint accepts the dataset name as its query parameter:\n\n<inferencesnippet>\n<python>\n```python\nimport requests\nheaders = {""Authorization"": f""Bearer {API_TOKEN}""}\nAPI_URL = ""https://datasets-server.huggingface.co/splits?dataset=duorc""\ndef query():\n    response = requests.get(API_URL, headers=headers)\n    return response.json()\ndata = query()', ""List splits and configurations\n\nDatasets typically have splits and may also have configurations. A _split_ is a subset of the dataset, like `train` and `test`, that are used during different stages of training and evaluating a model. A _configuration_ is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you're interested in learning more about splits and configurations, check out the [Load a dataset from the Hub tutorial](https://huggingface.co/docs/datasets/main/en/load_hub)!"", ""This guide shows you how to use Datasets Server's `/splits` endpoint to retrieve a dataset's splits and configurations programmatically. Feel free to also try it out with [Postman](https://www.postman.com/huggingface/workspace/hugging-face-apis/request/23242779-f0cde3b9-c2ee-4062-aaca-65c4cfdd96f8), [RapidAPI](https://rapidapi.com/hugging-face-hugging-face-default/api/hugging-face-datasets-api), or [ReDoc](https://redocly.github.io/redoc/?url=https://datasets-server.huggingface.co/openapi.json#operation/listSplits)\n\nThe `/splits` endpoint accepts the dataset name as its query parameter:"", '```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=""https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg"">\n    <img src=""https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg"" width = ""300"" height=""300"">\n</a>\n\n## Get a FP32 Textual Inversion model\n\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\n\n```bash\nexport MODEL_NAME=""CompVis/stable-diffusion-v1-4""\nexport DATA_DIR=""./dicoo""']","[5.783048629760742, -3.683194160461426, -4.545888900756836, -5.1221513748168945, -6.121996879577637]",0.7583069801330566,0.8588435649871826,0.8054501414299011
"What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?
","FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.","The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to enable training of large pretrained models with up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes. FSDP enhances memory efficiency, allowing for scalability in training with larger batch or model sizes.","['!--â ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\n<Tip warning={true}>', ""```\n\nFor example, your FSDP configuration file may look like the following:\n\n```yaml\ncommand_file: null\ncommands: null\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: FSDP\ndowncast_bf16: 'no'\ndynamo_backend: 'NO'\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_offload_params: true\n  fsdp_sharding_strategy: 1\n  fsdp_state_dict_type: FULL_STATE_DICT\n  fsdp_transformer_layer_cls_to_wrap: T5Block\ngpu_ids: null\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmegatron_lm_config: {}\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_name: null\ntpu_zone: null\nuse_cpu: false"", ""<Tip warning={true}>\n\nCurrently, FSDP does not confer any reduction in GPU memory usage and FSDP with CPU offload actually consumes 1.65x more GPU memory during training. You can track this PyTorch [issue](https://github.com/pytorch/pytorch/issues/91165) for any updates.\n\n</Tip>\n\nFSDP is supported in ð¤ Accelerate, and you can use it with ð¤ PEFT. This guide will help you learn how to use our FSDP [training script](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py). You'll configure the script to train a large model for conditional generation.\n\n## Configuration"", '```\n\nIn contrast, here are the changes needed to have this code run with distributed training are the followings:\n\n```diff\n+ import os\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from torch.utils.data import DistributedSampler\n+ from torch.nn.parallel import DistributedDataParallel\n\n+ local_rank = int(os.environ.get(""LOCAL_RANK"", -1))\n- device = \'cpu\'\n+ device = device = torch.device(""cuda"", local_rank)\n\n  model = torch.nn.Transformer().to(device)\n+ model = DistributedDataParallel(model)  \n  optim = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset(\'my_dataset\')\n+ sampler = DistributedSampler(dataset)\n- data = torch.utils.data.DataLoader(dataset, shuffle=True)\n+ data = torch.utils.data.DataLoader(dataset, sampler=sampler)', 'As a result its now trivialized to perform distributed training with Accelerate and keeping as much of the barebones PyTorch code the same as possible.\n\nEarlier it was mentioned that Accelerate also makes the DataLoaders more efficient. This is through custom Samplers that can send parts of the batches automatically to different devices during training allowing for a single copy of the data to be known at one time, rather than four at once into memory depending on the configuration. Along with this, there is only a single full copy of the original dataset in memory total. Subsets of this dataset are split between all of the nodes that are utilized for training, allowing for much larger datasets to be trained on a single instance without an explosion in memory utilized.']","[6.326905250549316, -0.3131650984287262, -0.9173055291175842, -2.9285950660705566, -4.426321983337402]",0.9094194769859314,0.9593172073364258,0.9337021112442017
"What file format is used to save and store PyTorch model weights more securely than `.bin` files?
",`.safetensors`,"The file format used to save and store PyTorch model weights more securely than `.bin` files is `.safetensors`. 

The provided context mentions: ""To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.""","[""Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>"", '```\n\nIf you save it using [`~PreTrainedModel.save_pretrained`], you will get a new folder with two files: the config of the model and its weights:\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n[\'config.json\', \'pytorch_model.bin\']\n```\n\nNow let\'s use a maximum shard size of 200MB:\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=""200MB"")\n...     print(sorted(os.listdir(tmp_dir)))\n[\'config.json\', \'pytorch_model-00001-of-00003.bin\', \'pytorch_model-00002-of-00003.bin\', \'pytorch_model-00003-of-00003.bin\', \'pytorch_model.bin.index.json\']', '```\n\nThe main advantage of doing this for big models is that during step 2 of the workflow shown above, each shard of the checkpoint is loaded after the previous one, capping the memory usage in RAM to the model size plus the size of the biggest shard.\n\nBehind the scenes, the index file is used to determine which keys are in the checkpoint, and where the corresponding weights are stored. We can load that index like any json and get a dictionary:\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=""200MB"")\n...     with open(os.path.join(tmp_dir, ""pytorch_model.bin.index.json""), ""r"") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys([\'metadata\', \'weight_map\'])', 'torch.save({""g"": generator.state_dict()}, ""your-model-name.pt"")\n\nfiles.download(\'your-model-name.pt\')\n\nlatent_dim = 512\ndevice=""cuda""\nmodel_path_s = hf_hub_download(repo_id=""akhaliq/jojogan-stylegan2-ffhq-config-f"", filename=""stylegan2-ffhq-config-f.pt"")\noriginal_generator = Generator(1024, latent_dim, 8, 2).to(device)\nckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)\noriginal_generator.load_state_dict(ckpt[""g_ema""], strict=False)\nmean_latent = original_generator.mean_latent(10000)\n\ngenerator = deepcopy(original_generator)\n\nckpt = torch.load(""/content/JoJoGAN/your-model-name.pt"", map_location=lambda storage, loc: storage)\ngenerator.load_state_dict(ckpt[""g""], strict=False)\ngenerator.eval()\n\nplt.rcParams[\'figure.dpi\'] = 150', 'Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save']","[6.596498489379883, 2.216726303100586, 2.102165460586548, -1.9055942296981812, -8.620147705078125]",0.8160896301269531,0.8704255819320679,0.8423823118209839
"What type of security certification does Hugging Face have?
",SOC2 Type 2 certified,"Hugging Face is also SOC2 Type 2 certified, meaning they provide security certification to their customers and actively monitor and patch any security weaknesses.","['Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n\n<img width=""150"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg"">\n\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\n\n## Contents\n\n- [User Access Tokens](./security-tokens)\n- [Git over SSH](./security-git-ssh)\n- [Signing commits with GPG](./security-gpg)\n- [Single Sign-On (SSO)](./security-sso)\n- [Malware Scanning](./security-malware)\n- [Pickle Scanning](./security-pickle)\n- [Secrets Scanning](./security-secrets)', ""Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing)."", '!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Research projects', '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Overview', 'If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources\n\n* [Hugging Face Infinity](https://huggingface.co/infinity)\n* [Hugging Face Infinity Trial](https://huggingface.co/infinity-trial)\n* [Amazon EC2 C6i instances](https://aws.amazon.com/ec2/instance-types/c6i) \n* [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n* [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n* [DistilBERT model](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion)\n* [ð¤ Infinity: CPU Ice-Lake Benchmark](https://docs.google.com/spreadsheets/d/1GWFb7L967vZtAS1yHhyTOZK1y-ZhdWUFqovv7-73Plg/edit?usp=sharing)']","[6.262009620666504, 2.7732715606689453, -3.358099937438965, -3.489673137664795, -3.835026979446411]",0.8327695727348328,0.9020912051200867,0.8660454154014587
"What do RAG models combine to generate outputs?
",Pretrained dense retrieval (DPR) and sequence-to-sequence models.,RAG models combine pretrained dense retrieval (DPR) and sequence-to-sequence models to generate outputs.,"['â ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# RAG\n\n<div class=""flex flex-wrap space-x-1"">\n<a href=""https://huggingface.co/models?filter=rag"">\n<img alt=""Models"" src=""https://img.shields.io/badge/All_model_pages-rag-blueviolet"">\n</a>\n</div>\n\n## Overview\n\nRetrieval-augmented generation (""RAG"") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\nboth retrieval and generation to adapt to downstream tasks.', 'This model was contributed by [ola13](https://huggingface.co/ola13).\n\n## Usage tips\n\nRetrieval-augmented generation (""RAG"") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \nto downstream tasks.\n\n## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel', 'general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) â models which combine pre-trained\nparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a\npre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a\npre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our\nmodels on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,', 'models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,\noutperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation\ntasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art\nparametric-only seq2seq baseline.*', '[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput\n\n## RagRetriever\n\n[[autodoc]] RagRetriever\n\n<frameworkcontent>\n<pt>\n\n## RagModel\n\n[[autodoc]] RagModel\n    - forward\n\n## RagSequenceForGeneration\n\n[[autodoc]] RagSequenceForGeneration\n    - forward\n    - generate\n\n## RagTokenForGeneration\n\n[[autodoc]] RagTokenForGeneration\n    - forward\n    - generate\n\n</pt>\n<tf>\n\n## TFRagModel\n\n[[autodoc]] TFRagModel\n    - call\n\n## TFRagSequenceForGeneration\n\n[[autodoc]] TFRagSequenceForGeneration\n    - call\n    - generate\n\n## TFRagTokenForGeneration\n\n[[autodoc]] TFRagTokenForGeneration\n    - call\n    - generate\n\n</tf>\n</frameworkcontent>']","[6.226227760314941, 6.145932197570801, 1.4125909805297852, 0.5933866500854492, -0.6433379054069519]",0.9235350489616394,0.9645267128944397,0.9435859322547913
"What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?
",Beautiful Soup,"MarkupLMFeatureExtractor uses Beautiful Soup, a Python library for pulling data out of HTML and XML files.","['[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\n\nIn total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\n\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**', '<small> MarkupLM architecture. Taken from the <a href=""https://arxiv.org/abs/2110.08518"">original paper.</a> </small>\n\n## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.', 'This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).\n\n## Usage tips\n\n- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.\nThese are the XPATH tags and subscripts respectively for each token in the input sequence.\n- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.\n\n<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg""\nalt=""drawing"" width=""600""/>', 'â ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# MarkupLM\n\n## Overview\n\nThe MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\nUnderstanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\napplied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\nperformance, similar to [LayoutLM](layoutlm).', '<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg""\nalt=""drawing"" width=""600""/> \n\n<small> MarkupLM architecture. Taken from the <a href=""https://arxiv.org/abs/2110.08518"">original paper.</a> </small>\n\n## Usage: MarkupLMProcessor']","[7.9298930168151855, 3.1590023040771484, -0.888055145740509, -2.2685036659240723, -2.5321359634399414]",0.7941176891326904,0.8311202526092529,0.812197744846344
"What is the file size limit for syncing to HF Spaces without using Git-LFS?
",10MB,"The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. This information is directly provided in the context where it states, ""files larger than 10MB, Spaces requires Git-LFS.""","[""Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME"", '```\n\nFinally, create an Action that automatically checks the file size of any new pull request:\n\n\n```yaml\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```', '```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main', '```\n\nNext, set up a GitHub Action to push your main branch to Spaces. In the example below:\n\n* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. \n* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.\n\n```yaml\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:', '```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main']","[2.4013829231262207, 1.831227421760559, -2.4710230827331543, -4.4462890625, -7.539224624633789]",0.7900829315185547,0.8567097187042236,0.8220484852790833
"What is the title of the paper introducing the ByT5 model?
",ByT5: Towards a token-free future with pre-trained byte-to-byte models,"The title of the paper introducing the ByT5 model is ""ByT5: Towards a token-free future with pre-trained byte-to-byte models"" by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.""","['â ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# ByT5\n\n## Overview\n\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel.\n\nThe abstract from the paper is the following:', '## Usage example\n\nByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:\n\n```python\n>>> from transformers import T5ForConditionalGeneration\n>>> import torch\n\n>>> model = T5ForConditionalGeneration.from_pretrained(""google/byt5-small"")\n\n>>> num_special_tokens = 3\n>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.\n>>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.\n\n>>> input_ids = torch.tensor([list(""Life is like a box of chocolates."".encode(""utf-8""))]) + num_special_tokens\n\n>>> labels = torch.tensor([list(""La vie est comme une boÃ®te de chocolat."".encode(""utf-8""))]) + num_special_tokens\n\n>>> loss = model(input_ids, labels=labels).loss\n>>> loss.item()\n2.66', '```\n\n\n## ByT5Tokenizer\n\n[[autodoc]] ByT5Tokenizer\n\nSee [`ByT5Tokenizer`] for all details.', 'pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our\nexperiments.*', ""â ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# LongT5\n\n## Overview\n\nThe LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\nby Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\nencoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\nT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)\nTransient-Global attention.\n\n\nThe abstract from the paper is the following:""]","[2.8680672645568848, -1.376708745956421, -3.292593002319336, -3.9644739627838135, -5.195285320281982]",0.7984133958816528,0.9468923807144165,0.8663370609283447
"What is the dimension of the feature vector for the base BERT model?
",768,The dimension of the feature vector for the base BERT model is 768.,"['Let\'s dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word ""to"", isn\'t the representation of only the ""to"" word. It also takes into account the words around it, which we call the âcontextâ.As in, it looks to the left context, the word on the left of the one we\'re studying (here the word ""Welcome"") and the context on the right (here the word ""NYC"") and outputs a value for the word, within its context. It is therefore a contextualized value. One', ""- ALBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather\n  than the left.\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains\n  similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same\n  number of (repeating) layers.\n- Embedding size E is different from hidden size H justified because the embeddings are context independent (one embedding vector represents one token), whereas hidden states are context dependent (one hidden state represents a sequence of tokens) so it's more logical to have H >> E. Also, the embedding matrix is large since it's V x E (V being the vocab size). If E < H, it has less parameters."", '[BERT](model_doc/bert) is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides.\n\n1. BERT uses [WordPiece](tokenizer_summary#wordpiece) tokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special `[SEP]` token is added to differentiate them. A special `[CLS]` token is added to the beginning of every sequence of text. The final output with the `[CLS]` token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences.', ""dimension is the hidden size of our model 768. To get an output linked to our classification problem, we need to use the AutoModelForSequenceClassification class. It works exactly as the AutoModel class, except that it will build a model with a classification head. There is one auto class for each common NLP task in the Transformers library. Here, after giving our model the two sentences, we get a tensor of size two by two: one result for each sentence and for each possible label. Those outputs are not probabilities yet (we can see they don't sum to 1). This is because each model of the Transformers library returns logits. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a"", '2. BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and ""predict"" the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word.']","[3.7991819381713867, -1.9283033609390259, -2.348433017730713, -4.739871025085449, -5.350332260131836]",0.7896232604980469,0.8243154883384705,0.8065965175628662
"What special identifier does the WordPiece Model use for continuing subwords?
",##,The special identifier that the WordPiece Model uses for continuing subwords is `##`.,"['| Name | Description |\n| :--- | :--- |\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\n| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `â` to identify whitespaces, and so this Decoder helps with decoding these. |\n| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |', '| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words donât exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |', 'These subwords end up providing a lot of semantic meaning: for instance, in the example above ""tokenization"" was split into ""token"" and ""ization"", two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.\n\nThis approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n\n### And more![[and-more]]\n\nUnsurprisingly, there are many more techniques out there. To name a few:\n\n- Byte-level BPE, as used in GPT-2\n- WordPiece, as used in BERT\n- SentencePiece or Unigram, as used in several multilingual models', '```\n\nThere are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large ""vocabularies,"" where a vocabulary is defined by the total number of independent tokens that we have in our corpus.\n\nEach word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.', 'Here is an example showing how a subword tokenization algorithm would tokenize the sequence ""Let\'s do tokenization!"":\n\n<div class=""flex justify-center"">\n  <img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg"" alt=""A subword tokenization algorithm.""/>\n  <img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg"" alt=""A subword tokenization algorithm.""/>\n</div>']","[5.844759941101074, 1.1428296566009521, -1.0237793922424316, -4.050319194793701, -5.330514907836914]",0.7666096091270447,0.8191182017326355,0.7919945120811462
"What is the purpose of the ð§¨ Diffusers tutorials?
",To provide a gentle introduction to diffusion models and help understand the library fundamentals.,"The purpose of the ð§¨ Diffusers tutorials is to provide a beginner-friendly introduction to diffusion models and the library fundamentals, helping users understand how to use the library as a modular toolbox for building their own diffusion systems. Users will learn how to use a pipeline for inference, deconstruct that pipeline, and eventually train their own diffusion model to generate desired outputs. After completing the tutorials, users will have the necessary skills to explore the library independently for their projects and applications.","[""# Overview\n\nWelcome to ð§¨ Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how ð§¨ Diffusers is meant to be used.\n\nYou'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want."", ""After completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.\n\nFeel free to join our community on [Discord](https://discord.com/invite/JfAtkvEtRb) or the [forums](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) to connect and collaborate with other users and developers!\n\nLet's start diffusing! ð§¨"", '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ð§¨ Diffusersâ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.', 'Great, now you should have generally understood why ð§¨ Diffusers is designed the way it is ð¤.\nWe try to apply these design principles consistently across the library. Nevertheless, there are some minor exceptions to the philosophy or some unlucky design choices. If you have feedback regarding the design, we would â¤ï¸  to hear it [directly on GitHub](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).\n\n## Design Philosophy in Details', '- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.']","[5.362502098083496, 0.6122381687164307, -0.6099563837051392, -1.6650502681732178, -2.3481106758117676]",0.8463228940963745,0.9192090630531311,0.8812614679336548
"What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?
","""manual""","The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `""manual""`.","['å¨ `gradio.Interface` ä¸­æ[åä¸ªåæ°](https://gradio.app/docs/#interface-header)æ§å¶æ è®°çå·¥ä½æ¹å¼ãæä»¬å°è¯¦ç»ä»ç»å®ä»¬ã\n\n- `allow_flagging`ï¼æ­¤åæ°å¯ä»¥è®¾ç½®ä¸º `""manual""`ï¼é»è®¤å¼ï¼ï¼`""auto""` æ `""never""`ã\n  - `manual`ï¼ç¨æ·å°çå°ä¸ä¸ªæ è®°æé®ï¼åªæå¨ç¹å»æé®æ¶æ ·æ¬æä¼è¢«æ è®°ã\n  - `auto`ï¼ç¨æ·å°ä¸ä¼çå°ä¸ä¸ªæ è®°æé®ï¼ä½æ¯ä¸ªæ ·æ¬é½ä¼èªå¨è¢«æ è®°ã\n  - `never`ï¼ç¨æ·å°ä¸ä¼çå°ä¸ä¸ªæ è®°æé®ï¼å¹¶ä¸ä¸ä¼æ è®°ä»»ä½æ ·æ¬ã\n- `flagging_options`ï¼æ­¤åæ°å¯ä»¥æ¯ `None`ï¼é»è®¤å¼ï¼æå­ç¬¦ä¸²åè¡¨ã\n  - å¦ææ¯ `None`ï¼åç¨æ·åªéç¹å»**æ è®°**æé®ï¼ä¸ä¼æ¾ç¤ºå¶ä»éé¡¹ã\n  - å¦ææä¾äºä¸ä¸ªå­ç¬¦ä¸²åè¡¨ï¼åç¨æ·ä¼çå°å¤ä¸ªæé®ï¼å¯¹åºäºæä¾çæ¯ä¸ªå­ç¬¦ä¸²ãä¾å¦ï¼å¦ææ­¤åæ°çå¼ä¸º`["" éè¯¯ "", "" æ¨¡ç³ ""]`ï¼åä¼æ¾ç¤ºæ è®°ä¸º**æ è®°ä¸ºéè¯¯**å**æ è®°ä¸ºæ¨¡ç³**çæé®ãè¿ä»éç¨äº `allow_flagging` ä¸º `""manual""` çæåµã\n  - æééé¡¹å°ä¸è¾å¥åè¾åºä¸èµ·è®°å½ã\n- `flagging_dir`ï¼æ­¤åæ°æ¥åä¸ä¸ªå­ç¬¦ä¸²ã\n  - å®è¡¨ç¤ºæ è®°æ°æ®å­å¨çç®å½åç§°ã\n- `flagging_callback`ï¼æ­¤åæ°æ¥å `FlaggingCallback` ç±»çå­ç±»çå®ä¾\n  - ä½¿ç¨æ­¤åæ°åè®¸æ¨ç¼åå¨ç¹å»æ è®°æé®æ¶è¿è¡çèªå®ä¹ä»£ç \n  - é»è®¤æåµä¸ï¼å®è®¾ç½®ä¸º `gr.CSVLogger` çä¸ä¸ªå®ä¾\n  - ä¸ä¸ªç¤ºä¾æ¯å°å¶è®¾ç½®ä¸º `gr.HuggingFaceDatasetSaver` çä¸ä¸ªå®ä¾ï¼è¿æ ·æ¨å¯ä»¥å°ä»»ä½æ è®°çæ°æ®å¯¼å¥å° HuggingFace æ°æ®éä¸­ï¼åè§ä¸æï¼ã', '## æ è®°çæ°æ®ä¼åçä»ä¹ï¼\n\nå¨ `flagging_dir` åæ°æä¾çç®å½ä¸­ï¼å°è®°å½æ è®°çæ°æ®ç CSV æä»¶ã\n\nä»¥ä¸æ¯ä¸ä¸ªç¤ºä¾ï¼ä¸é¢çä»£ç åå»ºäºåµå¥å¶ä¸­çè®¡ç®å¨çé¢ï¼\n\n```python\nimport gradio as gr\n\n\ndef calculator(num1, operation, num2):\n    if operation == ""add"":\n        return num1 + num2\n    elif operation == ""subtract"":\n        return num1 - num2\n    elif operation == ""multiply"":\n        return num1 * num2\n    elif operation == ""divide"":\n        return num1 / num2\n\n\niface = gr.Interface(\n    calculator,\n    [""number"", gr.Radio([""add"", ""subtract"", ""multiply"", ""divide""]), ""number""],\n    ""number"",\n    allow_flagging=""manual""\n)\n\niface.launch()', 'ä½¿ç¨æ è®°\n\nç¸å³ç©ºé´ï¼https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic\næ ç­¾ï¼æ è®°ï¼æ°æ®\n\n## ç®ä»\n\nå½æ¨æ¼ç¤ºä¸ä¸ªæºå¨å­¦ä¹ æ¨¡åæ¶ï¼æ¨å¯è½å¸ææ¶éè¯ç¨æ¨¡åçç¨æ·çæ°æ®ï¼ç¹å«æ¯æ¨¡åè¡ä¸ºä¸å¦é¢æçæ°æ®ç¹ãæè·è¿äºâå°é¾âæ°æ®ç¹æ¯æä»·å¼çï¼å ä¸ºå®åè®¸æ¨æ¹è¿æºå¨å­¦ä¹ æ¨¡åå¹¶ä½¿å¶æ´å¯é åç¨³å¥ã\n\nGradio éè¿å¨æ¯ä¸ªâçé¢âä¸­åå«ä¸ä¸ª**æ è®°**æé®æ¥ç®åè¿äºæ°æ®çæ¶éãè¿ä½¿å¾ç¨æ·ææµè¯äººåå¯ä»¥è½»æ¾å°å°æ°æ®åéåè¿è¡æ¼ç¤ºçæºå¨ãæ ·æ¬ä¼ä¿å­å¨ä¸ä¸ª CSV æ¥å¿æä»¶ä¸­ï¼é»è®¤æåµä¸ï¼ãå¦ææ¼ç¤ºæ¶åå¾åãé³é¢ãè§é¢æå¶ä»ç±»åçæä»¶ï¼åè¿äºæä»¶ä¼åç¬ä¿å­å¨ä¸ä¸ªå¹¶è¡ç®å½ä¸­ï¼å¹¶ä¸è¿äºæä»¶çè·¯å¾ä¼ä¿å­å¨ CSV æä»¶ä¸­ã\n\n## å¨ `gradio.Interface` ä¸­ä½¿ç¨**æ è®°**æé®\n\nä½¿ç¨ Gradio ç `Interface` è¿è¡æ è®°ç¹å«ç®åãé»è®¤æåµä¸ï¼å¨è¾åºç»ä»¶ä¸æ¹æä¸ä¸ªæ è®°ä¸º**æ è®°**çæé®ãå½ç¨æ·æµè¯æ¨çæ¨¡åæ¶ï¼å¦æçå°æè¶£çè¾åºï¼ä»ä»¬å¯ä»¥ç¹å»æ è®°æé®å°è¾å¥åè¾åºæ°æ®åéåè¿è¡æ¼ç¤ºçæºå¨ãæ ·æ¬ä¼ä¿å­å¨ä¸ä¸ª CSV æ¥å¿æä»¶ä¸­ï¼é»è®¤æåµä¸ï¼ãå¦ææ¼ç¤ºæ¶åå¾åãé³é¢ãè§é¢æå¶ä»ç±»åçæä»¶ï¼åè¿äºæä»¶ä¼åç¬ä¿å­å¨ä¸ä¸ªå¹¶è¡ç®å½ä¸­ï¼å¹¶ä¸è¿äºæä»¶çè·¯å¾ä¼ä¿å­å¨ CSV æä»¶ä¸­ã', '### 4. Gradio is designed primarily for NLP models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: ""True"",\n\t\t\texplain: ""Gradio works with pretty much any data type, not just NLP.""\n        },\n        {\n\t\t\ttext: ""False"",\n\t\t\texplain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",\n            correct: true\n        }\n\t]}\n/>\n\n### 5. Which of the following features are supported by Gradio?', '### 7. Select all the steps necessary for adding state to your Gradio interface']","[6.483207702636719, 0.4576154351234436, -0.2593238949775696, -6.179891586303711, -6.238180160522461]",0.7797573804855347,0.8471022844314575,0.8120359182357788
"Where can the full code for the Stable Diffusion demo be found?
",https://hf.co/spaces/stabilityai/stable-diffusion/tree/main,The full code for the Stable Diffusion demo can be found in the GitHub repository at the following link: [https://hf.co/spaces/stabilityai/stable-diffusion/tree/main](https://hf.co/spaces/stabilityai/stable-diffusion/tree/main).,"['ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main', 'The following design principles are followed:\n- Pipelines follow the single-file policy. All pipelines can be found in individual directories under src/diffusers/pipelines. One pipeline folder corresponds to one diffusion paper/project/release. Multiple pipeline files can be gathered in one pipeline folder, as itâs done for [`src/diffusers/pipelines/stable-diffusion`](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion). If pipelines share similar functionality, one can make use of the [#Copied from mechanism](https://github.com/huggingface/diffusers/blob/125d783076e5bd9785beb05367a2d2566843a271/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L251).\n- Pipelines all inherit from [`DiffusionPipeline`].', '| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb) | Show how to load and compare outputs from two Stable Diffusion models with different precision| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)|', 'The original codebase can be found at [salesforce/LAVIS](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion). You can find the official BLIP-Diffusion checkpoints under the [hf.co/SalesForce](https://hf.co/SalesForce) organization.\n\n`BlipDiffusionPipeline` and `BlipDiffusionControlNetPipeline` were contributed by [`ayushtues`](https://github.com/ayushtues/).\n\n<Tip>\n\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\n\n</Tip>', ""For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.""]","[4.900071144104004, 0.9401170015335083, -1.10306978225708, -2.9329464435577393, -6.890061378479004]",0.8860405683517456,0.931617259979248,0.9082574844360352
"What transformation does the FNet model use to replace the self-attention layer in a BERT model?
",Fourier transform,"The FNet model replaces the self-attention layer in a BERT model with a standard, unparameterized Fourier Transform.","['â ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n\n-->\n\n# FNet\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:', '*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that ""mix"" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,', '## FNetModel\n\n[[autodoc]] FNetModel\n    - forward\n\n## FNetForPreTraining\n\n[[autodoc]] FNetForPreTraining\n    - forward\n\n## FNetForMaskedLM\n\n[[autodoc]] FNetForMaskedLM\n    - forward\n\n## FNetForNextSentencePrediction\n\n[[autodoc]] FNetForNextSentencePrediction\n    - forward\n\n## FNetForSequenceClassification\n\n[[autodoc]] FNetForSequenceClassification\n    - forward\n\n## FNetForMultipleChoice\n\n[[autodoc]] FNetForMultipleChoice\n    - forward\n\n## FNetForTokenClassification\n\n[[autodoc]] FNetForTokenClassification\n    - forward\n\n## FNetForQuestionAnswering\n\n[[autodoc]] FNetForQuestionAnswering\n    - forward', '## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:', '2. BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and ""predict"" the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word.']","[7.696845531463623, 2.0875790119171143, -2.948857307434082, -4.454021453857422, -4.478570461273193]",0.8190121650695801,0.8911980390548706,0.8535816669464111
"What type of test should typically accompany a bug fix in Gradio's testing strategy?
",Dynamic code test,"The type of test that should typically accompany a bug fix in Gradio's testing strategy is a dynamic code test. This is supported by the following statement from the context: ""Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test...""","['For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).\n\n## Managing defects\n\nAs we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:\n\n- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.', ""## Objectives\n\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n\n1. Ensure that the Gradio library functions as we expect it to.\n2. Enable the maintenance team to quickly identify both the presence and source of defects.\n3. Prevent regressions, i.e. if we fix something it should stay fixed.\n4. Improve the quality of the codebase in order to ease maintenance efforts.\n5. Reduce the amount of manual testing required.\n\n## Scope\n\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas.\n\nWe should not focus on code coverage but on test coverage following the below criteria:"", '- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)', ""- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)"", ""## Types of testing\n\nOur tests will broadly fall into one of three categories:\n\n- Static Quality checks\n- Dynamic 'Code' tests\n- Dynamic Functional tests\n\n### Static Quality checks\n\nStatic quality checks are generally very fast to run and do not require building the code base. They also provide the least value. These tests would be things like linting, typechecking, and formatting.\n\nWhile they offer little in terms of testing functionality they align very closely with objective (4, 5) as they generally help to keep the codebase in good shape and offer very fast feedback. Such check are almost free from an authoring point of view as fixes can be mostly automated (either via scripts or editor integrations).\n\n### Dynamic code tests""]","[5.676948070526123, -0.9047448039054871, -0.9395880699157715, -0.9519144296646118, -4.896337509155273]",0.8095324635505676,0.8851202726364136,0.8456405997276306
"How can you force mixed precision training when initializing the Accelerator in ð¤ Accelerate?
",By passing `fp16=True` to the Accelerator init.,I do not have enough information to answer this question based on the provided context.,"['# Introducing ð¤ Accelerate\n\n\n## ð¤ Accelerate\n\nRun your **raw** PyTorch training scripts on any kind of device.\n\nMost high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. ð¤ Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism.', '```\n\nIf you prefer to use ð¤ Accelerate, find the ð¤ Accelerate example [further in this guide](#using-accelerate). \n\n### BF16\n\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \nis about the same as fp32 - because both have 8-bits used for the numerical range.\n\nYou can enable BF16 in the ð¤ Trainer with:\n\n```python\ntraining_args = TrainingArguments(bf16=True, **default_args)', '```\n\nAlternatively, use ð¤ Accelerate - find the ð¤ Accelerate example [further in this guide](#using-accelerate). \n\n<Tip>\n\nWhile gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.\n\n</Tip>\n\n## Mixed precision training', ""Alternatively, use ð¤ Accelerate to gain full control over the training loop. Find the ð¤ Accelerate example \n[further down in this guide](#using-accelerate).\n\nWhile it is advised to max out GPU usage as much as possible, a high number of gradient accumulation steps can \nresult in a more pronounced training slowdown. Consider the following example. Let's say, the `per_device_train_batch_size=4` \nwithout gradient accumulation hits the GPU's limit. If you would like to train with batches of size 64, do not set the \n`per_device_train_batch_size` to 1 and `gradient_accumulation_steps` to 64. Instead, keep `per_device_train_batch_size=4` \nand set `gradient_accumulation_steps=16`. This results in the same effective batch size while making better use of \nthe available GPU resources."", 'Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures \n(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check \nout the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about \nthe differences between these data types.\n\n### fp16']","[0.43935874104499817, 0.3864770233631134, -0.07915978133678436, -2.0089213848114014, -2.4704480171203613]",0.8353008031845093,0.8329035043716431,0.8341004252433777
"What is the purpose of tokenizers in the NLP pipeline?
",To translate text into data that can be processed by the model.,"The purpose of tokenizers in the NLP pipeline is to translate text into data that can be processed by the model by converting text inputs into numerical data. This process involves splitting the input text into small chunks called tokens, which can be words, parts of words, or punctuation symbols. Additionally, tokenizers may add special tokens like CLS and SEP tokens for specific model requirements. The tokenization process ultimately maps each token to a unique ID in the vocabulary of the pretrained model.","['{:else}\n\n<CourseFloatingBanner chapter={2}\n  classNames=""absolute z-10 right-0 top-0""\n  notebooks={[\n    {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb""},\n    {label: ""Aws Studio"", value: ""https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section4_tf.ipynb""},\n]} />\n\n{/if}\n\n<Youtube id=""VFp38yj8h3A""/>\n\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we\'ll explore exactly what happens in the tokenization pipeline.', '## Pre-tokenizers\n\nThe `PreTokenizer` takes care of splitting the input according to a set\nof rules. This pre-processing lets you ensure that the underlying\n`Model` does not build tokens across multiple ""splits"". For example if\nyou don\'t want to have whitespaces inside a token, then you can have a\n`PreTokenizer` that splits on these whitespaces.\n\nYou can easily combine multiple `PreTokenizer` together using a\n`Sequence` (see below). The `PreTokenizer` is also allowed to modify the\nstring, just like a `Normalizer` does. This is necessary to allow some\ncomplicated algorithms that require to split before normalizing (e.g.\nthe ByteLevel)', '## Models\n\nModels are the core algorithms used to actually tokenize, and therefore,\nthey are the only mandatory component of a Tokenizer.', ""transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will"", '### Tokenization[[tokenization]]\n\nThe tokenization process is done by the `tokenize()` method of the tokenizer:\n\n```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")\n\nsequence = ""Using a Transformer network is simple""\ntokens = tokenizer.tokenize(sequence)\n\nprint(tokens)']","[7.7939252853393555, -3.7950804233551025, -4.97704553604126, -5.162689208984375, -5.564684867858887]",0.8478643894195557,0.9450228214263916,0.893811047077179
"What is the purpose of the Safety Checker in the Diffusers library?
",The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.,"The purpose of the Safety Checker in the Diffusers library is to check and compare the class probability of hard-coded harmful concepts in the embedding space against an image after it has been generated. This Safety Checker hides the harmful concepts to prevent reverse engineering of the checker, ensuring safety measures are in place during the generation process.","['- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repositoryâs authors to have more control over its use.', '- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.\n- Diffusers prefers simple, self-explainable code over condensed, magic code. This means that short-hand code syntaxes such as lambda functions, and advanced PyTorch operators are often not desired.', '- Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible.\n- Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers.', ""For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable."", '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# ð§¨ Diffusersâ Ethical Guidelines\n\n## Preamble\n\n[Diffusers](https://huggingface.co/docs/diffusers/index) provides pre-trained diffusion models and serves as a modular toolbox for inference and training.']","[3.7487199306488037, -1.083662986755371, -2.300142765045166, -2.6879868507385254, -4.089613914489746]",0.9058143496513367,0.9371282458305359,0.9212052226066589
"What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?
",HfApi,The Python class that allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub is the `HfApi` class. This class provides methods such as `get_repo_discussions` for retrieving discussions and pull requests from a specified repository.,"['## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=""bigscience/bloom""):\n...     print(f""{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}"")\n\n# 11 - Add Flax weights, pr: True\n# 10 - Update README.md, pr: True\n# 9 - Training languages in the model card, pr: True\n# 8 - Update tokenizer_config.json, pr: True\n# 7 - Slurm training script, pr: False\n[...]', '```\n\nThe [`Discussion`] object returned by [`HfApi.get_repo_discussions`] contains high-level overview of the\nDiscussion or Pull Request. You can also get more detailed information using [`HfApi.get_discussion_details`]:\n\n```python\n>>> from huggingface_hub import get_discussion_details', '```\n\n`HfApi.get_repo_discussions` supports filtering by author, type (Pull Request or Discussion) and status (`open` or `closed`):\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(\n...    repo_id=""bigscience/bloom"",\n...    author=""ArthurZ"",\n...    discussion_type=""pull_request"",\n...    discussion_status=""open"",\n... ):\n...     print(f""{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}"")\n\n# 19 - Add Flax weights by ArthurZ, pr: True', '```\n\n`HfApi.get_repo_discussions` returns a [generator](https://docs.python.org/3.7/howto/functional.html#generators) that yields\n[`Discussion`] objects. To get all the Discussions in a single list, run:\n\n```python\n>>> from huggingface_hub import get_repo_discussions\n>>> discussions_list = list(get_repo_discussions(repo_id=""bert-base-uncased""))', '>>> create_discussion(\n...     repo_id=""username/repo-name"",\n...     title=""Hi from the huggingface_hub library!"",\n...     token=""<insert your access token here>"",\n... )\nDiscussionWithDetails(...)\n\n>>> create_pull_request(\n...     repo_id=""username/repo-name"",\n...     title=""Hi from the huggingface_hub library!"",\n...     token=""<insert your access token here>"",\n... )\nDiscussionWithDetails(..., is_pull_request=True)']","[9.424774169921875, 3.4745452404022217, 2.796079635620117, 1.3299789428710938, 0.40981999039649963]",0.7840472459793091,0.8871995806694031,0.8324400782585144
"What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?
",Skops,The new library introduced by Hugging Face for hosting scikit-learn models is called `skops`.,"['```\n\nAnd for larger-scale use cases, you can click ""Deploy > Accelerated Inference"" and see how to do this with Python.\n\n\n### Sharing your models\n\nBut probably the coolest feature is that now you can very easily share your models with the `spacy-huggingface-hub` [library](https://github.com/explosion/spacy-huggingface-hub), which extends the `spaCy` CLI with a new command, `huggingface-hub push`. \n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl', '```\n\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browser, and share it with the rest of the community. All the required metadata will be uploaded for you and you even get a cool model card.\n\nTry it out and share your models with the community!\n\n## Would you like to integrate your library to the Hub?\n\nThis integration is possible thanks to the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library which has all our widgets and the API for all our supported libraries. If you would like to integrate your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you!', '--\ntitle: ""Welcome spaCy to the Hugging Face Hub""\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.', '```\n\nYou can see the example repository pushed with above code [here](https://huggingface.co/scikit-learn/skops-blog-example).\nWe have prepared two examples to show how to save your models and use model card utilities. You can find them in the resources section below.\n\n\n##\xa0Resources\n- [Model card tutorial](https://skops.readthedocs.io/en/latest/auto_examples/plot_model_card.html)\n- [hub_utils tutorial](https://skops.readthedocs.io/en/latest/auto_examples/plot_hf_hub.html)\n- [skops documentation](https://skops.readthedocs.io/en/latest/modules/classes.html)', ""Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models""]","[1.4246749877929688, 1.2628774642944336, 0.22117938101291656, 0.16815011203289032, -1.991551399230957]",0.7919092178344727,0.8655640482902527,0.8271000981330872
"What is the purpose of Textual Inversion?
",Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.,"The purpose of Textual Inversion is to personalize text2image models by learning new text embeddings from a few example images. This method allows for training with minimal data, producing small file sizes for the embeddings, which can then be loaded into the text encoder for model personalization.","['# Textual Inversion\n\nTextual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.\n\n[`TextualInversionLoaderMixin`] provides a function for loading Textual Inversion embeddings from Diffusers and Automatic1111 into the text encoder and loading a special token to activate the embeddings.\n\n<Tip>\n\nTo learn more about how to load Textual Inversion embeddings, see the [Textual Inversion](../../using-diffusers/loading_adapters#textual-inversion) loading guide.\n\n</Tip>\n\n## TextualInversionLoaderMixin\n\n[[autodoc]] loaders.textual_inversion.TextualInversionLoaderMixin', ""Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:"", '!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Textual Inversion', 'Once you have the FP32 Textual Inversion model, the following command will take the FP32 Textual Inversion model as input to do distillation for quantization and generate the INT8 Textual Inversion model.\n\n```bash\nexport FP32_MODEL_NAME=""./dicoo_model""\nexport DATA_DIR=""./dicoo""', '```py\nimport gradio as gr\n\n\ndef flip_text(x):\n    return x[::-1]\n\n\ndemo = gr.Blocks()\n\nwith demo:\n    gr.Markdown(\n        """"""\n    # Flip Text!\n    Start typing below to see the output.\n    """"""\n    )\n    input = gr.Textbox(placeholder=""Flip this text"")\n    output = gr.Textbox()\n\n    input.change(fn=flip_text, inputs=input, outputs=output)\n\ndemo.launch()']","[4.764403820037842, 1.5893315076828003, -1.6112935543060303, -2.0674021244049072, -8.491634368896484]",0.8993563652038574,0.9544186592102051,0.9260697364807129
"What is the recommended multiple of batch size for fp16 data type on an A100 GPU?
",64,"The recommended multiple of batch size for fp16 data type on an A100 GPU is 64. This is specified in the context provided where it states: ""For fp16 data type a multiple of 8 is recommended, unless it's an A100 GPU, in which case use multiples of 64.""","[""[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) \ndefine the multiplier based on the dtype and the hardware. For instance, for fp16 data type a multiple of 8 is recommended, unless \nit's an A100 GPU, in which case use multiples of 64.\n\nFor parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). \nThis is where tiling happens and the right multiplier can have a significant speedup.\n\n## Gradient Accumulation"", ""## Batch size choice\n\nTo achieve optimal performance, start by identifying the appropriate batch size. It is recommended to use batch sizes and \ninput/output neuron counts that are of size 2^N. Often it's a multiple of 8, but it can be \nhigher depending on the hardware being used and the model's dtype.\n\nFor reference, check out NVIDIA's recommendation for [input/output neuron counts](\nhttps://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features) and \n[batch size](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size) for \nfully connected layers (which are involved in GEMMs (General Matrix Multiplications))."", 'For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).\n\n## Gradient Checkpointing\n\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. \nThis is because there are other components that also require memory storage.', 'Most commonly mixed precision training is achieved by using fp16 (float16) data types, however, some GPU architectures \n(such as the Ampere architecture) offer bf16 and tf32 (CUDA internal data type) data types. Check \nout the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/) to learn more about \nthe differences between these data types.\n\n### fp16', '```\n\nIn the above example, your effective batch size becomes 4. \n\nAlternatively, use ð¤ Accelerate to gain full control over the training loop. Find the ð¤ Accelerate example \n[further down in this guide](#using-accelerate).']","[4.634342193603516, -1.4348504543304443, -3.3296494483947754, -4.076211452484131, -4.679769515991211]",0.766133725643158,0.8313789367675781,0.7974239587783813
"How do you run a Gradio Blocks app in reload mode using a Python IDE?
",Run `gradio run.py` in the terminal.,"To run a Gradio Blocks app in reload mode using a Python IDE, you can change the command from `python` to `gradio`. This allows you to instantly reload your Gradio app during development without the need to manually close and rerun your code. 

In the provided context, it states:

""When you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes. To make it faster and more convenient to write your code, we've made it easier to 'reload' your Gradio apps instantly when you are developing in a Python IDE.""

Therefore, to run a Gradio Blocks app in reload mode using a Python IDE, you would execute the command `gradio run.py` in the terminal instead of `python run.py`.","['There is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(""# Greetings from Gradio!"")\n    inp = gr.Textbox(placeholder=""What is your name?"")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f""Welcome, {x}!"",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == ""__main__"":\n    my_demo.launch()', ""```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:\n\nIn the terminal, run `gradio run.py`. That's it!\n\nNow, you'll see that after you'll see something like this:\n\n```bash\nWatching: '/Users/freddy/sources/gradio/gradio', '/Users/freddy/sources/gradio/demo/'\n\nRunning on local URL:  http://127.0.0.1:7860"", 'This short Guide will cover both of these methods, so no matter how you write Python, you\'ll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload ð¥\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let\'s name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(""# Greetings from Gradio!"")\n    inp = gr.Textbox(placeholder=""What is your name?"")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f""Welcome, {x}!"",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == ""__main__"":\n    demo.launch()', '## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.\n\nTo make it faster and more convenient to write your code, we\'ve made it easier to ""reload"" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We\'ve also developed an analogous ""magic command"" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\n\nThis short Guide will cover both of these methods, so no matter how you write Python, you\'ll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload ð¥', 'Developing Faster with Auto-Reloading\n\n**Prerequisite**: This Guide requires you to know about Blocks. Make sure to [read the Guide to Blocks first](https://gradio.app/guides/quickstart/#blocks-more-flexibility-and-control).\n\nThis guide covers auto reloading, reloading in a Python IDE, and using gradio with Jupyter Notebooks.\n\n## Why Auto-Reloading?\n\nWhen you are building a Gradio demo, particularly out of Blocks, you may find it cumbersome to keep re-running your code to test your changes.']","[4.930307388305664, 4.808098793029785, 4.685585975646973, 4.515538215637207, 1.3536564111709595]",0.8099023103713989,0.910504162311554,0.8572618365287781
"How can you install the Hugging Face Unity API in your Unity project?
","To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.","To install the Hugging Face Unity API in your Unity project, follow these steps:

1. Open your Unity project.
2. Go to `Window` -> `Package Manager`.
3. Click `+` and select `Add Package from git URL`.
4. Enter `https://github.com/huggingface/unity-api`.
5. Once installed, if the Unity API wizard does not pop up, go to `Window` -> `Hugging Face API Wizard`.

In summary, you can install the Hugging Face Unity API in your Unity project by adding the package from the provided git URL.","['## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n\n<figure class=""image text-center"">\n  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif"">\n</figure>', 'Use the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n\n## Usage Tips\n\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n\n## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!', '6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.', '--\ntitle: ""How to Install and Use the Hugging Face Unity API""\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we\'ll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation', '### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?']","[7.571166515350342, 5.0268874168396, 4.4876813888549805, 3.992706060409546, -4.91181755065918]",0.9129859209060669,0.9656972289085388,0.9386021494865417
"What is the pretraining objective of the Wav2Vec2 context network?
",The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.,The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones. This encourages the model to find the most similar context vector and quantized speech unit (the target label).,"['3. About half of the feature vectors are randomly masked, and the masked feature vector is fed to a *context network*, which is a Transformer encoder that also adds relative positional embeddings.\n\n4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n\nNow that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\n\n### Audio classification', '<Tip>\n\nBefore you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. If you\'re just getting started or need a refresher, check out our [course](https://huggingface.co/course/chapter1/4?fw=pt) for more information! \n\n</Tip>\n\n## Speech and audio\n\n[Wav2Vec2](model_doc/wav2vec2) is a self-supervised model pretrained on unlabeled speech data and finetuned on labeled data for audio classification and automatic speech recognition. \n\n<div class=""flex justify-center"">\n    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png""/>\n</div>\n\nThis model has four main components:', ""Now that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\n\n### Audio classification\n\nTo use the pretrained model for audio classification, add a sequence classification head on top of the base Wav2Vec2 model. The classification head is a linear layer that accepts the encoder's hidden states. The hidden states represent the learned features from each audio frame which can have varying lengths. To create one vector of fixed-length, the hidden states are pooled first and then transformed into logits over the class labels. The cross-entropy loss is calculated between the logits and target to find the most likely class."", ""### Automatic speech recognition\n\nTo use the pretrained model for automatic speech recognition, add a language modeling head on top of the base Wav2Vec2 model for [connectionist temporal classification (CTC)](glossary#connectionist-temporal-classification-ctc). The language modeling head is a linear layer that accepts the encoder's hidden states and transforms them into logits. Each logit represents a token class (the number of tokens comes from the task vocabulary). The CTC loss is calculated between the logits and targets to find the most likely sequence of tokens, which are then decoded into a transcription."", ""To explain how tasks are solved, we'll walk through what goes on inside the model to output useful predictions.\n\n- [Wav2Vec2](model_doc/wav2vec2) for audio classification and automatic speech recognition (ASR)\n- [Vision Transformer (ViT)](model_doc/vit) and [ConvNeXT](model_doc/convnext) for image classification\n- [DETR](model_doc/detr) for object detection\n- [Mask2Former](model_doc/mask2former) for image segmentation\n- [GLPN](model_doc/glpn) for depth estimation\n- [BERT](model_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder\n- [GPT2](model_doc/gpt2) for NLP tasks like text generation that use a decoder\n- [BART](model_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder\n\n<Tip>""]","[5.915207862854004, -0.060475341975688934, -0.14320743083953857, -1.0501996278762817, -2.878293514251709]",0.9468826055526733,0.9869524836540222,0.9665024280548096
"What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?
",distilbert base uncased finetuned sst2 english,"The default checkpoint used by the sentiment analysis pipeline in the Transformers library is ""distilbert base uncased finetuned sst2 english.""","['ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array. This is done by the tokenizer with the option padding=True. With truncation=True, we ensure that any sentence longer than the maximum the model can handle is truncated.', ""hat happens inside the pipeline function? In this video, we will look at what actually happens when we use the pipeline function of the Transformers library. More specifically, we will look at the sentiment analysis pipeline, and how it went from the two following sentences to the positive labels with their respective scores. As we have seen in the pipeline presentation, there are three stages in the pipeline. First, we convert the raw texts to numbers the model can make sense of, using a tokenizer. Then, those numbers go through the model, which outputs logits. Finally, the post-processing steps transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization."", '<Tip>\n\nNote that the randomly created model is initialized with ""empty"" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints', ""transforms those logits into labels and scores. Let's look in detail at those three steps, and how to replicate them using the Transformers library, beginning with the first stage, tokenization. The tokenization process has several steps. First, the text is split into small chunks called tokens. They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will"", 'With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity']","[8.074857711791992, -0.6296815872192383, -3.2687835693359375, -4.685791969299316, -5.510014533996582]",0.8605305552482605,0.9285821318626404,0.8932620882987976
"What is the purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi""?
",To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.,"The purpose of the notebook ""How to use DeepSpeed to train models with billions of parameters on Habana Gaudi"" is to demonstrate how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.","['| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |', 'And for Pytorch DeepSpeed has built one as well: [DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale](https://arxiv.org/abs/2201.05596), [Mixture of Experts](https://www.deepspeed.ai/tutorials/mixture-of-experts/) - blog posts:  [1](https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/), [2](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/) and specific deployment with large transformer-based natural language generation models: [blog post](https://www.deepspeed.ai/2021/12/09/deepspeed-moe-nlg.html), [Megatron-Deepspeed branch](https://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training).', 'and launch DeepSpeed: \n \n* For an in-depth guide on DeepSpeed integration with [`Trainer`], review [the corresponding documentation](main_classes/deepspeed), specifically the \n[section for a single GPU](main_classes/deepspeed#deployment-with-one-gpu). Some adjustments are required to use DeepSpeed in a notebook; please take a look at the [corresponding guide](main_classes/deepspeed#deployment-in-notebooks).\n* If you prefer to use ð¤ Accelerate, refer to [ð¤ Accelerate DeepSpeed guide](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed).', ""If your model fits onto a single GPU and you have enough space to fit a small batch size, you don't need to use DeepSpeed\nas it'll only slow things down. However, if the model doesn't fit onto a single GPU or you can't fit a small batch, you can \nleverage DeepSpeed ZeRO + CPU Offload, or NVMe Offload for much larger models. In this case, you need to separately\n[install the library](main_classes/deepspeed#installation), then follow one of the guides to create a configuration file \nand launch DeepSpeed: \n \n* For an in-depth guide on DeepSpeed integration with [`Trainer`], review [the corresponding documentation](main_classes/deepspeed), specifically the"", ""-->\n\n# Methods and tools for efficient training on a single GPU\n\nThis guide demonstrates practical techniques that you can use to increase the efficiency of your model's training by \noptimizing memory utilization, speeding up the training, or both. If you'd like to understand how GPU is utilized during \ntraining, please refer to the [Model training anatomy](model_memory_anatomy) conceptual guide first. This guide \nfocuses on practical techniques.  \n\n<Tip>\n\nIf you have access to a machine with multiple GPUs, these approaches are still valid, plus you can leverage additional methods outlined in the [multi-GPU section](perf_train_gpu_many).\n\n</Tip>\n\nWhen training large models, there are two aspects that should be considered at the same time:""]","[-0.05833275616168976, -1.9562220573425293, -1.9725308418273926, -2.4025630950927734, -4.865843296051025]",0.9335915446281433,0.9624800682067871,0.9478157162666321
"What command line module does PyTorch provide to run a script on multiple GPUs?
",torchrun,"The PyTorch command line module provided to run a script on multiple GPUs is `torchrun`. It allows for distributed training by specifying the number of GPUs or nodes to utilize in the command line. The relevant information from the context is: 

```bash
torchrun --nproc_per_node=2 --nnodes=1 example_script.py
```

Therefore, the potential answer based on the context is: 

""The command line module that PyTorch provides to run a script on multiple GPUs is `torchrun`.""

This answer is solely based on the provided context.","['```\n\nTypically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook.\n\nHowever, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n\n## PyTorch Distributed Data Parallelism\n\nAs the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.', '```\n\nThe optimizer needs to be declared based on the model *on the specific device* (so `ddp_model` and not `model`) for all of the gradients to properly be calculated.\n\nLastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n\n```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py', ""### One launcher to rule them all\n\nThe scripts using Accelerate will be completely compatible with your traditional launchers, such as `torch.distributed.launch`. But remembering all the arguments to them is a bit annoying and when you've setup your instance with 4 GPUs, you'll run most of your trainings using them all. Accelerate comes with a handy CLI that works in two steps:\n\n```bash\naccelerate config"", ""```\n\nBy just adding five lines of code to any standard PyTorch training script, you can now run said script on any kind of distributed setting, as well as with or without mixed precision. ð¤ Accelerate even handles the device placement for you, so you can simplify the training loop above even further:\n\n```diff\n  import torch\n  import torch.nn.functional as F\n  from datasets import load_dataset\n+ from accelerate import Accelerator\n\n+ accelerator = Accelerator()\n- device = 'cpu'\n\n- model = torch.nn.Transformer().to(device)\n+ model = torch.nn.Transformer()\n  optim = torch.optim.Adam(model.parameters())\n\n  dataset = load_dataset('my_dataset')\n  data = torch.utils.data.DataLoader(dataset, shuffle=True)\n\n+ model, optim, data = accelerator.prepare(model, optim, data)"", ""-->\n\n# Methods and tools for efficient training on a single GPU\n\nThis guide demonstrates practical techniques that you can use to increase the efficiency of your model's training by \noptimizing memory utilization, speeding up the training, or both. If you'd like to understand how GPU is utilized during \ntraining, please refer to the [Model training anatomy](model_memory_anatomy) conceptual guide first. This guide \nfocuses on practical techniques.  \n\n<Tip>\n\nIf you have access to a machine with multiple GPUs, these approaches are still valid, plus you can leverage additional methods outlined in the [multi-GPU section](perf_train_gpu_many).\n\n</Tip>\n\nWhen training large models, there are two aspects that should be considered at the same time:""]","[3.8920416831970215, 3.5212810039520264, 0.3965171277523041, 0.25297844409942627, -5.7824859619140625]",0.7495229244232178,0.7899150252342224,0.7691890597343445
"What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?
",google/vit-base-patch16-224,"The most popular vision transformer model on the Hugging Face Model Hub for image classification at the time of writing is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels.","['Expand the Tasks category on the left sidebar and select ""Image Classification"" as our task of interest. You will then see all of the models on the Hub that are designed to classify images.\n\nAt the time of writing, the most popular one is `google/vit-base-patch16-224`, which has been trained on ImageNet images at a resolution of 224x224 pixels. We will use this model for our demo.\n\n## Step 2 â Loading the Vision Transformer Model with Gradio\n\nWhen using a model from the Hugging Face Hub, we do not need to define the input or output components for the demo. Similarly, we do not need to be concerned with the details of preprocessing or postprocessing.\nAll of these are automatically inferred from the model tags.', ""The CLIPSeg paper contains some tips on improving the effectiveness of visual prompting. They find that cropping the query image (so that it only contains the object you want to segment) helps a lot. Blurring and darkening the background of the query image also helps a little bit. In the next section, we'll show how you can try out visual prompting yourself using [`ð¤ transformers`](https://huggingface.co/transformers).\n\n## Using CLIPSeg with Hugging Face Transformers\n\nUsing Hugging Face Transformers, you can easily download and run a\npre-trained CLIPSeg model on your images. Let's start by installing\ntransformers.\n\n```python\n!pip install -q transformers"", 'Image Classification with Vision Transformers\n\nRelated spaces: https://huggingface.co/spaces/abidlabs/vision-transformer\nTags: VISION, TRANSFORMERS, HUB\n\n## Introduction\n\nImage classification is a central task in computer vision. Building better classifiers to classify what object is present in a picture is an active area of research, as it has applications stretching from facial recognition to manufacturing quality control.', ""[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer ð¤¯ Weâve been expanding the number of tasks that we support, and weâre proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task weâve added, with more on the way. But what does this mean for you?"", '### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?']","[5.584432601928711, -1.1629157066345215, -1.737079381942749, -2.3108325004577637, -4.623334884643555]",0.8185967206954956,0.9137163758277893,0.8635450601577759
"What is the command to upload an ESPnet model to a Hugging Face repository?
",./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo,"To upload an ESPnet model to a Hugging Face repository, you can use the `run.sh` script with specific parameters. The command to upload the model is:

```bash
./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo
```

This command is based on the information provided in the context where it specifies how to upload a given model to a Hugging Face repository using the `run.sh` script with the appropriate parameters.","['The `run.sh` script allows to upload a given model to a Hugging Face repository.\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo', '```\n\nIf you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \n\n<div class=""flex justify-center"">\n<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png""/>\n<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png""/>\n</div>\n\n## Sharing your models\n\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).', 'Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.', '<div class=""flex justify-center"">\n<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget.png""/>\n<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_widget-dark.png""/>\n</div>\n\n## Using existing models\n\nFor a full guide on loading pre-trained models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo)).', '### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?']","[5.635936737060547, 1.8174245357513428, 0.3169279396533966, -1.5200717449188232, -3.111754894256592]",0.8428489565849304,0.9517905116081238,0.8940131664276123
"What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?
",requirements.txt,"To install custom Python dependencies for Inference Endpoints in a model repository on the Hugging Face Hub, you should add a `requirements.txt` file with the Python dependencies you want to install. In the provided context, it states:

""To add custom dependencies, add a `requirements.txt` file with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt` file and installs the dependencies listed within.""

Therefore, the file that should be added to a model repository to install custom Python dependencies for Inference Endpoints is the `requirements.txt` file.","['To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl', '```\n\nCheck out the `requirements.txt` files in the following model repositories for examples:\n\n* [Optimum and onnxruntime](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt)\n* [diffusers](https://huggingface.co/philschmid/stable-diffusion-v1-4-endpoints/blob/main/requirements.txt)\n\nFor more information, take a look at how you can create and install dependencies when you [use your own custom container](/docs/inference-endpoints/guides/custom_container) for inference.', 'Add custom Dependencies\n\nInference Endpointsâ base image includes all required libraries to run inference on ð¤ Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).', '```\n\nOnce we push the model to the Hub, anyone can use it unless the repository is private. You can download the models using `download`. Apart from the model file, the repository contains the model configuration and the environment requirements.\n\n```python\ndownload_repo = ""downloaded-model""\nhub_utils.download(repo_id=repo_id, dst=download_repo)\n```\n\nThe inference widget is enabled to make predictions in the repository.\n\n![Hosted Inference Widget](assets/94_skops/skops_widget.png)\n\nIf the requirements of your project have changed, you can use `update_env` to update the environment.\n\n```python\nhub_utils.update_env(path=local_repo, requirements=[""scikit-learn""])', '# let\'s save the model\nmodel_path = ""example.pkl""\nlocal_repo = ""my-awesome-model""\nwith open(model_path, mode=""bw"") as f:\n    pickle.dump(model, file=f)\n\n# we will now initialize a local repository\nhub_utils.init(\n    model=model_path, \n    requirements=[f""scikit-learn={sklearn.__version__}""], \n    dst=local_repo,\n    task=""tabular-classification"",\n    data=X_test,\n)']","[6.7703142166137695, 3.794332981109619, 3.386892318725586, 0.621869683265686, -3.7125966548919678]",0.7767137885093689,0.905979573726654,0.8363815546035767
"How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?
",3-5 images,"The context provides the following relevant information regarding the user's query:

""By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images."" 

Based on this information, the answer to the user's query ""How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?"" is:

""3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion.""

","[""Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:"", '```\n\n## Prepare Datasets\n\nOne picture which is from the huggingface datasets [sd-concepts-library/dicoo2](https://huggingface.co/sd-concepts-library/dicoo2) is needed, and save it to the `./dicoo` directory. The picture is shown below:\n\n<a href=""https://huggingface.co/sd-concepts-library/dicoo2/blob/main/concept_images/1.jpeg"">\n    <img src=""https://huggingface.co/sd-concepts-library/dicoo2/resolve/main/concept_images/1.jpeg"" width = ""300"" height=""300"">\n</a>\n\n## Get a FP32 Textual Inversion model\n\nUse the following command to fine-tune the Stable Diffusion model on the above dataset to obtain the FP32 Textual Inversion model.\n\n```bash\nexport MODEL_NAME=""CompVis/stable-diffusion-v1-4""\nexport DATA_DIR=""./dicoo""', '```\n<div class=""flex justify-center"">\n    <img src=""https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png"">\n</div>\n\nYou can find more examples in the ð¤ Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion), and Stable Diffusion is supported for text-to-image, image-to-image, and inpainting.\n\n## Stable Diffusion XL\n\nTo load and run inference with SDXL, use the [`~optimum.intel.OVStableDiffusionXLPipeline`]:\n\n```python\nfrom optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = ""stabilityai/stable-diffusion-xl-base-1.0""\npipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = ""sailing ship in storm by Rembrandt""\nimage = pipeline(prompt).images[0]', ""- Separately trained components of the diffusion pipeline, *e.g.* the text encoder, the unet, and the variational autoencoder, each have their own model class. This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training\nis very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline."", '```\n\nTo further speed-up inference, [statically reshape](#stable-diffusion) the model as shown in the Stable Diffusion section.\n\nYou can find more examples in the ð¤ Optimum [documentation](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion-xl), and running SDXL in OpenVINO is supported for text-to-image and image-to-image.']","[4.9622087478637695, 0.8755930662155151, -2.3637096881866455, -4.108606338500977, -4.119953155517578]",0.7761934995651245,0.8214800357818604,0.798194944858551
"What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?
",10GB,"The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is when the checkpoint exceeds 10GB. This is stated in the context: ""model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces.""","['## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we\'ll use a normal-size models with a small shard size: let\'s take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(""bert-base-cased"")', '```bash\n`Sharding Strategy`: [1] FULL_SHARD (shards optimizer states, gradients and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD\n`Offload Params`: Decides Whether to offload parameters and gradients to CPU\n`Auto Wrap Policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3] NO_WRAP \n`Transformer Layer Class to Wrap`: When using `TRANSFORMER_BASED_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g, \n`BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`...\n`Min Num Params`: minimum number of parameters when using `SIZE_BASED_WRAP`\n`Backward Prefetch`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH', '<Tip>\n\nNote that the randomly created model is initialized with ""empty"" tensors, which take the space in memory without filling it (thus the random values are whatever was in this chunk of memory at a given time). The random initialization following the appropriate distribution for the kind of model/parameters instantiated (like a normal distribution for instance) is only performed after step 3 on the non-initialized weights, to be as fast as possible! \n\n</Tip>\n\nIn this guide, we explore the solutions Transformers offer to deal with this issue. Note that this is an area of active development, so the APIs explained here may change slightly in the future.\n\n## Sharded checkpoints', 'For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)\nand [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).\n\n## Gradient Checkpointing\n\nSome large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. \nThis is because there are other components that also require memory storage.', '```\n\n## Low memory loading\n\nSharded checkpoints reduce the memory usage during step 2 of the workflow mentioned above, but in order to use that model in a low memory setting, we recommend leveraging our tools based on the Accelerate library.\n\nPlease read the following guide for more information: [Large model loading using Accelerate](./main_classes/model#large-model-loading)']","[4.744496822357178, -3.6378064155578613, -3.8610434532165527, -3.953829050064087, -4.730417251586914]",0.7856171131134033,0.8584163188934326,0.8204049468040466
"What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?
","To track their machine learning experiments at every stage, from training to production.","The purpose of Weights and Biases (W&B) for data scientists and machine learning scientists is to track their machine learning experiments at every stage, from training to production. W&B allows for the aggregation of metrics over samples and provides a customizable and searchable dashboard for monitoring experiments.","[""Gradio and W&B Integration\n\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\nTags: WANDB, SPACES\nContributed by Gradio team\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\n\n\n## What is Wandb?\n\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:"", '```\n\n3. Finetune StyleGAN and W&B experiment tracking\n\n   This next step will open a W&B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here\'s the code you need for that:\n\n   ```python\n\n   alpha =  1.0\n   alpha = 1-alpha\n\n   preserve_color = True\n   num_iter = 100\n   log_interval = 50\n\n\n   samples = []\n   column_names = [""Reference (y)"", ""Style Code(w)"", ""Real Face Image(x)""]\n\n   wandb.init(project=""JoJoGAN"")\n   config = wandb.config\n   config.num_iter = num_iter\n   config.preserve_color = preserve_color\n   wandb.log(\n   {""Style reference"": [wandb.Image(transforms.ToPILImage()(target_im))]},\n   step=0)', 'g_optim.zero_grad()\n    loss.backward()\n    g_optim.step()\n\nout_table = wandb.Table(data=samples, columns=column_names)\nwandb.log({""Current Samples"": out_table})\n\n`', '### 11. What possible source can the bias observed in a model have?\n\n<Question\n\tchoices={[\n\t\t{\n\t\t\ttext: ""The model is a fine-tuned version of a pretrained model and it picked up its bias from it."",\n\t\t\texplain: ""When applying Transfer Learning, the bias in the pretrained model used persists in the fine-tuned model."",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: ""The data the model was trained on is biased."",\n\t\t\texplain: ""This is the most obvious source of bias, but not the only one."",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: ""The metric the model was optimizing for is biased."",\n\t\t\texplain: ""A less obvious source of bias is the way the model is trained. Your model will blindly optimize for whatever metric you chose, without any second thoughts."",\n\t\t\tcorrect: true\n\t\t}\n\t]}\n/>', ""Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>""]","[6.532749176025391, -3.5551819801330566, -5.5279974937438965, -7.221482753753662, -10.452218055725098]",0.8495670557022095,0.9607831239700317,0.9017588496208191
"What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?
",Optimum,The open-source library created by Hugging Face to simplify Transformer acceleration is called Accelerate.,"['For more complex examples in situation, you can look at the official [Transformers examples](https://github.com/huggingface/transformers/tree/master/examples). Each folder contains a `run_task_no_trainer.py` that leverages the Accelerate library!', '### How to get involved?\n\nTo get started, just `pip install accelerate` or see the [documentation](https://huggingface.co/docs/accelerate/installation.html) for more install options.\n\nAccelerate is a fully open-sourced project, you can find it on [GitHub](https://github.com/huggingface/accelerate), have a look at its [documentation](https://huggingface.co/docs/accelerate/) or skim through our [basic examples](https://github.com/huggingface/accelerate/tree/main/examples). Please let us know if you have any issue or feature you would like the library to support. For all questions, the [forums](https://discuss.huggingface.co/c/accelerate) is the place to check!', 'With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity', 'Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. \n\nIf you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) \n\n\n## Resources', '--\ntitle: ""Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration""\nthumbnail: /blog/assets/80_intel/01.png\nauthors:\n- user: juliensimon\n---\n\n\n\n# Intel and Hugging Face Partner to Democratize Machine Learning Hardware Acceleration\n\n\n\n\n![image](assets/80_intel/01.png)\n\nThe mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption.']","[2.9424405097961426, 2.4637651443481445, 1.2619612216949463, 0.42829689383506775, -0.41013264656066895]",0.8155030608177185,0.8058191537857056,0.8106322288513184
"What parameter is used to ensure that elements in a row have the same height in Gradio?
",equal_height,The parameter used to ensure that elements in a row have the same height in Gradio is the `equal_height` parameter. This can be achieved by setting `equal_height=True` in the `Row` component as shown in the context provided.,"['```\n\nè¦ä½¿è¡ä¸­çæ¯ä¸ªåç´ å·æç¸åçé«åº¦ï¼è¯·ä½¿ç¨ `style` æ¹æ³ç `equal_height` åæ°ã\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(""æé®2"")\n```\n\nå¯ä»¥éè¿æ¯ä¸ªç»ä»¶ä¸­å­å¨ç `scale` å `min_width` åæ°æ¥æ§å¶è¡ä¸­åç´ çå®½åº¦ã\n\n- `scale` æ¯ä¸ä¸ªæ´æ°ï¼å®ä¹äºåç´ å¨è¡ä¸­çå ç¨ç©ºé´ãå¦æå° scale è®¾ç½®ä¸º `0`ï¼ååç´ ä¸ä¼æ©å±å ç¨ç©ºé´ãå¦æå° scale è®¾ç½®ä¸º `1` ææ´å¤§ï¼ååç´ å°æ©å±ãè¡ä¸­çå¤ä¸ªåç´ å°ææ¯ä¾æ©å±ãå¨ä¸é¢çç¤ºä¾ä¸­ï¼`btn1` å°æ¯ `btn2` æ©å±ä¸¤åï¼è `btn0` å°æ ¹æ¬ä¸ä¼æ©å±ï¼\n\n```python\nwith gr.Blocks() as demo:\n    with gr.Row():\n        btn0 = gr.Button(""æé®0"", scale=0)\n        btn1 = gr.Button(""æé®1"", scale=1)\n        btn2 = gr.Button(""æé®2"", scale=2)', '### 8. Which of the following are components included in the Gradio library?\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: ""Textbox."",\n\t\t\texplain: ""Yes, you can create textboxes with the Textbox component."",\n            correct: true\n        },\n        {\n\t\t\ttext: ""Graph."",\n\t\t\texplain: ""There is currently no Graph component."",\n        },\n        {\n\t\t\ttext: ""Image."",\n\t\t\texplain: ""Yes, you can create an image upload widget with the Image component."",\n            correct: true\n        },\n        {\n\t\t\ttext: ""Audio."",\n\t\t\texplain: ""Yes, you can create an audio upload widget with the Audio component."",\n            correct: true\n        },\n\t]}\n/>\n\n### 9. What does Gradio `Blocks` allow you to do?', '### 4. Gradio is designed primarily for NLP models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: ""True"",\n\t\t\texplain: ""Gradio works with pretty much any data type, not just NLP.""\n        },\n        {\n\t\t\ttext: ""False"",\n\t\t\texplain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",\n            correct: true\n        }\n\t]}\n/>\n\n### 5. Which of the following features are supported by Gradio?', '!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={9}\n    classNames=""absolute z-10 right-0 top-0""\n/>\n\nLet\'s test what you learned in this chapter!\n\n### 1. What can you use Gradio to do?', '### 7. Select all the steps necessary for adding state to your Gradio interface']","[2.313002109527588, -5.377772808074951, -5.592459678649902, -5.76461124420166, -7.319007873535156]",0.8023748993873596,0.8682721257209778,0.8340238928794861
"What is the command to install the latest version of Optimum with OpenVINO support?
","pip install --upgrade-strategy eager optimum[""openvino""]","To install the latest version of Optimum with OpenVINO support, you need to use the following command from the provided context:

```bash
pip install --upgrade-strategy eager optimum[""openvino""]
```","['# OpenVINO\n\nð¤ [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\n\nYou\'ll need to install ð¤ Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n\n```bash\npip install --upgrade-strategy eager optimum[""openvino""]', '## Optimum Intel\n\n### OpenVINO', '```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.\n\n## Stable Diffusion\n\nTo load and run inference, use the [`~optimum.intel.OVStableDiffusionPipeline`]. If you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, set `export=True`:\n\n```python\nfrom optimum.intel import OVStableDiffusionPipeline\n\nmodel_id = ""runwayml/stable-diffusion-v1-5""\npipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nprompt = ""sailing ship in storm by Rembrandt""\nimage = pipeline(prompt).images[0]\n\n# Don\'t forget to save the exported model\npipeline.save_pretrained(""openvino-sd-v1-5"")', '| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|', '## Optimum ONNX Runtime']","[4.491132736206055, -2.1894102096557617, -2.2540807723999023, -2.7243189811706543, -7.23805046081543]",0.8438802361488342,0.9253576993942261,0.8827428221702576
