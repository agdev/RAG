# -*- coding: utf-8 -*-
"""contextual_retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1moEwlhfNWhT0QCpzTK0d2a1RW51Ik-D4
"""



"""# Description

## This aim of this notbook to show and compare Contextual Retrieval implementation of RAG vs. simple/traditional implemintation
### Steps:
- Chucking
- Summarization
- BM25 embedding
- BM25 model saving to file
- Model embedding
- Storage of dense and sparse vectors
- Retrieval of sparse and dense vectors
- Fusion of Ranking
- Simple Retrieval

"""

!pip install sentence_transformers -qU
!pip install rank_bm25 -qU
!pip install datasets -qU
!pip install pinecone -qU
!pip install langchain -qU
!pip install langchain_core -qU
!pip install langchain_groq -qU
!pip install langchain-google-genai -qU
!pip install langchain-openai -qU



"""# Importing libraries"""

import numpy as np
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from sklearn.metrics.pairwise import cosine_similarity
from nltk.translate.bleu_score import sentence_bleu
# from rouge import Rouge
from datasets import load_dataset
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
import pinecone
import pandas as pd # for dataframe
import getpass
from google.colab import userdata
import os



"""# Loading dataset"""

# Step 1: Load and Chunk the Knowledge Base
# Load dataset from Hugging Face
nltk.download('punkt')
dataset = load_dataset("m-ric/huggingface_doc_qa_eval")

df = pd.DataFrame(dataset['train'])
print(df.head())

best_answers_df = df[df['standalone_score'] >= 4]
print(best_answers_df.head())

best_answers_df.info()

"""# Extract contexts from the dataset and create Langchain documents"""

# Extract contexts from the dataset and create Langchain documents
documents = [Document(page_content=context) for context in best_answers_df['context']]  # Assuming we're using the 'train' split
print(documents)

"""# **Setting up Embedding model**

## **sentence-transformers**
"""

load ' sentence-transformers/all-MiniLM-L6-v2' embedding model from Hugging Face
from transformers import AutoTokenizer, AutoModel
model_name = 'sentence-transformers/all-MiniLM-L6-v2'
tokenizer = AutoTokenizer.from_pretrained(model_name)
max_seq_length = tokenizer.model_max_length
embedding_model = AutoModel.from_pretrained(model_name)

"""## **openai**"""

openai_api_key = userdata.get("OPENAI_API_KEY")
if not openai_api_key:
  openai_api_key = getpass("Please enter your OPENAI API KEY: ")

os.environ["OPENAI_API_KEY"] = openai_api_key

from langchain_openai import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

max_seq_length = embedding_model.embedding_ctx_length
# index_dimensions = embedding_model.dimensions
index_dimensions = 1536 # default setting of text-embedding-3-small
print(f'max_seq_length:{max_seq_length}, index_dimensions:{index_dimensions}')

"""## **Google**"""

# from langchain_google_genai import GoogleGenerativeAIEmbeddings

# MODEL_GEMINI_EMBED = "text-embedding-004"
# embedding_model = GoogleGenerativeAIEmbeddings(model=MODEL_GEMINI_EMBED)

# print(f'max_seq_length:{max_seq_length}, index_dimensions:{index_dimensions}')

"""# Defining text splitter

###openai
"""

MARKDOWN_SEPARATORS = [
    "\n#{1,6} ",
    "```\n",
    "\n\\*\\*\\*+\n",
    "\n---+\n",
    "\n___+\n",
    "\n\n",
    "\n",
    " ",
    "",
]
# Use RecursiveCharacterTextSplitter to split documents into chunks
chunk_overlap = 50
chunk_size = 512 - chunk_overlap
print('chunk_size',chunk_size)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap,
    separators=MARKDOWN_SEPARATORS,
)

"""####Other"""

# def get_seq_length(text: str):
#     tokens = tokenizer.encode(text, add_special_tokens=True)
#     return len(tokens)

# MARKDOWN_SEPARATORS = [
#     "\n#{1,6} ",
#     "```\n",
#     "\n\\*\\*\\*+\n",
#     "\n---+\n",
#     "\n___+\n",
#     "\n\n",
#     "\n",
#     " ",
#     "",
# ]
# # Use RecursiveCharacterTextSplitter to split documents into chunks
# chunk_overlap = 50
# chunk_size = max_seq_length - chunk_overlap
# print('chunk_size',chunk_size)
# text_splitter = RecursiveCharacterTextSplitter(
#     chunk_size=chunk_size,
#     chunk_overlap=chunk_overlap,
#     length_function=get_seq_length,
#     add_start_index=True,
#     separators=MARKDOWN_SEPARATORS,
# )

class Chunk:
    def __init__(self, text: str):
        self.text = text
        self.context = None

class ProcessedDocument:
    def __init__(self, document: Document, chunks: list[Chunk]):
        self.document = document
        self.chunks = chunks

docs_processed: list[ProcessedDocument] = []
for doc in documents:
    text = doc.page_content  # Extract the text content from the Document
    chunks = text_splitter.split_text(text)  # Split the text into chunks (strings)
    print(f"Number of chunks for document #{len(docs_processed)}: {len(chunks)}")
    processed_doc = ProcessedDocument(
        doc,
        [Chunk(chunk_text) for chunk_text in chunks]
    )
    docs_processed.append(processed_doc)
print(f"Number of Processed document: {len(docs_processed)}")

# for doc in docs_processed:
#     for chunk in doc.chunks:
#         try:
#           chunk_length = get_seq_length(chunk.text)
#           if chunk_length > max_seq_length:
#               print(f"Chunk exceeds max length: {chunk_length} tokens")
#         except Exception as e:
#           print(f"Error processing chunk: {e}")
#           print("===========================")
#           print(f"Chunk: {chunk.text}")

# Count total chunks
total_chunks = sum(len(doc.chunks) for doc in docs_processed)
print(f"Total number of chunks across all documents: {total_chunks}")

"""# **Define summary chain**"""

from langchain.prompts import PromptTemplate
from google.colab import userdata

"""### **OPENAI**"""

from langchain_openai import ChatOpenAI


model_chat_name = "gpt-3.5-turbo"
llm = ChatOpenAI(model=model_chat_name)
sum_provider = 'OPENAI'

from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate.from_messages([
    ("system",
            """You are an AI assistant specializing in document summarization and contextualization. Your task is to provide brief, relevant context for a specific chunk of text based on a larger document. Here's how to proceed:

First, carefully read and analyze the following document:

<document>
{document}
</document>

Now, consider this specific chunk of text from the document:

<chunk>
{chunk}
</chunk>

Your goal is to provide a concise context for this chunk, situating it within the whole document. Follow these guidelines:

1. Analyze how the chunk relates to the overall document's themes, arguments, or narrative.
2. Identify the chunk's role or significance within the broader context of the document.
3. Determine what information from the rest of the document is most relevant to understanding this chunk.

Compose your response as follows:
- Provide 3-4 sentences maximum of context.
- Begin directly with the context, without any introductory phrases.
- Use language like "Focuses on..." or "Addresses..." to describe the chunk's content.
- Ensure the context would be helpful for improving search retrieval of the chunk.

Important notes:
- Do not use phrases like "this chunk" or "this section" in your response.
- Do not repeat the chunk's content verbatim; provide context from the rest of the document.
- Avoid unnecessary details; be succinct and relevant.
- Do not include any additional commentary or meta-discussion about the task itself.

 Remember, your goal is to provide clear, concise, and relevant context that situates the given chunk within the larger document.
            """

     )
])

def create_context_chain(llm):
    return prompt_template | llm

context_chain = create_context_chain(llm)

def get_context(doc: ProcessedDocument, chunk: Chunk):
    context= context_chain.invoke({"document": doc.document.page_content, "chunk": chunk})
    return context.content

def generate_context(docs_processed: list[ProcessedDocument]):
    for doc in docs_processed:
        for chunk in doc.chunks:
            context: str = get_context(doc= doc.document.page_content, chunk= chunk)
            doc.context = context.context

generate_context(docs_processed)

"""### **GROQ**"""

# from pydantic import BaseModel, Field
# from typing import Optional
# class Context(BaseModel):
#     context: Optional[str] = Field(description="Summary of the chunk in the context of the document")

#It hits the limit even thoug lower than daily
# from langchain_groq import ChatGroq

# # MODEL_GROQ = "llama-3.1-8b-instant"
# MODEL_GROQ = "llama-3.2-90b-text-preview"
# groq_api_key = userdata.get("GROQ_API_KEY")
# if not groq_api_key:
#   groq_api_key = getpass("Please enter your GROQ API KEY: ")

# llm = ChatGroq(api_key=groq_api_key, model=MODEL_GROQ,
#                         temperature=0,
#                         max_tokens=None,
#                         timeout=None,
#                         max_retries=2,)
# sum_provider = 'GROQ'

"""### **GOOGLE**"""

# MODEL_GEMINI_CHAT = "gemini-1.5-flash"

# gemini_api_key = userdata.get("GEMINI_API_KEY")
# if not gemini_api_key:
#   gemini_api_key = getpass("Please enter your GEMINI API KEY: ")

# os.environ["GOOGLE_API_KEY"] = gemini_api_key

# from langchain_google_genai import GoogleGenerativeAI
# llm = GoogleGenerativeAI(model=MODEL_GEMINI_CHAT)
# sum_provider = 'GOOGLE'

# prompt_template = PromptTemplate(
#     input_variables=["document", "chunk"],
#     template=
#        """You are an AI assistant specializing in document summarization and contextualization. Your task is to provide brief, relevant context for a specific chunk of text based on a larger document. Here's how to proceed:

# First, carefully read and analyze the following document:

# <document>
# {{DOCUMENT}}
# </document>

# Now, consider this specific chunk of text from the document:

# <chunk>
# {{CHUNK}}
# </chunk>

# Your goal is to provide a concise context for this chunk, situating it within the whole document. Follow these guidelines:

# 1. Analyze how the chunk relates to the overall document's themes, arguments, or narrative.
# 2. Identify the chunk's role or significance within the broader context of the document.
# 3. Determine what information from the rest of the document is most relevant to understanding this chunk.

# Compose your response as follows:
# - Provide 3-4 sentences maximum of context.
# - Begin directly with the context, without any introductory phrases.
# - Use language like "Focuses on..." or "Addresses..." to describe the chunk's content.
# - Ensure the context would be helpful for improving search retrieval of the chunk.

# Important notes:
# - Do not use phrases like "this chunk" or "this section" in your response.
# - Do not repeat the chunk's content verbatim; provide context from the rest of the document.
# - Avoid unnecessary details; be succinct and relevant.
# - Do not include any additional commentary or meta-discussion about the task itself.

#  Remember, your goal is to provide clear, concise, and relevant context that situates the given chunk within the larger document.)

# def create_context_chain(llm, structure: bool = True):
#     # Configure the LLM to produce structured output
#     if structure:
#         l_llm = llm.with_structured_output(Context)
#     else:
#         l_llm = llm
#     # Create the chain using the pipe operator
#     chain = prompt_template | l_llm
#     return chain

# context_chain = create_context_chain(llm, structure = False)

doc = docs_processed[30]
print("page:\n",doc.document.page_content)
# for chunk in doc.chunks:
chunk = doc.chunks[0]
print('chunk:\n', chunk.text)

# def get_context(doc: ProcessedDocument, chunk: Chunk, provider: str):
#   if (provider == 'OPEMAI')
#     context= context_chain.invoke({"document": doc.document.page_content, "chunk": chunk})
#     return context.content
#   else:
#     context: Context = context_chain.invoke({"document": doc.document.page_content, "chunk": chunk})
#     return context.context

# print(f"chunk with context: Context: \n\n {context.context} \n\n Chunk: {chunk.text}")

# import time
# from datetime import datetime

# def generate_context(docs_processed: list[ProcessedDocument]):
#     # Initialize counters
#     calls_per_minute = 0
#     last_reset_time = time.time()

#     for doc in docs_processed:
#         for chunk in doc.chunks:
#             current_time = time.time()

#             # Check if a minute has passed since last reset
#             if current_time - last_reset_time >= 60:
#                 print(f"Made {calls_per_minute} calls in the last minute")
#                 calls_per_minute = 0
#                 last_reset_time = current_time
#             else:
#                 # If we're still within the same minute and hit rate limit
#                 if calls_per_minute >= 15:  # Assuming 30 calls per minute limit
#                     wait_time = 60 - (current_time - last_reset_time)
#                     print(f"Rate limit reached. Waiting {wait_time:.2f} seconds...")
#                     time.sleep(wait_time)
#                     calls_per_minute = 0
#                     last_reset_time = time.time()

#             # Make the API call
#             context: str = get_context(doc= doc.document.page_content, chunk= chunk, provider=???)
#             doc.context = context.context

#             # Increment counter
#             calls_per_minute += 1

#             # Optional: print progress
#             print(f"Processed chunk {calls_per_minute} in current minute. Total chunks processed: {sum(len(d.chunks) for d in docs_processed[:docs_processed.index(doc)]) + len(doc.chunks[:doc.chunks.index(chunk) + 1])}")

"""## Save processed documents to file
## Downloading processed documents in case notebook times out

"""

import joblib
from datetime import datetime
from google.colab import files
import glob
import os

def save_download_object(object, filename):
    joblib.dump(object, filename)
    print(f"Saved object to {filename}")
    files.download(filename)
    print(f"Downloaded {filename}")

def create_timestamp() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def create_filename_timestamp(filename, extension = "joblib") -> str:
    timestamp = create_timestamp()
    return f"{filename}_{timestamp}.{extension}"

# def load_bm25_model(filename):
#     try:
#         return joblib.load(filename)
#     except (FileNotFoundError, OSError):
#         return None

# def get_latest_bm25_file():
#     # Look for files matching the pattern bm25_*.joblib
#     files = glob.glob("bm25_*.joblib")
#     if not files:
#         return None
#     # Return the most recent file
#     return max(files, key=os.path.getctime)

# Create filename with timestamp
docs_processed_filename = create_filename_timestamp("docs_processed")

# Save the processed documents
save_download_object(docs_processed, docs_processed_filename)

# Create list of chunks with their contexts
chunks_with_context = []
chunks_regular=[]
for doc in docs_processed:
    for chunk in doc.chunks:
        chunks_regular.append(chunk.text)
        if chunk.context:  # Only include chunks that have a context
            chunks_with_context.append(
              f"{chunk.context} \n\n {chunk.text}"
            )

"""# **Saving Context + Chunks to dataset**"""

from datasets import Dataset
from huggingface_hub import login

# Create lists to store the data
chunk_texts = []
document_texts = []
contexts = []

# Extract data from docs_processed
for doc in docs_processed:
    for chunk in doc.chunks:
        chunk_texts.append(chunk.text)
        contexts.append(chunk.context)
        document_texts.append(doc.document.page_content)

# Create dictionary for dataset
dataset_dict = {
    'chunk': chunk_texts,
    'document': document_texts,
    'context': contexts
}

# Convert to Hugging Face Dataset
dataset = Dataset.from_dict(dataset_dict)

hf_token = userdata.get("HuggingFace")
if not hf_token:
  # Login to Hugging Face (you'll need your token)
  hf_token = input("Please enter your Hugging Face token: ")
login(hf_token)

# Push to Hugging Face Hub
dataset.push_to_hub(
    f"AIEnthusiast369/hf_doc_qa_eval_chunk_size_{chunk_size}_open_ai",  # Replace with your username and desired dataset name
    private=False  # Set to False if you want it public
)

# prompt: print chunks from docs_processed where context has value

# for doc in docs_processed:
#   for chunk in doc.chunks:
#     if chunk.context:
#       print(f"chunk with context: Context: \n\n {chunk.context} \n\n Chunk: {chunk.text}")

"""# **Setiing up Pinecone**"""

from pinecone.grpc import PineconeGRPC as Pinecone
from pinecone import ServerlessSpec

pinecone_api_key = userdata.get("PINECONE_API_KEY")
if not pinecone_api_key:
  pinecone_api_key = input("Please enter your PINECONE API KEY: ")

spec=ServerlessSpec(
    cloud="aws",
    region="us-east-1"
  )

EMBEDDING_INDEX_CONTEXTUAL: str = "embedding_index_openai_contextual"
EMBEDDING_INDEX_REGULAR: str = "embedding_index_openai_regular"

pc = Pinecone(api_key=pinecone_api_key)

def create_bm25(chunks: list[str]):
    # # Try to load existing BM25 model
    # latest_bm25_file = get_latest_bm25_file()
    # if latest_bm25_file:
    #     bm25 = load_bm25_model(latest_bm25_file)
    #     if bm25 is not None:
    #         print(f"Loaded existing BM25 model from {latest_bm25_file}")
    #         return bm25

    # If no existing model found or loading failed, create a new one
    print("Creating BM25 model...")
    tokenized_chunks = [nltk.word_tokenize(chunk) for chunk in chunks]
    bm25 = BM25Okapi(tokenized_chunks)

    # # Save the new model
    # bm25_filename = create_filename_timestamp("bm25")
    # save_download_object(bm25, bm25_filename)

    return bm25

from typing import Any
def wait_for_index(index_name):
    while True:
        desc = pinecone.describe_index(index_name)
        if desc['ready']:
            print("Index is ready!")
            break
        sleep(5)

def create_pinecone_indexes(pinecone, embedding_model, index_name: str, chunks: list[str], specs: ServerlessSpec, dimensions) -> Any:

    if EMBEDDING_INDEX_CONTEXTUAL not in pinecone.list_indexes():
        pinecone.create_index(index_name, dimension=dimensions, metric="cosine", spec=specs)
        wait_for_index(index_name)

    # Connect to Pinecone indexes
    embedding_index = pinecone.Index(EMBEDDING_INDEX_CONTEXTUAL)


    # Semantic Embeddings using a Pre-trained Transformer Model
    embeddings = embedding_model.encode(chunks, convert_to_tensor=False)
    # Store embeddings in Pinecone
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        embedding_index.upsert([(str(i), embedding, {"text": chunk})])
    return embedding_index

"""# **Creating Indeses**"""

create_pinecone_indexes(pc, embedding_model, EMBEDDING_INDEX_CONTEXTUAL chunks_with_context, spec, 1536)
create_pinecone_indexes(pc, embedding_model, EMBEDDING_INDEX_CONTEXTUAL chunks_regular, spec, 1536)
bm25_regular = create_bm25(chunks_regular)
bm25_contextual = create_bm25(chunks_with_context)

# from sentence_transformers import CrossEncoder

def fusion_rank_search(
    query: str,
    bm25,
    chunks: list[str],
    model,
    embedding_index,
    reranker_model = None,
    k: int = 5,
    weight_sparse: float = 0.5,
    reranker_cutoff: int = 20  # Number of top results to rerank
):
    # Get BM25 results
    tokenized_query = nltk.word_tokenize(query)
    bm25_scores = bm25.get_scores(tokenized_query)
    bm25_top_indices = np.argsort(bm25_scores)[::-1][:reranker_cutoff]

    # Normalize BM25 scores using min-max normalization
    bm25_scores_norm = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))
    bm25_results = [
        {
            'id': str(i),
            'score': bm25_scores_norm[i],
            'metadata': {'text': chunks[i]}
        }
        for i in bm25_top_indices
    ]

    # Get embedding results
    query_embedding = model.encode(query, convert_to_tensor=False).tolist()
    embedding_results = embedding_index.query(query_embedding, top_k=reranker_cutoff, include_metadata=True)

    # Extract and normalize embedding scores
    dense_scores = np.array([match['score'] for match in embedding_results['matches']])
    dense_scores_norm = (dense_scores - np.min(dense_scores)) / (np.max(dense_scores) - np.min(dense_scores))

    # Create dictionaries to store normalized scores
    fusion_scores = defaultdict(lambda: {'sparse': 0.0, 'dense': 0.0, 'text': ''})

    # Store normalized BM25 scores
    for result in bm25_results:
        doc_id = result['id']
        fusion_scores[doc_id]['sparse'] = result['score']
        fusion_scores[doc_id]['text'] = result['metadata']['text']

    # Store normalized embedding scores
    for match, norm_score in zip(embedding_results['matches'], dense_scores_norm):
        doc_id = match['id']
        fusion_scores[doc_id]['dense'] = norm_score
        fusion_scores[doc_id]['text'] = match['metadata']['text']

    # Combine scores using weighted average
    weight_dense = 1.0 - weight_sparse
    initial_results = [
        {
            'id': doc_id,
            'score': (
                weight_sparse * scores['sparse'] +
                weight_dense * scores['dense']
            ),
            'metadata': {
                'text': scores['text'],
                'sparse_score': scores['sparse'],
                'dense_score': scores['dense']
            }
        }
        for doc_id, scores in fusion_scores.items()
    ]

    # Sort by combined score
    initial_results.sort(key=lambda x: x['score'], reverse=True)
    initial_results = initial_results[:reranker_cutoff]

    # Apply reranking if reranker model is provided
    if reranker_model is not None:
        # Prepare pairs for reranking
        pairs = [(query, result['metadata']['text']) for result in initial_results]

        # Get reranker scores - use them directly for final ranking
        rerank_scores = reranker_model.predict(pairs)

        # Update results with reranker scores
        for result, rerank_score in zip(initial_results, rerank_scores):
            result['metadata']['rerank_score'] = float(rerank_score)
            # Use reranker score as the final score
            result['score'] = float(rerank_score)

        # Resort based on reranker scores
        initial_results.sort(key=lambda x: x['score'], reverse=True)

    return initial_results[:k]

from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from tqdm import tqdm
import pandas as pd

def evaluate_rag_system(
    best_answers_df: pd.DataFrame,
    bm25,
    chunks: list[str],
    embedding_model,
    embedding_index,
    llm_chain,
    n_samples: int = None  # Optional: limit number of samples for testing
):
    # Initialize ROUGE scorer
    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    # Initialize results storage
    results = []

    # Get subset of dataframe if n_samples is specified
    eval_df = best_answers_df.head(n_samples) if n_samples else best_answers_df

    # Iterate through questions and answers
    for idx, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc="Evaluating Questions"):
        query = row['question']
        reference_answer = row['answer']

        try:
            # Get relevant context using fusion ranking
            retrieved_results = fusion_rank_search(
                query=query,
                bm25=bm25,
                chunks=chunks,
                model=embedding_model,
                embedding_index=embedding_index,
                k=20
            )

            # Prepare context for LLM
            context = "\n".join([res['metadata']['text'] for res in retrieved_results])

            # Generate answer using LLM
            llm_response = llm_chain.invoke({
                "context": context,
                "question": query
            })
            generated_answer = llm_response.content if hasattr(llm_response, 'content') else llm_response

            # Calculate BLEU score
            reference_tokens = [reference_answer.split()]
            candidate_tokens = generated_answer.split()
            bleu_score = sentence_bleu(reference_tokens, candidate_tokens)

            # Calculate ROUGE scores
            rouge_scores = rouge_scorer_instance.score(reference_answer, generated_answer)

            # Store results
            result = {
                'question': query,
                'reference_answer': reference_answer,
                'generated_answer': generated_answer,
                'bleu_score': bleu_score,
                'rouge1_f1': rouge_scores['rouge1'].fmeasure,
                'rouge2_f1': rouge_scores['rouge2'].fmeasure,
                'rougeL_f1': rouge_scores['rougeL'].fmeasure,
                'retrieved_contexts': [res['metadata']['text'] for res in retrieved_results],
                'context_scores': [res['score'] for res in retrieved_results]
            }
            results.append(result)

        except Exception as e:
            print(f"Error processing question {idx}: {str(e)}")
            continue

    # Convert results to DataFrame
    results_df = pd.DataFrame(results)

    # Calculate and print average scores
    avg_scores = {
        'Average BLEU': results_df['bleu_score'].mean(),
        'Average ROUGE-1': results_df['rouge1_f1'].mean(),
        'Average ROUGE-2': results_df['rouge2_f1'].mean(),
        'Average ROUGE-L': results_df['rouge2_f1'].mean()
    }

    return results_df, avg_scores

# Example usage:
def print_evaluation_results(results_df, avg_scores):
    print("\nAverage Scores:")
    for metric, score in avg_scores.items():
        print(f"{metric}: {score:.4f}")

    print("\nDetailed Results Sample (first 3):")
    for idx, row in results_df.head(3).iterrows():
        print("\nQuestion:", row['question'])
        print("Reference Answer:", row['reference_answer'])
        print("Generated Answer:", row['generated_answer'])
        print(f"BLEU Score: {row['bleu_score']:.4f}")
        print(f"ROUGE-1 F1: {row['rouge1_f1']:.4f}")
        print(f"ROUGE-2 F1: {row['rouge2_f1']:.4f}")
        print(f"ROUGE-L F1: {row['rougeL_f1']:.4f}")
        print("\nRetrieved Contexts:")
        for context, score in zip(row['retrieved_contexts'], row['context_scores']):
            print(f"Score: {score:.4f}")
            print(f"Context: {context[:200]}...")

# Run evaluation
results_df, avg_scores = evaluate_rag_system(
    best_answers_df=best_answers_df,
    bm25=bm25,
    chunks=chunks,
    model=embedding_model,
    embedding_index=embedding_index,
    llm_chain=llm_chain,
    n_samples=10  # Optional: start with a small sample for testing
)

# Print results
print_evaluation_results(results_df, avg_scores)

# Save results to CSV (optional)
results_df.to_csv('rag_evaluation_results.csv', index=False)